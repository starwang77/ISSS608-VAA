[
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "Cardiovascular diseases are a growing global health concern, with heart attacks being one of the leading causes of morbidity and mortality. Understanding the risk factors and health profiles associated with heart attacks is crucial for developing targeted prevention strategies. This dataset provides a comprehensive analysis of heart attack occurrences in Japan, specifically comparing youth and adult age groups. By examining key health indicators, lifestyle choices, and medical histories, this dataset offers valuable insights into the distinct patterns and potential triggers of heart attacks across different demographics.\n\n\n\n\nIn this exercise, Exploratory Data Analysis (EDA) methods and ggplot functions are used to explore:\nWhether the incidence rate of heart disease is different in different ages and genders, and whether there is a significant relationship between heart disease and various diseases, including whether lifestyle, eating habits, etc. affect the incidence rate of heart disease\n\n\n\n\n\nFor this exercise, we load the following R packages using the pacman::p_load() function\n\npacman::p_load(tidyverse, haven,\n               ggrepel, ggthemes,\n               ggridges, ggdist,\n               patchwork, scales,\n               corrr, ggcorrplot)\n\n\n\n\nIn this section, we need to read this data-set\n\ndf &lt;- read.csv(\"data/japan_heart_attack_dataset.csv\")\n\ncheck dataset structure and show the head data\n\nstr(df)\n\n'data.frame':   30000 obs. of  32 variables:\n $ Age                    : int  56 69 46 32 60 25 78 38 56 75 ...\n $ Gender                 : chr  \"Male\" \"Male\" \"Male\" \"Female\" ...\n $ Region                 : chr  \"Urban\" \"Urban\" \"Rural\" \"Urban\" ...\n $ Smoking_History        : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ Diabetes_History       : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ Hypertension_History   : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ Cholesterol_Level      : num  186 185 211 211 224 ...\n $ Physical_Activity      : chr  \"Moderate\" \"Low\" \"Low\" \"Moderate\" ...\n $ Diet_Quality           : chr  \"Poor\" \"Good\" \"Average\" \"Good\" ...\n $ Alcohol_Consumption    : chr  \"Low\" \"Low\" \"Moderate\" \"High\" ...\n $ Stress_Levels          : num  3.64 3.38 3.81 6.01 6.81 ...\n $ BMI                    : num  34 28.2 27.6 23.7 19.8 ...\n $ Heart_Rate             : num  72.3 57.5 64.7 55.1 76.7 ...\n $ Systolic_BP            : num  124 130 146 132 101 ...\n $ Diastolic_BP           : num  85.7 73.5 72 68.2 92.9 ...\n $ Family_History         : chr  \"No\" \"Yes\" \"No\" \"No\" ...\n $ Heart_Attack_Occurrence: chr  \"No\" \"No\" \"No\" \"No\" ...\n $ Extra_Column_1         : num  0.405 0.0363 0.853 0.3909 0.9336 ...\n $ Extra_Column_2         : num  0.433 0.513 0.22 0.297 0.976 ...\n $ Extra_Column_3         : num  0.629 0.668 0.613 0.156 0.174 ...\n $ Extra_Column_4         : num  0.702 0.116 0.508 0.87 0.295 ...\n $ Extra_Column_5         : num  0.498 0.424 0.901 0.39 0.959 ...\n $ Extra_Column_6         : num  0.0079 0.0839 0.2272 0.4032 0.6898 ...\n $ Extra_Column_7         : num  0.795 0.689 0.496 0.741 0.905 ...\n $ Extra_Column_8         : num  0.291 0.83 0.752 0.224 0.757 ...\n $ Extra_Column_9         : num  0.497 0.634 0.182 0.329 0.338 ...\n $ Extra_Column_10        : num  0.522 0.302 0.629 0.143 0.362 ...\n $ Extra_Column_11        : num  0.7997 0.0437 0.0183 0.9078 0.7286 ...\n $ Extra_Column_12        : num  0.7224 0.4517 0.0632 0.5423 0.1767 ...\n $ Extra_Column_13        : num  0.149 0.879 0.147 0.922 0.485 ...\n $ Extra_Column_14        : num  0.834 0.536 0.997 0.626 0.312 ...\n $ Extra_Column_15        : num  0.0616 0.6178 0.9745 0.2286 0.4528 ...\n\nhead(df)\n\n  Age Gender Region Smoking_History Diabetes_History Hypertension_History\n1  56   Male  Urban             Yes               No                   No\n2  69   Male  Urban              No               No                   No\n3  46   Male  Rural             Yes               No                   No\n4  32 Female  Urban              No               No                   No\n5  60 Female  Rural              No               No                   No\n6  25 Female  Rural              No               No                   No\n  Cholesterol_Level Physical_Activity Diet_Quality Alcohol_Consumption\n1          186.4002          Moderate         Poor                 Low\n2          185.1367               Low         Good                 Low\n3          210.6966               Low      Average            Moderate\n4          211.1655          Moderate         Good                High\n5          223.8143              High         Good                High\n6          220.3400               Low         Good                High\n  Stress_Levels      BMI Heart_Rate Systolic_BP Diastolic_BP Family_History\n1      3.644786 33.96135   72.30153    123.9021     85.68281             No\n2      3.384056 28.24287   57.45764    129.8933     73.52426            Yes\n3      3.810911 27.60121   64.65870    145.6549     71.99481             No\n4      6.014878 23.71729   55.13147    131.7852     68.21133             No\n5      6.806883 19.77158   76.66792    100.6946     92.90249             No\n6      8.207233 20.24744   67.66268    134.5966     73.14970             No\n  Heart_Attack_Occurrence Extra_Column_1 Extra_Column_2 Extra_Column_3\n1                      No     0.40498852      0.4333000      0.6287124\n2                      No     0.03627815      0.5125669      0.6683928\n3                      No     0.85297888      0.2195908      0.6134366\n4                      No     0.39085280      0.2968468      0.1557240\n5                      No     0.93356280      0.9756513      0.1737750\n6                      No     0.52447124      0.2328291      0.5885502\n  Extra_Column_4 Extra_Column_5 Extra_Column_6 Extra_Column_7 Extra_Column_8\n1      0.7016095      0.4981423    0.007901312      0.7945826      0.2907792\n2      0.1155287      0.4238194    0.083932768      0.6889511      0.8301636\n3      0.5080100      0.9006698    0.227205241      0.4963436      0.7521068\n4      0.8702514      0.3903559    0.403181621      0.7414089      0.2239681\n5      0.2953319      0.9594067    0.689787430      0.9045740      0.7570983\n6      0.4169773      0.1703778    0.448359048      0.3690557      0.1958173\n  Extra_Column_9 Extra_Column_10 Extra_Column_11 Extra_Column_12\n1      0.4971931       0.5219945      0.79965663      0.72239788\n2      0.6344903       0.3020434      0.04368285      0.45166789\n3      0.1815012       0.6291803      0.01827617      0.06322702\n4      0.3293139       0.1431905      0.90778075      0.54232201\n5      0.3377609       0.3623747      0.72855208      0.17669914\n6      0.3959485       0.8354301      0.00359574      0.94144789\n  Extra_Column_13 Extra_Column_14 Extra_Column_15\n1       0.1487387       0.8340099     0.061632229\n2       0.8786714       0.5356022     0.617825340\n3       0.1465122       0.9972962     0.974455410\n4       0.9224606       0.6262165     0.228606344\n5       0.4847487       0.3120910     0.452808843\n6       0.8757355       0.7186975     0.008745039\n\n\n\n\n\n\n\n\nThe dataset consists of 30,000 rows and 32 columns.\n\n\n\n\n1️⃣ Demographic Information:\n\nAge (years)\nGender (Male/Female)\nRegion (Geographical location)\n\n2️⃣ Health Conditions:\n\nSmoking_History (History of smoking)\nDiabetes_History (History of diabetes)\nHypertension_History (History of hypertension)\nCholesterol_Level (Cholesterol levels)\nBMI (Body Mass Index)\n\n3️⃣ Lifestyle Factors:\n\nPhysical_Activity (Level of physical activity)\nDiet_Quality (Dietary quality)\nAlcohol_Consumption (Alcohol intake)\n\n4️⃣ Physiological Measurements:\n\nHeart_Rate (Heart rate)\nSystolic_BP (Systolic blood pressure)\nDiastolic_BP (Diastolic blood pressure)\n\n5️⃣ Heart Attack Occurrence:\n\nHeart_Attack_Occurrence (Indicates whether a heart attack occurred)\n\nThis dataset provides a comprehensive view of demographic, health, lifestyle, and physiological factors that may influence heart attack occurrences. The analysis will explore potential correlations and risk factors associated with heart disease.\n\n\n\n\nI found the many columns-“Extra_column” in this dataset, we try to check. We first take a look at the data, and check if there are any duplicate entries.\n\nglimpse(df)\n\nRows: 30,000\nColumns: 32\n$ Age                     &lt;int&gt; 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36, 40…\n$ Gender                  &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"F…\n$ Region                  &lt;chr&gt; \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Rural\", \"…\n$ Smoking_History         &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"Y…\n$ Diabetes_History        &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Hypertension_History    &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Cholesterol_Level       &lt;dbl&gt; 186.4002, 185.1367, 210.6966, 211.1655, 223.81…\n$ Physical_Activity       &lt;chr&gt; \"Moderate\", \"Low\", \"Low\", \"Moderate\", \"High\", …\n$ Diet_Quality            &lt;chr&gt; \"Poor\", \"Good\", \"Average\", \"Good\", \"Good\", \"Go…\n$ Alcohol_Consumption     &lt;chr&gt; \"Low\", \"Low\", \"Moderate\", \"High\", \"High\", \"Hig…\n$ Stress_Levels           &lt;dbl&gt; 3.644786, 3.384056, 3.810911, 6.014878, 6.8068…\n$ BMI                     &lt;dbl&gt; 33.96135, 28.24287, 27.60121, 23.71729, 19.771…\n$ Heart_Rate              &lt;dbl&gt; 72.30153, 57.45764, 64.65870, 55.13147, 76.667…\n$ Systolic_BP             &lt;dbl&gt; 123.90209, 129.89331, 145.65490, 131.78522, 10…\n$ Diastolic_BP            &lt;dbl&gt; 85.68281, 73.52426, 71.99481, 68.21133, 92.902…\n$ Family_History          &lt;chr&gt; \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ Heart_Attack_Occurrence &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ Extra_Column_1          &lt;dbl&gt; 0.40498852, 0.03627815, 0.85297888, 0.39085280…\n$ Extra_Column_2          &lt;dbl&gt; 0.43330004, 0.51256694, 0.21959083, 0.29684675…\n$ Extra_Column_3          &lt;dbl&gt; 0.62871236, 0.66839275, 0.61343656, 0.15572404…\n$ Extra_Column_4          &lt;dbl&gt; 0.70160955, 0.11552874, 0.50800995, 0.87025144…\n$ Extra_Column_5          &lt;dbl&gt; 0.49814235, 0.42381938, 0.90066981, 0.39035591…\n$ Extra_Column_6          &lt;dbl&gt; 0.007901312, 0.083932768, 0.227205241, 0.40318…\n$ Extra_Column_7          &lt;dbl&gt; 0.79458257, 0.68895108, 0.49634358, 0.74140891…\n$ Extra_Column_8          &lt;dbl&gt; 0.29077922, 0.83016364, 0.75210679, 0.22396813…\n$ Extra_Column_9          &lt;dbl&gt; 0.49719307, 0.63449028, 0.18150125, 0.32931387…\n$ Extra_Column_10         &lt;dbl&gt; 0.52199452, 0.30204337, 0.62918031, 0.14319054…\n$ Extra_Column_11         &lt;dbl&gt; 0.79965663, 0.04368285, 0.01827617, 0.90778075…\n$ Extra_Column_12         &lt;dbl&gt; 0.72239788, 0.45166789, 0.06322702, 0.54232201…\n$ Extra_Column_13         &lt;dbl&gt; 0.1487387, 0.8786714, 0.1465122, 0.9224606, 0.…\n$ Extra_Column_14         &lt;dbl&gt; 0.8340099, 0.5356022, 0.9972962, 0.6262165, 0.…\n$ Extra_Column_15         &lt;dbl&gt; 0.061632229, 0.617825340, 0.974455410, 0.22860…\n\n\nDisplay only the first few rows after removing the Extra_Column.\n\nhead(df %&gt;% select(-starts_with(\"Extra_Column\")))\n\n  Age Gender Region Smoking_History Diabetes_History Hypertension_History\n1  56   Male  Urban             Yes               No                   No\n2  69   Male  Urban              No               No                   No\n3  46   Male  Rural             Yes               No                   No\n4  32 Female  Urban              No               No                   No\n5  60 Female  Rural              No               No                   No\n6  25 Female  Rural              No               No                   No\n  Cholesterol_Level Physical_Activity Diet_Quality Alcohol_Consumption\n1          186.4002          Moderate         Poor                 Low\n2          185.1367               Low         Good                 Low\n3          210.6966               Low      Average            Moderate\n4          211.1655          Moderate         Good                High\n5          223.8143              High         Good                High\n6          220.3400               Low         Good                High\n  Stress_Levels      BMI Heart_Rate Systolic_BP Diastolic_BP Family_History\n1      3.644786 33.96135   72.30153    123.9021     85.68281             No\n2      3.384056 28.24287   57.45764    129.8933     73.52426            Yes\n3      3.810911 27.60121   64.65870    145.6549     71.99481             No\n4      6.014878 23.71729   55.13147    131.7852     68.21133             No\n5      6.806883 19.77158   76.66792    100.6946     92.90249             No\n6      8.207233 20.24744   67.66268    134.5966     73.14970             No\n  Heart_Attack_Occurrence\n1                      No\n2                      No\n3                      No\n4                      No\n5                      No\n6                      No\n\n\n\n\nUsing the duplicated function, we see that there are no duplicate entries in the data.\n\ndf[duplicated(df),]\n\n [1] Age                     Gender                  Region                 \n [4] Smoking_History         Diabetes_History        Hypertension_History   \n [7] Cholesterol_Level       Physical_Activity       Diet_Quality           \n[10] Alcohol_Consumption     Stress_Levels           BMI                    \n[13] Heart_Rate              Systolic_BP             Diastolic_BP           \n[16] Family_History          Heart_Attack_Occurrence Extra_Column_1         \n[19] Extra_Column_2          Extra_Column_3          Extra_Column_4         \n[22] Extra_Column_5          Extra_Column_6          Extra_Column_7         \n[25] Extra_Column_8          Extra_Column_9          Extra_Column_10        \n[28] Extra_Column_11         Extra_Column_12         Extra_Column_13        \n[31] Extra_Column_14         Extra_Column_15        \n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\n\n\ncolSums(is.na(df))\n\n                    Age                  Gender                  Region \n                      0                       0                       0 \n        Smoking_History        Diabetes_History    Hypertension_History \n                      0                       0                       0 \n      Cholesterol_Level       Physical_Activity            Diet_Quality \n                      0                       0                       0 \n    Alcohol_Consumption           Stress_Levels                     BMI \n                      0                       0                       0 \n             Heart_Rate             Systolic_BP            Diastolic_BP \n                      0                       0                       0 \n         Family_History Heart_Attack_Occurrence          Extra_Column_1 \n                      0                       0                       0 \n         Extra_Column_2          Extra_Column_3          Extra_Column_4 \n                      0                       0                       0 \n         Extra_Column_5          Extra_Column_6          Extra_Column_7 \n                      0                       0                       0 \n         Extra_Column_8          Extra_Column_9         Extra_Column_10 \n                      0                       0                       0 \n        Extra_Column_11         Extra_Column_12         Extra_Column_13 \n                      0                       0                       0 \n        Extra_Column_14         Extra_Column_15 \n                      0                       0 \n\n\n✅ No missing values in the data, ready for use.\n✅ No duplicate entries, no need for deduplication.\n✅ Ready for data visualization and analysis.\n\n\n\n\n\nThis stacked histogram visualizes the distribution of heart attack occurrences across different age groups.\nThe x-axis (Age) represents the age of individuals.The y-axis (Count) represents the number of individuals in each age group.\nThe bars are color-coded:Blue (“No”) represents individuals who did not experience a heart attack.Red (“Yes”) represents individuals who experienced a heart attack.\n\nggplot(df, aes(x = Age, fill = Heart_Attack_Occurrence)) +\n  geom_histogram(binwidth = 5, color = \"black\", alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Heart Attack Occurrence by Age Distribution\",\n       x = \"Age\",\n       y = \"Count\") +\n  scale_fill_manual(values = c(\"No\" = \"blue\", \"Yes\" = \"red\"))\n\n\n\n\n\n\n\n\n\n\n\nThe dataset has a relatively uniform distribution of individuals across different age groups, except for a slight drop in count near age 80.\nHeart attacks (red section) appear in all age groups, but their proportion increases slightly with age.\nYounger individuals (below 40) have a lower incidence of heart attacks, whereas older individuals (50+) show a higher proportion of heart attack occurrences.\n\nThis visualization effectively highlights the relationship between age and heart attack occurrence, showing a general trend that older individuals are more likely to experience heart attacks.\n\n\n\n\nWe could find there is a relationship between age and heart attack occurrence. So we can try to analysis the different heart attack occurrence from different age group.\nFirstly, we define the age group, then we can calculate the heart attack rate, and finally we attempt to draw a stacked bar chart that allows us to directly observe the relationship between age and heart attack rate.\n\ndf_summary &lt;- df %&gt;%\n  mutate(Age_Group = cut(Age, breaks = c(18, 35, 50, 65, 80), \n                         labels = c(\"18-35\", \"36-50\", \"51-65\", \"66-80\"),\n                         right = FALSE)) %&gt;%\n  group_by(Age_Group, Heart_Attack_Occurrence) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'Age_Group'. You can override using the\n`.groups` argument.\n\nggplot(df_summary, aes(x = Age_Group, y = Count, fill = Heart_Attack_Occurrence)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +  \n  geom_text(aes(label = Count), \n            position = position_dodge(width = 0.8), \n            vjust = -0.5, size = 3) +  \n  theme_minimal() +\n  labs(title = \"Heart Attack Occurrence by Age Group\",\n       x = \"Age Group\",\n       y = \"Count\") +\n  scale_fill_manual(values = c(\"No\" = \"blue\", \"Yes\" = \"red\"))\n\n\n\n\n\n\n\n\n\n\nThe total number of individuals decreases slightly as age increases.\n\n18-35 age group has the highest count (7487 individuals).\n36-50, 51-65, and 66-80 age groups have relatively similar numbers (~6500-6600 individuals).\nThe absolute count of heart attack occurrences (red bars) is relatively stable across all age groups:\n\n18-35: 779 cases\n36-50: 733 cases\n51-65: 740 cases\n66-80: 712 cases\n\n\n\nThe absolute number of heart attacks does not show a significant increase with age.\nHowever, this does not account for population size differences. We need to look at heart attack rates for a better interpretation.\n\n\n\ndf_rate &lt;- df %&gt;%\n  mutate(Age_Group = cut(Age, breaks = c(18, 35, 50, 65, 80), \n                         labels = c(\"18-35\", \"36-50\", \"51-65\", \"66-80\"),\n                         right = FALSE)) %&gt;%\n  group_by(Age_Group) %&gt;%\n  summarise(Heart_Attack_Rate = mean(Heart_Attack_Occurrence == \"Yes\")) %&gt;%\n  ungroup()\n\nggplot(df_rate, aes(x = Age_Group, y = Heart_Attack_Rate, group = 1)) +\n  geom_line(color = \"red\", size = 1) +  \n  geom_point(color = \"red\", size = 3) +  \n  geom_text(aes(label = round(Heart_Attack_Rate, 3)),  \n            vjust = -0.4, size = 3, color = \"black\") +  \n  theme_minimal() +\n  labs(title = \"Heart Attack Rate by Age Group\",\n       x = \"Age Group\",\n       y = \"Heart Attack Rate\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nHeart attack rate is not strictly increasing with age.\n\nThe 18-35 group has the lowest rate (0.094).\nThe rate increases in the 36-50 group (0.101 or 10.1%) and peaks at 51-65 (0.104).\nSurprisingly, the 66-80 group sees a slight decline (0.097) in heart attack rate.\nThe highest heart attack risk is in the 51-65 age group.\n\n\n\n\n✅ Heart attack risk is lowest in the youngest group (18-35) and peaks at 51-65.\n✅ The absolute number of heart attacks remains relatively stable across age groups.\n✅ The oldest group (66-80) shows a slight decrease in heart attack rate, possibly due to survivor bias or better health management.\n\n\n\n\n\ndf_gender_rate &lt;- df %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(Heart_Attack_Rate = mean(Heart_Attack_Occurrence == \"Yes\"))\n\nggplot(df_gender_rate, aes(x = Gender, y = Heart_Attack_Rate, fill = Gender)) +\n  geom_bar(stat = \"identity\") +  \n  geom_text(aes(label = round(Heart_Attack_Rate, 3)), vjust = -0.5) +\n  theme_minimal() +\n  labs(title = \"Heart Attack Rate by Gender\",\n       x = \"Gender\",\n       y = \"Heart Attack Rate\")\n\n\n\n\n\n\n\n\n\n\nHeart Attack Rate Comparison\nThe heart attack rate for males (0.102) is slightly higher than for females (0.096). The difference is small but noticeable, indicating that gender may have a minor impact on heart attack occurrence.\nGender-Specific Risk\nMales tend to have a slightly higher likelihood of experiencing heart attacks. Females have a lower but still significant heart attack rate\n\n\n\n\n\nregion_rate &lt;- df %&gt;%\n  group_by(Region) %&gt;%\n  summarise(Heart_Attack_Rate = mean(Heart_Attack_Occurrence == \"Yes\")) %&gt;%\n  arrange(desc(Heart_Attack_Rate))  \n\n\nggplot(region_rate, aes(x = reorder(Region, -Heart_Attack_Rate), y = Heart_Attack_Rate, fill = Region)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = round(Heart_Attack_Rate, 4)), \n            vjust = -0.5, size = 4, color = \"black\") +  \n  theme_minimal() +\n  labs(title = \"Heart Attack Rate by Region\",\n       x = \"Region\",\n       y = \"Heart Attack Rate\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nFrom the above graph, the heart attack occurrence in Urban and Rural is the noticeable difference.\n\n\n\nWe need to compare the heart attack rate between individuals who have the condition and those who don’t.\n\n\n\n# Calculate heart attack rates for each health factor\nhealth_factors &lt;- df %&gt;%\n  summarise(\n    Smoking_Yes = mean(Heart_Attack_Occurrence[Smoking_History == \"Yes\"] == \"Yes\"),\n    Smoking_No = mean(Heart_Attack_Occurrence[Smoking_History == \"No\"] == \"Yes\"),\n    \n    Diabetes_Yes = mean(Heart_Attack_Occurrence[Diabetes_History == \"Yes\"] == \"Yes\"),\n    Diabetes_No = mean(Heart_Attack_Occurrence[Diabetes_History == \"No\"] == \"Yes\"),\n    \n    Hypertension_Yes = mean(Heart_Attack_Occurrence[Hypertension_History == \"Yes\"] == \"Yes\"),\n    Hypertension_No = mean(Heart_Attack_Occurrence[Hypertension_History == \"No\"] == \"Yes\")\n  ) \n\n# Convert to long format for visualization\nhealth_factors_long &lt;- tidyr::pivot_longer(health_factors, \n                                           cols = everything(), \n                                           names_to = c(\"Condition\", \"Group\"),\n                                           names_sep = \"_\",\n                                           values_to = \"Heart_Attack_Rate\")\n\nprint(health_factors_long)\n\n# A tibble: 6 × 3\n  Condition    Group Heart_Attack_Rate\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt;\n1 Smoking      Yes              0.102 \n2 Smoking      No               0.0974\n3 Diabetes     Yes              0.103 \n4 Diabetes     No               0.0976\n5 Hypertension Yes              0.101 \n6 Hypertension No               0.0982\n\n\n\n# Plot heart attack rate for each health condition with values displayed\nggplot(health_factors_long, aes(x = Group, y = Heart_Attack_Rate, fill = Group)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = round(Heart_Attack_Rate, 4)),  # Display values rounded to 4 decimal places\n            vjust = -0.5, size = 4, color = \"black\") +\n  facet_wrap(~Condition, scales = \"free_y\") +  # Create separate graphs for each condition\n  theme_minimal() +\n  labs(title = \"Impact of Health Conditions on Heart Attack Occurrence\",\n       x = \"Health Condition Group\",\n       y = \"Heart Attack Rate\") +\n  scale_fill_manual(values = c(\"Yes\" = \"red\", \"No\" = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\nHigher Heart Attack Rates for Individuals with Health Conditions:\nThe heart attack occurrence rate is slightly higher for individuals with a history of smoking, diabetes, and hypertension compared to those without.\nSpecifically:\nSmoking: 10.2% for smokers vs. 9.74% for non-smokers.\nDiabetes: 10.33% for individuals with diabetes vs. 9.76% for those without.\nHypertension: 10.08% for individuals with hypertension vs. 9.82% for those without.\nThe findings suggest that smoking, diabetes, and hypertension slightly increase the likelihood of a heart attack, with diabetes having the most significant impact.\n\n\n\n\n# Compare Cholesterol Level and BMI for Heart Attack vs. No Heart Attack\nnumerical_health &lt;- df %&gt;%\n  group_by(Heart_Attack_Occurrence) %&gt;%\n  summarise(\n    Avg_Cholesterol = mean(Cholesterol_Level, na.rm = TRUE),\n    Avg_BMI = mean(BMI, na.rm = TRUE)\n  )\n\nprint(numerical_health)\n\n# A tibble: 2 × 3\n  Heart_Attack_Occurrence Avg_Cholesterol Avg_BMI\n  &lt;chr&gt;                             &lt;dbl&gt;   &lt;dbl&gt;\n1 No                                 200.    25.0\n2 Yes                                200.    24.9\n\n# Create boxplots for cholesterol and BMI\ndf_long &lt;- df %&gt;%\n  tidyr::pivot_longer(cols = c(Cholesterol_Level, BMI), \n                      names_to = \"Health_Metric\",\n                      values_to = \"Value\")\n\nggplot(df_long, aes(x = Heart_Attack_Occurrence, y = Value, fill = Heart_Attack_Occurrence)) +\n  geom_boxplot() +\n  facet_wrap(~Health_Metric, scales = \"free\") + \n  theme_minimal() +\n  labs(title = \"Comparison of Cholesterol Level and BMI in Heart Attack Cases\",\n       x = \"Heart Attack Occurrence\",\n       y = \"Value\") +\n  scale_fill_manual(values = c(\"Yes\" = \"red\", \"No\" = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\nThe average cholesterol levels (199.91 vs. 199.79) show almost no difference, suggesting that cholesterol level alone may not be a strong distinguishing factor for heart attack risk.\nThe average BMI values (25.01 vs. 24.91) are very close, indicating no significant difference between the two groups.\nCholesterol levels and BMI are nearly identical between those who experienced a heart attack and those who did not, suggesting they might not be the primary driving factors for heart attacks in this dataset.\n\n\n\n\ndf_numeric &lt;- df %&gt;%\n  mutate(\n    Heart_Attack_Occurrence = ifelse(Heart_Attack_Occurrence == \"Yes\", 1, 0),\n    Smoking_History = ifelse(Smoking_History == \"Yes\", 1, 0),\n    Diabetes_History = ifelse(Diabetes_History == \"Yes\", 1, 0),\n    Hypertension_History = ifelse(Hypertension_History == \"Yes\", 1, 0)\n  ) %&gt;%\n  select(Heart_Attack_Occurrence, Smoking_History, Diabetes_History, Hypertension_History, \n         Cholesterol_Level, BMI)  \n\n\ndf_numeric &lt;- df_numeric %&gt;% na.omit()\n\n\ncor_matrix &lt;- cor(df_numeric, method = \"pearson\")\n\n\nggcorrplot(cor_matrix, lab = TRUE, hc.order = TRUE, type = \"lower\", colors = c(\"blue\", \"white\", \"red\"))\n\n\n\n\n\n\n\n\n\ndf &lt;- df %&gt;%\n  mutate(\n    Heart_Attack_Occurrence = ifelse(Heart_Attack_Occurrence == \"Yes\", 1, 0),\n    Smoking_History = ifelse(Smoking_History == \"Yes\", 1, 0),\n    Diabetes_History = ifelse(Diabetes_History == \"Yes\", 1, 0),\n    Hypertension_History = ifelse(Hypertension_History == \"Yes\", 1, 0),\n    Cholesterol_Level = scale(Cholesterol_Level),  \n    BMI = scale(BMI)  \n  )\n\n\nmodel &lt;- glm(Heart_Attack_Occurrence ~ Smoking_History + Diabetes_History + Hypertension_History + \n                                       Cholesterol_Level + BMI, \n             data = df, family = binomial())\n\nsummary(model)\n\n\nCall:\nglm(formula = Heart_Attack_Occurrence ~ Smoking_History + Diabetes_History + \n    Hypertension_History + Cholesterol_Level + BMI, family = binomial(), \n    data = df)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -2.24672    0.02773 -81.015   &lt;2e-16 ***\nSmoking_History       0.05125    0.04189   1.223    0.221    \nDiabetes_History      0.06251    0.04739   1.319    0.187    \nHypertension_History  0.02956    0.04452   0.664    0.507    \nCholesterol_Level    -0.00360    0.01935  -0.186    0.852    \nBMI                  -0.01728    0.01935  -0.893    0.372    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 19346  on 29999  degrees of freedom\nResidual deviance: 19342  on 29994  degrees of freedom\nAIC: 19354\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nCorrelation Heatmap:\nThe heatmap shows very weak correlations between heart attack occurrence and other health factors.\nAll correlation values are close to zero, suggesting that none of the selected health factors strongly correlate with heart attack occurrence.\nThe highest correlation is only 0.01, which is negligible.\nThere are also weak correlations among other health-related variables, implying that these factors do not exhibit strong interdependencies in this dataset.\nLogistic Regression Results:\nThe logistic regression model was used to analyze the relationship between health factors and heart attack occurrence.\nNone of the predictor variables (Smoking History, Diabetes History, Hypertension History, Cholesterol Level, BMI) were statistically significant in predicting heart attack occurrence (p-values &gt; 0.05 for all variables).\nThe intercept is significant, indicating that the base probability of heart attack occurrence (without considering the predictor variables) is non-trivial.\nThe estimated coefficients suggest that:Smoking history and diabetes history have positive coefficients, indicating a slight increase in heart attack occurrence. Cholesterol level and BMI have negative coefficients.\nBoth the logistic regression model and the correlation analysis suggest that the selected health factors (Smoking, Diabetes, Hypertension, Cholesterol Level, and BMI) do not significantly contribute to predicting heart attack occurrence in this dataset.\n\n\n\n\n\ndf &lt;- read.csv(\"data/japan_heart_attack_dataset.csv\")\n\ndf &lt;- df %&gt;%\n  mutate(Heart_Attack_Occurrence = ifelse(Heart_Attack_Occurrence == \"Yes\", 1, 0))\n\n\ndf &lt;- df %&gt;%\n  mutate(Stress_Category = case_when(\n    Stress_Levels &gt;= 0 & Stress_Levels &lt;= 3 ~ \"Low\",\n    Stress_Levels &gt; 3 & Stress_Levels &lt;= 6 ~ \"Moderate\",\n    Stress_Levels &gt; 6 & Stress_Levels &lt;= 10 ~ \"High\"\n  ))\n\n\nstress_impact &lt;- df %&gt;%\n  group_by(Stress_Category) %&gt;%\n  summarise(\n    Total_Count = n(),  \n    Heart_Attack_Count = sum(Heart_Attack_Occurrence),  \n    Heart_Attack_Rate = mean(Heart_Attack_Occurrence)  \n  )\n\nprint(stress_impact)\n\n# A tibble: 3 × 4\n  Stress_Category Total_Count Heart_Attack_Count Heart_Attack_Rate\n  &lt;chr&gt;                 &lt;int&gt;              &lt;dbl&gt;             &lt;dbl&gt;\n1 High                   9279                874            0.0942\n2 Low                    4698                504            0.107 \n3 Moderate              16023               1586            0.0990\n\nggplot(stress_impact, aes(x = Stress_Category, y = Heart_Attack_Rate, group = 1)) +\n  geom_line(color = \"red\", size = 1) +  \n  geom_point(color = \"red\", size = 3) +  \n  geom_text(aes(label = round(Heart_Attack_Rate, 4)),  \n            vjust = -0.5, size = 5, color = \"black\") +  \n  theme_minimal() +\n  labs(title = \"Heart Attack Rate by Stress Level\",\n       x = \"Stress Level Category\",\n       y = \"Heart Attack Rate\")\n\n\n\n\n\n\n\n\nHeart Attack Rate Across Stress Levels:\nThe heart attack rate is highest in the low stress category (10.73%).\nThe moderate stress category has a slightly lower heart attack rate (9.99%).\nThe high stress category has the lowest heart attack rate (9.42%).\n\n\n\n\n\n\ndf &lt;- df %&gt;%\n  mutate(\n    Physical_Activity = factor(Physical_Activity, levels = c(\"Low\", \"Moderate\", \"High\")),\n    Heart_Attack_Occurrence = as.numeric(Heart_Attack_Occurrence == \"Yes\")\n  )\n\n\nggplot(df, aes(x = Heart_Attack_Occurrence, y = Physical_Activity, fill = ..x..)) +\n  geom_density_ridges_gradient(scale = 2, rel_min_height = 0.01) +\n  scale_fill_viridis_c() +  \n  theme_minimal() +\n  labs(title = \"Heart Attack Occurrence Distribution by Physical Activity Level\",\n       x = \"Heart Attack Occurrence Probability\",\n       y = \"Physical Activity Level\",\n       fill = \"Probability\")\n\nWarning: The dot-dot notation (`..x..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(x)` instead.\n\n\nPicking joint bandwidth of 0.143\n\n\n\n\n\n\n\n\n\nThe distribution of heart attack occurrence probability is similar across different physical activity levels (High, Moderate, Low).\n\n\n\n\ndf &lt;- df %&gt;%\n  mutate(\n    Diet_Quality = factor(Diet_Quality, levels = c(\"Poor\", \"Average\", \"Good\")),\n    Heart_Attack_Occurrence = as.numeric(Heart_Attack_Occurrence == \"Yes\")\n  )\n\n\nggplot(df, aes(x = Heart_Attack_Occurrence, y = Diet_Quality, fill = ..x..)) +\n  geom_density_ridges_gradient(scale = 2, rel_min_height = 0.01) +\n  scale_fill_viridis_c() +  \n  theme_minimal() +\n  labs(title = \"Heart Attack Occurrence Distribution by Diet Quality\",\n       x = \"Heart Attack Occurrence Probability\",\n       y = \"Diet Quality\",\n       fill = \"Probability\")\n\nPicking joint bandwidth of 0.144\n\n\n\n\n\n\n\n\n\nAll three categories show overlapping probability distributions, meaning diet quality alone may not be a decisive factor for heart attack occurrence.\n\ndf &lt;- df %&gt;%\n  mutate(\n    Alcohol_Consumption = factor(Alcohol_Consumption, levels = c(\"None\", \"Low\", \"Moderate\", \"High\")),\n    Heart_Attack_Occurrence = as.numeric(Heart_Attack_Occurrence == \"Yes\")\n  )\n\n\nggplot(df, aes(x = Heart_Attack_Occurrence, y = Alcohol_Consumption, fill = ..x..)) +\n  geom_density_ridges_gradient(scale = 2, rel_min_height = 0.01) +\n  scale_fill_viridis_c() +  \n  theme_minimal() +\n  labs(title = \"Heart Attack Occurrence Distribution by Alcohol Consumption Level\",\n       x = \"Heart Attack Occurrence Probability\",\n       y = \"Alcohol Consumption Level\",\n       fill = \"Probability\")\n\nPicking joint bandwidth of 0.156\n\n\n\n\n\n\n\n\n\nThe distribution remains centered around a similar probability range for all levels of alcohol consumption.\n\n\n\n\nIn this analysis, we explored various factors related to heart attack occurrence, including age, gender, lifestyle choices, and health conditions. Through data visualization and statistical modeling, we examined their potential impact on heart disease risk. The key findings are:\nAge and Heart Attack Risk: Heart attack occurrence is not strictly increasing with age. While the absolute number of cases is relatively stable across age groups, the heart attack rate is lowest in the youngest group (18-35) and peaks in the 51-65 age group before slightly decreasing in the oldest (66-80) group.\nGender Influence: Males have a slightly higher heart attack rate (10.2%) compared to females (9.6%). Though the difference is small, it suggests a minor gender-based variation in heart attack susceptibility.\nImpact of Health Conditions: Individuals with a history of smoking, diabetes, and hypertension show slightly higher heart attack rates compared to those without these conditions. Among these, diabetes has the most noticeable effect, with a heart attack rate of 10.33% for diabetics versus 9.76% for non-diabetics.\nCholesterol and BMI Effects: The average cholesterol levels and BMI values between individuals who experienced heart attacks and those who did not are nearly identical.\nLifestyle Factors: The influence of physical activity, diet quality, and alcohol consumption on heart attack occurrence appears to be minimal. The probability distributions for heart attack occurrences remain similar across all categories within these factors, suggesting that none of them are strong standalone predictors of heart attacks.\nGiven the weak correlations and the non-significant predictors in logistic regression, further studies with more comprehensive datasets and additional health indicators may be necessary to develop a more accurate risk assessment model for heart attacks. Statistical hypothesis testing and more advanced modeling techniques could provide deeper insights into the complex relationships between these factors and heart disease risk.\nAt last, this exercise was a helpful way to practice presenting data effectively, and create data visualization.\n\n\n\n\nCode Book\nLEW YING ZHEN SERENA"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#overview",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#overview",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "Cardiovascular diseases are a growing global health concern, with heart attacks being one of the leading causes of morbidity and mortality. Understanding the risk factors and health profiles associated with heart attacks is crucial for developing targeted prevention strategies. This dataset provides a comprehensive analysis of heart attack occurrences in Japan, specifically comparing youth and adult age groups. By examining key health indicators, lifestyle choices, and medical histories, this dataset offers valuable insights into the distinct patterns and potential triggers of heart attacks across different demographics."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#our-task",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#our-task",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "In this exercise, Exploratory Data Analysis (EDA) methods and ggplot functions are used to explore:\nWhether the incidence rate of heart disease is different in different ages and genders, and whether there is a significant relationship between heart disease and various diseases, including whether lifestyle, eating habits, etc. affect the incidence rate of heart disease"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#getting-started",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "For this exercise, we load the following R packages using the pacman::p_load() function\n\npacman::p_load(tidyverse, haven,\n               ggrepel, ggthemes,\n               ggridges, ggdist,\n               patchwork, scales,\n               corrr, ggcorrplot)\n\n\n\n\nIn this section, we need to read this data-set\n\ndf &lt;- read.csv(\"data/japan_heart_attack_dataset.csv\")\n\ncheck dataset structure and show the head data\n\nstr(df)\n\n'data.frame':   30000 obs. of  32 variables:\n $ Age                    : int  56 69 46 32 60 25 78 38 56 75 ...\n $ Gender                 : chr  \"Male\" \"Male\" \"Male\" \"Female\" ...\n $ Region                 : chr  \"Urban\" \"Urban\" \"Rural\" \"Urban\" ...\n $ Smoking_History        : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ Diabetes_History       : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ Hypertension_History   : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ Cholesterol_Level      : num  186 185 211 211 224 ...\n $ Physical_Activity      : chr  \"Moderate\" \"Low\" \"Low\" \"Moderate\" ...\n $ Diet_Quality           : chr  \"Poor\" \"Good\" \"Average\" \"Good\" ...\n $ Alcohol_Consumption    : chr  \"Low\" \"Low\" \"Moderate\" \"High\" ...\n $ Stress_Levels          : num  3.64 3.38 3.81 6.01 6.81 ...\n $ BMI                    : num  34 28.2 27.6 23.7 19.8 ...\n $ Heart_Rate             : num  72.3 57.5 64.7 55.1 76.7 ...\n $ Systolic_BP            : num  124 130 146 132 101 ...\n $ Diastolic_BP           : num  85.7 73.5 72 68.2 92.9 ...\n $ Family_History         : chr  \"No\" \"Yes\" \"No\" \"No\" ...\n $ Heart_Attack_Occurrence: chr  \"No\" \"No\" \"No\" \"No\" ...\n $ Extra_Column_1         : num  0.405 0.0363 0.853 0.3909 0.9336 ...\n $ Extra_Column_2         : num  0.433 0.513 0.22 0.297 0.976 ...\n $ Extra_Column_3         : num  0.629 0.668 0.613 0.156 0.174 ...\n $ Extra_Column_4         : num  0.702 0.116 0.508 0.87 0.295 ...\n $ Extra_Column_5         : num  0.498 0.424 0.901 0.39 0.959 ...\n $ Extra_Column_6         : num  0.0079 0.0839 0.2272 0.4032 0.6898 ...\n $ Extra_Column_7         : num  0.795 0.689 0.496 0.741 0.905 ...\n $ Extra_Column_8         : num  0.291 0.83 0.752 0.224 0.757 ...\n $ Extra_Column_9         : num  0.497 0.634 0.182 0.329 0.338 ...\n $ Extra_Column_10        : num  0.522 0.302 0.629 0.143 0.362 ...\n $ Extra_Column_11        : num  0.7997 0.0437 0.0183 0.9078 0.7286 ...\n $ Extra_Column_12        : num  0.7224 0.4517 0.0632 0.5423 0.1767 ...\n $ Extra_Column_13        : num  0.149 0.879 0.147 0.922 0.485 ...\n $ Extra_Column_14        : num  0.834 0.536 0.997 0.626 0.312 ...\n $ Extra_Column_15        : num  0.0616 0.6178 0.9745 0.2286 0.4528 ...\n\nhead(df)\n\n  Age Gender Region Smoking_History Diabetes_History Hypertension_History\n1  56   Male  Urban             Yes               No                   No\n2  69   Male  Urban              No               No                   No\n3  46   Male  Rural             Yes               No                   No\n4  32 Female  Urban              No               No                   No\n5  60 Female  Rural              No               No                   No\n6  25 Female  Rural              No               No                   No\n  Cholesterol_Level Physical_Activity Diet_Quality Alcohol_Consumption\n1          186.4002          Moderate         Poor                 Low\n2          185.1367               Low         Good                 Low\n3          210.6966               Low      Average            Moderate\n4          211.1655          Moderate         Good                High\n5          223.8143              High         Good                High\n6          220.3400               Low         Good                High\n  Stress_Levels      BMI Heart_Rate Systolic_BP Diastolic_BP Family_History\n1      3.644786 33.96135   72.30153    123.9021     85.68281             No\n2      3.384056 28.24287   57.45764    129.8933     73.52426            Yes\n3      3.810911 27.60121   64.65870    145.6549     71.99481             No\n4      6.014878 23.71729   55.13147    131.7852     68.21133             No\n5      6.806883 19.77158   76.66792    100.6946     92.90249             No\n6      8.207233 20.24744   67.66268    134.5966     73.14970             No\n  Heart_Attack_Occurrence Extra_Column_1 Extra_Column_2 Extra_Column_3\n1                      No     0.40498852      0.4333000      0.6287124\n2                      No     0.03627815      0.5125669      0.6683928\n3                      No     0.85297888      0.2195908      0.6134366\n4                      No     0.39085280      0.2968468      0.1557240\n5                      No     0.93356280      0.9756513      0.1737750\n6                      No     0.52447124      0.2328291      0.5885502\n  Extra_Column_4 Extra_Column_5 Extra_Column_6 Extra_Column_7 Extra_Column_8\n1      0.7016095      0.4981423    0.007901312      0.7945826      0.2907792\n2      0.1155287      0.4238194    0.083932768      0.6889511      0.8301636\n3      0.5080100      0.9006698    0.227205241      0.4963436      0.7521068\n4      0.8702514      0.3903559    0.403181621      0.7414089      0.2239681\n5      0.2953319      0.9594067    0.689787430      0.9045740      0.7570983\n6      0.4169773      0.1703778    0.448359048      0.3690557      0.1958173\n  Extra_Column_9 Extra_Column_10 Extra_Column_11 Extra_Column_12\n1      0.4971931       0.5219945      0.79965663      0.72239788\n2      0.6344903       0.3020434      0.04368285      0.45166789\n3      0.1815012       0.6291803      0.01827617      0.06322702\n4      0.3293139       0.1431905      0.90778075      0.54232201\n5      0.3377609       0.3623747      0.72855208      0.17669914\n6      0.3959485       0.8354301      0.00359574      0.94144789\n  Extra_Column_13 Extra_Column_14 Extra_Column_15\n1       0.1487387       0.8340099     0.061632229\n2       0.8786714       0.5356022     0.617825340\n3       0.1465122       0.9972962     0.974455410\n4       0.9224606       0.6262165     0.228606344\n5       0.4847487       0.3120910     0.452808843\n6       0.8757355       0.7186975     0.008745039\n\n\n\n\n\n\n\n\nThe dataset consists of 30,000 rows and 32 columns.\n\n\n\n\n1️⃣ Demographic Information:\n\nAge (years)\nGender (Male/Female)\nRegion (Geographical location)\n\n2️⃣ Health Conditions:\n\nSmoking_History (History of smoking)\nDiabetes_History (History of diabetes)\nHypertension_History (History of hypertension)\nCholesterol_Level (Cholesterol levels)\nBMI (Body Mass Index)\n\n3️⃣ Lifestyle Factors:\n\nPhysical_Activity (Level of physical activity)\nDiet_Quality (Dietary quality)\nAlcohol_Consumption (Alcohol intake)\n\n4️⃣ Physiological Measurements:\n\nHeart_Rate (Heart rate)\nSystolic_BP (Systolic blood pressure)\nDiastolic_BP (Diastolic blood pressure)\n\n5️⃣ Heart Attack Occurrence:\n\nHeart_Attack_Occurrence (Indicates whether a heart attack occurred)\n\nThis dataset provides a comprehensive view of demographic, health, lifestyle, and physiological factors that may influence heart attack occurrences. The analysis will explore potential correlations and risk factors associated with heart disease.\n\n\n\n\nI found the many columns-“Extra_column” in this dataset, we try to check. We first take a look at the data, and check if there are any duplicate entries.\n\nglimpse(df)\n\nRows: 30,000\nColumns: 32\n$ Age                     &lt;int&gt; 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36, 40…\n$ Gender                  &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"F…\n$ Region                  &lt;chr&gt; \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Rural\", \"…\n$ Smoking_History         &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"Y…\n$ Diabetes_History        &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Hypertension_History    &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Cholesterol_Level       &lt;dbl&gt; 186.4002, 185.1367, 210.6966, 211.1655, 223.81…\n$ Physical_Activity       &lt;chr&gt; \"Moderate\", \"Low\", \"Low\", \"Moderate\", \"High\", …\n$ Diet_Quality            &lt;chr&gt; \"Poor\", \"Good\", \"Average\", \"Good\", \"Good\", \"Go…\n$ Alcohol_Consumption     &lt;chr&gt; \"Low\", \"Low\", \"Moderate\", \"High\", \"High\", \"Hig…\n$ Stress_Levels           &lt;dbl&gt; 3.644786, 3.384056, 3.810911, 6.014878, 6.8068…\n$ BMI                     &lt;dbl&gt; 33.96135, 28.24287, 27.60121, 23.71729, 19.771…\n$ Heart_Rate              &lt;dbl&gt; 72.30153, 57.45764, 64.65870, 55.13147, 76.667…\n$ Systolic_BP             &lt;dbl&gt; 123.90209, 129.89331, 145.65490, 131.78522, 10…\n$ Diastolic_BP            &lt;dbl&gt; 85.68281, 73.52426, 71.99481, 68.21133, 92.902…\n$ Family_History          &lt;chr&gt; \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ Heart_Attack_Occurrence &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ Extra_Column_1          &lt;dbl&gt; 0.40498852, 0.03627815, 0.85297888, 0.39085280…\n$ Extra_Column_2          &lt;dbl&gt; 0.43330004, 0.51256694, 0.21959083, 0.29684675…\n$ Extra_Column_3          &lt;dbl&gt; 0.62871236, 0.66839275, 0.61343656, 0.15572404…\n$ Extra_Column_4          &lt;dbl&gt; 0.70160955, 0.11552874, 0.50800995, 0.87025144…\n$ Extra_Column_5          &lt;dbl&gt; 0.49814235, 0.42381938, 0.90066981, 0.39035591…\n$ Extra_Column_6          &lt;dbl&gt; 0.007901312, 0.083932768, 0.227205241, 0.40318…\n$ Extra_Column_7          &lt;dbl&gt; 0.79458257, 0.68895108, 0.49634358, 0.74140891…\n$ Extra_Column_8          &lt;dbl&gt; 0.29077922, 0.83016364, 0.75210679, 0.22396813…\n$ Extra_Column_9          &lt;dbl&gt; 0.49719307, 0.63449028, 0.18150125, 0.32931387…\n$ Extra_Column_10         &lt;dbl&gt; 0.52199452, 0.30204337, 0.62918031, 0.14319054…\n$ Extra_Column_11         &lt;dbl&gt; 0.79965663, 0.04368285, 0.01827617, 0.90778075…\n$ Extra_Column_12         &lt;dbl&gt; 0.72239788, 0.45166789, 0.06322702, 0.54232201…\n$ Extra_Column_13         &lt;dbl&gt; 0.1487387, 0.8786714, 0.1465122, 0.9224606, 0.…\n$ Extra_Column_14         &lt;dbl&gt; 0.8340099, 0.5356022, 0.9972962, 0.6262165, 0.…\n$ Extra_Column_15         &lt;dbl&gt; 0.061632229, 0.617825340, 0.974455410, 0.22860…\n\n\nDisplay only the first few rows after removing the Extra_Column.\n\nhead(df %&gt;% select(-starts_with(\"Extra_Column\")))\n\n  Age Gender Region Smoking_History Diabetes_History Hypertension_History\n1  56   Male  Urban             Yes               No                   No\n2  69   Male  Urban              No               No                   No\n3  46   Male  Rural             Yes               No                   No\n4  32 Female  Urban              No               No                   No\n5  60 Female  Rural              No               No                   No\n6  25 Female  Rural              No               No                   No\n  Cholesterol_Level Physical_Activity Diet_Quality Alcohol_Consumption\n1          186.4002          Moderate         Poor                 Low\n2          185.1367               Low         Good                 Low\n3          210.6966               Low      Average            Moderate\n4          211.1655          Moderate         Good                High\n5          223.8143              High         Good                High\n6          220.3400               Low         Good                High\n  Stress_Levels      BMI Heart_Rate Systolic_BP Diastolic_BP Family_History\n1      3.644786 33.96135   72.30153    123.9021     85.68281             No\n2      3.384056 28.24287   57.45764    129.8933     73.52426            Yes\n3      3.810911 27.60121   64.65870    145.6549     71.99481             No\n4      6.014878 23.71729   55.13147    131.7852     68.21133             No\n5      6.806883 19.77158   76.66792    100.6946     92.90249             No\n6      8.207233 20.24744   67.66268    134.5966     73.14970             No\n  Heart_Attack_Occurrence\n1                      No\n2                      No\n3                      No\n4                      No\n5                      No\n6                      No\n\n\n\n\nUsing the duplicated function, we see that there are no duplicate entries in the data.\n\ndf[duplicated(df),]\n\n [1] Age                     Gender                  Region                 \n [4] Smoking_History         Diabetes_History        Hypertension_History   \n [7] Cholesterol_Level       Physical_Activity       Diet_Quality           \n[10] Alcohol_Consumption     Stress_Levels           BMI                    \n[13] Heart_Rate              Systolic_BP             Diastolic_BP           \n[16] Family_History          Heart_Attack_Occurrence Extra_Column_1         \n[19] Extra_Column_2          Extra_Column_3          Extra_Column_4         \n[22] Extra_Column_5          Extra_Column_6          Extra_Column_7         \n[25] Extra_Column_8          Extra_Column_9          Extra_Column_10        \n[28] Extra_Column_11         Extra_Column_12         Extra_Column_13        \n[31] Extra_Column_14         Extra_Column_15        \n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\n\n\ncolSums(is.na(df))\n\n                    Age                  Gender                  Region \n                      0                       0                       0 \n        Smoking_History        Diabetes_History    Hypertension_History \n                      0                       0                       0 \n      Cholesterol_Level       Physical_Activity            Diet_Quality \n                      0                       0                       0 \n    Alcohol_Consumption           Stress_Levels                     BMI \n                      0                       0                       0 \n             Heart_Rate             Systolic_BP            Diastolic_BP \n                      0                       0                       0 \n         Family_History Heart_Attack_Occurrence          Extra_Column_1 \n                      0                       0                       0 \n         Extra_Column_2          Extra_Column_3          Extra_Column_4 \n                      0                       0                       0 \n         Extra_Column_5          Extra_Column_6          Extra_Column_7 \n                      0                       0                       0 \n         Extra_Column_8          Extra_Column_9         Extra_Column_10 \n                      0                       0                       0 \n        Extra_Column_11         Extra_Column_12         Extra_Column_13 \n                      0                       0                       0 \n        Extra_Column_14         Extra_Column_15 \n                      0                       0 \n\n\n✅ No missing values in the data, ready for use.\n✅ No duplicate entries, no need for deduplication.\n✅ Ready for data visualization and analysis."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda-1-heart-attack-occurrence-by-age-distribution",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda-1-heart-attack-occurrence-by-age-distribution",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "This stacked histogram visualizes the distribution of heart attack occurrences across different age groups.\nThe x-axis (Age) represents the age of individuals.The y-axis (Count) represents the number of individuals in each age group.\nThe bars are color-coded:Blue (“No”) represents individuals who did not experience a heart attack.Red (“Yes”) represents individuals who experienced a heart attack.\n\nggplot(df, aes(x = Age, fill = Heart_Attack_Occurrence)) +\n  geom_histogram(binwidth = 5, color = \"black\", alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Heart Attack Occurrence by Age Distribution\",\n       x = \"Age\",\n       y = \"Count\") +\n  scale_fill_manual(values = c(\"No\" = \"blue\", \"Yes\" = \"red\"))\n\n\n\n\n\n\n\n\n\n\n\nThe dataset has a relatively uniform distribution of individuals across different age groups, except for a slight drop in count near age 80.\nHeart attacks (red section) appear in all age groups, but their proportion increases slightly with age.\nYounger individuals (below 40) have a lower incidence of heart attacks, whereas older individuals (50+) show a higher proportion of heart attack occurrences.\n\nThis visualization effectively highlights the relationship between age and heart attack occurrence, showing a general trend that older individuals are more likely to experience heart attacks."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda-2-heart-attack-occurrence-by-age-group",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda-2-heart-attack-occurrence-by-age-group",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "We could find there is a relationship between age and heart attack occurrence. So we can try to analysis the different heart attack occurrence from different age group.\nFirstly, we define the age group, then we can calculate the heart attack rate, and finally we attempt to draw a stacked bar chart that allows us to directly observe the relationship between age and heart attack rate.\n\ndf_summary &lt;- df %&gt;%\n  mutate(Age_Group = cut(Age, breaks = c(18, 35, 50, 65, 80), \n                         labels = c(\"18-35\", \"36-50\", \"51-65\", \"66-80\"),\n                         right = FALSE)) %&gt;%\n  group_by(Age_Group, Heart_Attack_Occurrence) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'Age_Group'. You can override using the\n`.groups` argument.\n\nggplot(df_summary, aes(x = Age_Group, y = Count, fill = Heart_Attack_Occurrence)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +  \n  geom_text(aes(label = Count), \n            position = position_dodge(width = 0.8), \n            vjust = -0.5, size = 3) +  \n  theme_minimal() +\n  labs(title = \"Heart Attack Occurrence by Age Group\",\n       x = \"Age Group\",\n       y = \"Count\") +\n  scale_fill_manual(values = c(\"No\" = \"blue\", \"Yes\" = \"red\"))\n\n\n\n\n\n\n\n\n\n\nThe total number of individuals decreases slightly as age increases.\n\n18-35 age group has the highest count (7487 individuals).\n36-50, 51-65, and 66-80 age groups have relatively similar numbers (~6500-6600 individuals).\nThe absolute count of heart attack occurrences (red bars) is relatively stable across all age groups:\n\n18-35: 779 cases\n36-50: 733 cases\n51-65: 740 cases\n66-80: 712 cases\n\n\n\nThe absolute number of heart attacks does not show a significant increase with age.\nHowever, this does not account for population size differences. We need to look at heart attack rates for a better interpretation.\n\n\n\ndf_rate &lt;- df %&gt;%\n  mutate(Age_Group = cut(Age, breaks = c(18, 35, 50, 65, 80), \n                         labels = c(\"18-35\", \"36-50\", \"51-65\", \"66-80\"),\n                         right = FALSE)) %&gt;%\n  group_by(Age_Group) %&gt;%\n  summarise(Heart_Attack_Rate = mean(Heart_Attack_Occurrence == \"Yes\")) %&gt;%\n  ungroup()\n\nggplot(df_rate, aes(x = Age_Group, y = Heart_Attack_Rate, group = 1)) +\n  geom_line(color = \"red\", size = 1) +  \n  geom_point(color = \"red\", size = 3) +  \n  geom_text(aes(label = round(Heart_Attack_Rate, 3)),  \n            vjust = -0.4, size = 3, color = \"black\") +  \n  theme_minimal() +\n  labs(title = \"Heart Attack Rate by Age Group\",\n       x = \"Age Group\",\n       y = \"Heart Attack Rate\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nHeart attack rate is not strictly increasing with age.\n\nThe 18-35 group has the lowest rate (0.094).\nThe rate increases in the 36-50 group (0.101 or 10.1%) and peaks at 51-65 (0.104).\nSurprisingly, the 66-80 group sees a slight decline (0.097) in heart attack rate.\nThe highest heart attack risk is in the 51-65 age group.\n\n\n\n\n✅ Heart attack risk is lowest in the youngest group (18-35) and peaks at 51-65.\n✅ The absolute number of heart attacks remains relatively stable across age groups.\n✅ The oldest group (66-80) shows a slight decrease in heart attack rate, possibly due to survivor bias or better health management."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda-2-the-relationship-between-gender-and-heart-attack",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda-2-the-relationship-between-gender-and-heart-attack",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "df_gender_rate &lt;- df %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(Heart_Attack_Rate = mean(Heart_Attack_Occurrence == \"Yes\"))\n\nggplot(df_gender_rate, aes(x = Gender, y = Heart_Attack_Rate, fill = Gender)) +\n  geom_bar(stat = \"identity\") +  \n  geom_text(aes(label = round(Heart_Attack_Rate, 3)), vjust = -0.5) +\n  theme_minimal() +\n  labs(title = \"Heart Attack Rate by Gender\",\n       x = \"Gender\",\n       y = \"Heart Attack Rate\")\n\n\n\n\n\n\n\n\n\n\nHeart Attack Rate Comparison\nThe heart attack rate for males (0.102) is slightly higher than for females (0.096). The difference is small but noticeable, indicating that gender may have a minor impact on heart attack occurrence.\nGender-Specific Risk\nMales tend to have a slightly higher likelihood of experiencing heart attacks. Females have a lower but still significant heart attack rate"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda3-calculate-the-incidence-rate-of-heart-disease-in-different-regions",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda3-calculate-the-incidence-rate-of-heart-disease-in-different-regions",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "region_rate &lt;- df %&gt;%\n  group_by(Region) %&gt;%\n  summarise(Heart_Attack_Rate = mean(Heart_Attack_Occurrence == \"Yes\")) %&gt;%\n  arrange(desc(Heart_Attack_Rate))  \n\n\nggplot(region_rate, aes(x = reorder(Region, -Heart_Attack_Rate), y = Heart_Attack_Rate, fill = Region)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = round(Heart_Attack_Rate, 4)), \n            vjust = -0.5, size = 4, color = \"black\") +  \n  theme_minimal() +\n  labs(title = \"Heart Attack Rate by Region\",\n       x = \"Region\",\n       y = \"Heart Attack Rate\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nFrom the above graph, the heart attack occurrence in Urban and Rural is the noticeable difference."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda4-the-relationship-between-health-conditions-and-heart-attack-occurrence",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda4-the-relationship-between-health-conditions-and-heart-attack-occurrence",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "We need to compare the heart attack rate between individuals who have the condition and those who don’t.\n\n\n\n# Calculate heart attack rates for each health factor\nhealth_factors &lt;- df %&gt;%\n  summarise(\n    Smoking_Yes = mean(Heart_Attack_Occurrence[Smoking_History == \"Yes\"] == \"Yes\"),\n    Smoking_No = mean(Heart_Attack_Occurrence[Smoking_History == \"No\"] == \"Yes\"),\n    \n    Diabetes_Yes = mean(Heart_Attack_Occurrence[Diabetes_History == \"Yes\"] == \"Yes\"),\n    Diabetes_No = mean(Heart_Attack_Occurrence[Diabetes_History == \"No\"] == \"Yes\"),\n    \n    Hypertension_Yes = mean(Heart_Attack_Occurrence[Hypertension_History == \"Yes\"] == \"Yes\"),\n    Hypertension_No = mean(Heart_Attack_Occurrence[Hypertension_History == \"No\"] == \"Yes\")\n  ) \n\n# Convert to long format for visualization\nhealth_factors_long &lt;- tidyr::pivot_longer(health_factors, \n                                           cols = everything(), \n                                           names_to = c(\"Condition\", \"Group\"),\n                                           names_sep = \"_\",\n                                           values_to = \"Heart_Attack_Rate\")\n\nprint(health_factors_long)\n\n# A tibble: 6 × 3\n  Condition    Group Heart_Attack_Rate\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt;\n1 Smoking      Yes              0.102 \n2 Smoking      No               0.0974\n3 Diabetes     Yes              0.103 \n4 Diabetes     No               0.0976\n5 Hypertension Yes              0.101 \n6 Hypertension No               0.0982\n\n\n\n# Plot heart attack rate for each health condition with values displayed\nggplot(health_factors_long, aes(x = Group, y = Heart_Attack_Rate, fill = Group)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = round(Heart_Attack_Rate, 4)),  # Display values rounded to 4 decimal places\n            vjust = -0.5, size = 4, color = \"black\") +\n  facet_wrap(~Condition, scales = \"free_y\") +  # Create separate graphs for each condition\n  theme_minimal() +\n  labs(title = \"Impact of Health Conditions on Heart Attack Occurrence\",\n       x = \"Health Condition Group\",\n       y = \"Heart Attack Rate\") +\n  scale_fill_manual(values = c(\"Yes\" = \"red\", \"No\" = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\nHigher Heart Attack Rates for Individuals with Health Conditions:\nThe heart attack occurrence rate is slightly higher for individuals with a history of smoking, diabetes, and hypertension compared to those without.\nSpecifically:\nSmoking: 10.2% for smokers vs. 9.74% for non-smokers.\nDiabetes: 10.33% for individuals with diabetes vs. 9.76% for those without.\nHypertension: 10.08% for individuals with hypertension vs. 9.82% for those without.\nThe findings suggest that smoking, diabetes, and hypertension slightly increase the likelihood of a heart attack, with diabetes having the most significant impact.\n\n\n\n\n# Compare Cholesterol Level and BMI for Heart Attack vs. No Heart Attack\nnumerical_health &lt;- df %&gt;%\n  group_by(Heart_Attack_Occurrence) %&gt;%\n  summarise(\n    Avg_Cholesterol = mean(Cholesterol_Level, na.rm = TRUE),\n    Avg_BMI = mean(BMI, na.rm = TRUE)\n  )\n\nprint(numerical_health)\n\n# A tibble: 2 × 3\n  Heart_Attack_Occurrence Avg_Cholesterol Avg_BMI\n  &lt;chr&gt;                             &lt;dbl&gt;   &lt;dbl&gt;\n1 No                                 200.    25.0\n2 Yes                                200.    24.9\n\n# Create boxplots for cholesterol and BMI\ndf_long &lt;- df %&gt;%\n  tidyr::pivot_longer(cols = c(Cholesterol_Level, BMI), \n                      names_to = \"Health_Metric\",\n                      values_to = \"Value\")\n\nggplot(df_long, aes(x = Heart_Attack_Occurrence, y = Value, fill = Heart_Attack_Occurrence)) +\n  geom_boxplot() +\n  facet_wrap(~Health_Metric, scales = \"free\") + \n  theme_minimal() +\n  labs(title = \"Comparison of Cholesterol Level and BMI in Heart Attack Cases\",\n       x = \"Heart Attack Occurrence\",\n       y = \"Value\") +\n  scale_fill_manual(values = c(\"Yes\" = \"red\", \"No\" = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\nThe average cholesterol levels (199.91 vs. 199.79) show almost no difference, suggesting that cholesterol level alone may not be a strong distinguishing factor for heart attack risk.\nThe average BMI values (25.01 vs. 24.91) are very close, indicating no significant difference between the two groups.\nCholesterol levels and BMI are nearly identical between those who experienced a heart attack and those who did not, suggesting they might not be the primary driving factors for heart attacks in this dataset.\n\n\n\n\ndf_numeric &lt;- df %&gt;%\n  mutate(\n    Heart_Attack_Occurrence = ifelse(Heart_Attack_Occurrence == \"Yes\", 1, 0),\n    Smoking_History = ifelse(Smoking_History == \"Yes\", 1, 0),\n    Diabetes_History = ifelse(Diabetes_History == \"Yes\", 1, 0),\n    Hypertension_History = ifelse(Hypertension_History == \"Yes\", 1, 0)\n  ) %&gt;%\n  select(Heart_Attack_Occurrence, Smoking_History, Diabetes_History, Hypertension_History, \n         Cholesterol_Level, BMI)  \n\n\ndf_numeric &lt;- df_numeric %&gt;% na.omit()\n\n\ncor_matrix &lt;- cor(df_numeric, method = \"pearson\")\n\n\nggcorrplot(cor_matrix, lab = TRUE, hc.order = TRUE, type = \"lower\", colors = c(\"blue\", \"white\", \"red\"))\n\n\n\n\n\n\n\n\n\ndf &lt;- df %&gt;%\n  mutate(\n    Heart_Attack_Occurrence = ifelse(Heart_Attack_Occurrence == \"Yes\", 1, 0),\n    Smoking_History = ifelse(Smoking_History == \"Yes\", 1, 0),\n    Diabetes_History = ifelse(Diabetes_History == \"Yes\", 1, 0),\n    Hypertension_History = ifelse(Hypertension_History == \"Yes\", 1, 0),\n    Cholesterol_Level = scale(Cholesterol_Level),  \n    BMI = scale(BMI)  \n  )\n\n\nmodel &lt;- glm(Heart_Attack_Occurrence ~ Smoking_History + Diabetes_History + Hypertension_History + \n                                       Cholesterol_Level + BMI, \n             data = df, family = binomial())\n\nsummary(model)\n\n\nCall:\nglm(formula = Heart_Attack_Occurrence ~ Smoking_History + Diabetes_History + \n    Hypertension_History + Cholesterol_Level + BMI, family = binomial(), \n    data = df)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -2.24672    0.02773 -81.015   &lt;2e-16 ***\nSmoking_History       0.05125    0.04189   1.223    0.221    \nDiabetes_History      0.06251    0.04739   1.319    0.187    \nHypertension_History  0.02956    0.04452   0.664    0.507    \nCholesterol_Level    -0.00360    0.01935  -0.186    0.852    \nBMI                  -0.01728    0.01935  -0.893    0.372    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 19346  on 29999  degrees of freedom\nResidual deviance: 19342  on 29994  degrees of freedom\nAIC: 19354\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nCorrelation Heatmap:\nThe heatmap shows very weak correlations between heart attack occurrence and other health factors.\nAll correlation values are close to zero, suggesting that none of the selected health factors strongly correlate with heart attack occurrence.\nThe highest correlation is only 0.01, which is negligible.\nThere are also weak correlations among other health-related variables, implying that these factors do not exhibit strong interdependencies in this dataset.\nLogistic Regression Results:\nThe logistic regression model was used to analyze the relationship between health factors and heart attack occurrence.\nNone of the predictor variables (Smoking History, Diabetes History, Hypertension History, Cholesterol Level, BMI) were statistically significant in predicting heart attack occurrence (p-values &gt; 0.05 for all variables).\nThe intercept is significant, indicating that the base probability of heart attack occurrence (without considering the predictor variables) is non-trivial.\nThe estimated coefficients suggest that:Smoking history and diabetes history have positive coefficients, indicating a slight increase in heart attack occurrence. Cholesterol level and BMI have negative coefficients.\nBoth the logistic regression model and the correlation analysis suggest that the selected health factors (Smoking, Diabetes, Hypertension, Cholesterol Level, and BMI) do not significantly contribute to predicting heart attack occurrence in this dataset."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda5-the-relationship-between-stress-level-and-heart-attack-occurrence",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda5-the-relationship-between-stress-level-and-heart-attack-occurrence",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "df &lt;- read.csv(\"data/japan_heart_attack_dataset.csv\")\n\ndf &lt;- df %&gt;%\n  mutate(Heart_Attack_Occurrence = ifelse(Heart_Attack_Occurrence == \"Yes\", 1, 0))\n\n\ndf &lt;- df %&gt;%\n  mutate(Stress_Category = case_when(\n    Stress_Levels &gt;= 0 & Stress_Levels &lt;= 3 ~ \"Low\",\n    Stress_Levels &gt; 3 & Stress_Levels &lt;= 6 ~ \"Moderate\",\n    Stress_Levels &gt; 6 & Stress_Levels &lt;= 10 ~ \"High\"\n  ))\n\n\nstress_impact &lt;- df %&gt;%\n  group_by(Stress_Category) %&gt;%\n  summarise(\n    Total_Count = n(),  \n    Heart_Attack_Count = sum(Heart_Attack_Occurrence),  \n    Heart_Attack_Rate = mean(Heart_Attack_Occurrence)  \n  )\n\nprint(stress_impact)\n\n# A tibble: 3 × 4\n  Stress_Category Total_Count Heart_Attack_Count Heart_Attack_Rate\n  &lt;chr&gt;                 &lt;int&gt;              &lt;dbl&gt;             &lt;dbl&gt;\n1 High                   9279                874            0.0942\n2 Low                    4698                504            0.107 \n3 Moderate              16023               1586            0.0990\n\nggplot(stress_impact, aes(x = Stress_Category, y = Heart_Attack_Rate, group = 1)) +\n  geom_line(color = \"red\", size = 1) +  \n  geom_point(color = \"red\", size = 3) +  \n  geom_text(aes(label = round(Heart_Attack_Rate, 4)),  \n            vjust = -0.5, size = 5, color = \"black\") +  \n  theme_minimal() +\n  labs(title = \"Heart Attack Rate by Stress Level\",\n       x = \"Stress Level Category\",\n       y = \"Heart Attack Rate\")\n\n\n\n\n\n\n\n\nHeart Attack Rate Across Stress Levels:\nThe heart attack rate is highest in the low stress category (10.73%).\nThe moderate stress category has a slightly lower heart attack rate (9.99%).\nThe high stress category has the lowest heart attack rate (9.42%)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda6-exploring-the-impact-of-lifestyle-level-of-physical-activitydietary-qualityalcohol-intake-on-the-onset-of-heart-disease",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda6-exploring-the-impact-of-lifestyle-level-of-physical-activitydietary-qualityalcohol-intake-on-the-onset-of-heart-disease",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "df &lt;- df %&gt;%\n  mutate(\n    Physical_Activity = factor(Physical_Activity, levels = c(\"Low\", \"Moderate\", \"High\")),\n    Heart_Attack_Occurrence = as.numeric(Heart_Attack_Occurrence == \"Yes\")\n  )\n\n\nggplot(df, aes(x = Heart_Attack_Occurrence, y = Physical_Activity, fill = ..x..)) +\n  geom_density_ridges_gradient(scale = 2, rel_min_height = 0.01) +\n  scale_fill_viridis_c() +  \n  theme_minimal() +\n  labs(title = \"Heart Attack Occurrence Distribution by Physical Activity Level\",\n       x = \"Heart Attack Occurrence Probability\",\n       y = \"Physical Activity Level\",\n       fill = \"Probability\")\n\nWarning: The dot-dot notation (`..x..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(x)` instead.\n\n\nPicking joint bandwidth of 0.143\n\n\n\n\n\n\n\n\n\nThe distribution of heart attack occurrence probability is similar across different physical activity levels (High, Moderate, Low).\n\n\n\n\ndf &lt;- df %&gt;%\n  mutate(\n    Diet_Quality = factor(Diet_Quality, levels = c(\"Poor\", \"Average\", \"Good\")),\n    Heart_Attack_Occurrence = as.numeric(Heart_Attack_Occurrence == \"Yes\")\n  )\n\n\nggplot(df, aes(x = Heart_Attack_Occurrence, y = Diet_Quality, fill = ..x..)) +\n  geom_density_ridges_gradient(scale = 2, rel_min_height = 0.01) +\n  scale_fill_viridis_c() +  \n  theme_minimal() +\n  labs(title = \"Heart Attack Occurrence Distribution by Diet Quality\",\n       x = \"Heart Attack Occurrence Probability\",\n       y = \"Diet Quality\",\n       fill = \"Probability\")\n\nPicking joint bandwidth of 0.144\n\n\n\n\n\n\n\n\n\nAll three categories show overlapping probability distributions, meaning diet quality alone may not be a decisive factor for heart attack occurrence.\n\ndf &lt;- df %&gt;%\n  mutate(\n    Alcohol_Consumption = factor(Alcohol_Consumption, levels = c(\"None\", \"Low\", \"Moderate\", \"High\")),\n    Heart_Attack_Occurrence = as.numeric(Heart_Attack_Occurrence == \"Yes\")\n  )\n\n\nggplot(df, aes(x = Heart_Attack_Occurrence, y = Alcohol_Consumption, fill = ..x..)) +\n  geom_density_ridges_gradient(scale = 2, rel_min_height = 0.01) +\n  scale_fill_viridis_c() +  \n  theme_minimal() +\n  labs(title = \"Heart Attack Occurrence Distribution by Alcohol Consumption Level\",\n       x = \"Heart Attack Occurrence Probability\",\n       y = \"Alcohol Consumption Level\",\n       fill = \"Probability\")\n\nPicking joint bandwidth of 0.156\n\n\n\n\n\n\n\n\n\nThe distribution remains centered around a similar probability range for all levels of alcohol consumption."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#summary-and-conclusion",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#summary-and-conclusion",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "In this analysis, we explored various factors related to heart attack occurrence, including age, gender, lifestyle choices, and health conditions. Through data visualization and statistical modeling, we examined their potential impact on heart disease risk. The key findings are:\nAge and Heart Attack Risk: Heart attack occurrence is not strictly increasing with age. While the absolute number of cases is relatively stable across age groups, the heart attack rate is lowest in the youngest group (18-35) and peaks in the 51-65 age group before slightly decreasing in the oldest (66-80) group.\nGender Influence: Males have a slightly higher heart attack rate (10.2%) compared to females (9.6%). Though the difference is small, it suggests a minor gender-based variation in heart attack susceptibility.\nImpact of Health Conditions: Individuals with a history of smoking, diabetes, and hypertension show slightly higher heart attack rates compared to those without these conditions. Among these, diabetes has the most noticeable effect, with a heart attack rate of 10.33% for diabetics versus 9.76% for non-diabetics.\nCholesterol and BMI Effects: The average cholesterol levels and BMI values between individuals who experienced heart attacks and those who did not are nearly identical.\nLifestyle Factors: The influence of physical activity, diet quality, and alcohol consumption on heart attack occurrence appears to be minimal. The probability distributions for heart attack occurrences remain similar across all categories within these factors, suggesting that none of them are strong standalone predictors of heart attacks.\nGiven the weak correlations and the non-significant predictors in logistic regression, further studies with more comprehensive datasets and additional health indicators may be necessary to develop a more accurate risk assessment model for heart attacks. Statistical hypothesis testing and more advanced modeling techniques could provide deeper insights into the complex relationships between these factors and heart disease risk.\nAt last, this exercise was a helpful way to practice presenting data effectively, and create data visualization."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#reference",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#reference",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "Code Book\nLEW YING ZHEN SERENA"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "In-class_Ex04",
    "section": "",
    "text": "pacman::p_load(haven, SmartEDA, ggdist, ggridges, ggthemes,\n               colorspace, ggstatsplot, tidyverse)\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nggplot(data = exam_data,\n      aes(x = ENGLISH,\n          y = CLASS)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nchurn_telecom &lt;- read_sas(\"data/churn_telecom.sas7bdat\") %&gt;% \n  mutate(CHURN_FLG = factor(\n    CHURN_FLG,\n    levels = c(\"1\",\"0\"),\n    labels = c(\"churn\", \"Non-Churn\")))\n\n\nggplot(data = churn_telecom,\n       aes(x = CHURN_FLG)) + geom_bar()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, you will gain hands-on experience on using R to plot static and interactive heatmap for visualising and analysing multivariate data.\n\n\n\nBefore you get started, you are required to open a new Quarto document. Keep the default html as the authoring format.\nNext, you will use the code chunk below to install and launch seriation, heatmaply, dendextend and tidyverse in RStudio.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)\n\n\n\n\nIn this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv\n\n\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nRows: 156 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Country, Region\ndbl (10): Happiness score, Whisker-high, Whisker-low, Dystopia, GDP per capi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe output tibbled data frame is called wh.\n\n\n\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\nWarning: Setting row names on a tibble is deprecated.\n\n\n\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\n\n\n\n\nThere are many R packages and functions can be used to drawing static heatmaps, they are:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nIn this section, you will learn how to plot static heatmaps by using heatmap() of R Stats package.\n\n\nIn this sub-section, we will plot a heatmap by using heatmap() of Base Stats. The code chunk is given below.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\n\nNote:\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\n\n\n\nNote:\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively.\n\n\n\n\nheatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nBefore we get started, you should review the Introduction to Heatmaply to have an overall understanding of the features and functions of Heatmaply package. You are also required to have the user manualof the package handy with you for reference purposes.\nIn this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n\n\nheatmaply(mtcars)\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\nNote that:\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\n\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\nRegistered S3 method overwritten by 'gclus':\n  method         from     \n  reorder.hclust seriation\n\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#overview",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, you will gain hands-on experience on using R to plot static and interactive heatmap for visualising and analysing multivariate data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "Before you get started, you are required to open a new Quarto document. Keep the default html as the authoring format.\nNext, you will use the code chunk below to install and launch seriation, heatmaply, dendextend and tidyverse in RStudio.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#importing-and-preparing-the-data-set",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#importing-and-preparing-the-data-set",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "In this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv\n\n\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nRows: 156 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Country, Region\ndbl (10): Happiness score, Whisker-high, Whisker-low, Dystopia, GDP per capi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe output tibbled data frame is called wh.\n\n\n\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\nWarning: Setting row names on a tibble is deprecated.\n\n\n\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#static-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#static-heatmap",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "There are many R packages and functions can be used to drawing static heatmaps, they are:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nIn this section, you will learn how to plot static heatmaps by using heatmap() of R Stats package.\n\n\nIn this sub-section, we will plot a heatmap by using heatmap() of Base Stats. The code chunk is given below.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\n\nNote:\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\n\n\n\nNote:\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#creating-interactive-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#creating-interactive-heatmap",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "heatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nBefore we get started, you should review the Introduction to Heatmaply to have an overall understanding of the features and functions of Heatmaply package. You are also required to have the user manualof the package handy with you for reference purposes.\nIn this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n\n\nheatmaply(mtcars)\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\nNote that:\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\n\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\nRegistered S3 method overwritten by 'gclus':\n  method         from     \n  reorder.hclust seriation\n\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d/Hands-on_Ex04d.html",
    "href": "Hands-on_Ex/Hands-on_Ex04d/Hands-on_Ex04d.html",
    "title": "Hands-on Exercise 4d",
    "section": "",
    "text": "Funnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. By the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting funnel plots by using funnelPlotR package,\nplotting static funnel plot by using ggplot2 package, and\nplotting interactive funnel plot by using both plotly R and ggplot2 packages.\n\n\n\n\nIn this exercise, four R packages will be used. They are:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\n\n\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)\n\nRows: 267 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): City, District, Sub-district\ndbl (4): Sub-district ID, Positive, Recovered, Death\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Positive,\n  denominator = Death,\n  group = `Sub-district`\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 0 are outliers.  Plot is adjusted for overdispersion. \nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\nWarning: The `xrange` argument deprecated; please use the `x_range` argument\ninstead.  For more options, see the help: `?funnel_plot`\n\n\nWarning: The `yrange` argument deprecated; please use the `y_range` argument\ninstead.  For more options, see the help: `?funnel_plot`\n\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 7 are outliers. Plot is adjusted for overdispersion.\nThings to learn from the code chunk above. + data_type argument is used to change from default “SR” to “PR” (i.e. proportions). + xrange and yrange are used to set the range of x-axis and y-axis\n\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\nWarning: The `xrange` argument deprecated; please use the `x_range` argument\ninstead.  For more options, see the help: `?funnel_plot`\n\n\nWarning: The `yrange` argument deprecated; please use the `y_range` argument\ninstead.  For more options, see the help: `?funnel_plot`\n\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 7 are outliers. Plot is adjusted for overdispersion.\nThings to learn from the code chunk above.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles.\n\n\n\n\n\nIn this section, you will gain hands-on experience on building funnel plots step-by-step by using ggplot2. It aims to enhance you working experience of ggplot2 to customise speciallised data visualisation like funnel plot.\n\n\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\n\nWarning in geom_point(aes(label = `Sub-district`), alpha = 0.4): Ignoring\nunknown aesthetics: label\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\np\n\n\n\n\n\n\n\n\n\n\n\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly\n\n\n\n\n\n\n\n\n\n\nfunnelPlotR package.\nFunnel Plots for Indirectly-standardised ratios.\nChanging funnel plot options\nggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d/Hands-on_Ex04d.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex04d/Hands-on_Ex04d.html#overview",
    "title": "Hands-on Exercise 4d",
    "section": "",
    "text": "Funnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. By the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting funnel plots by using funnelPlotR package,\nplotting static funnel plot by using ggplot2 package, and\nplotting interactive funnel plot by using both plotly R and ggplot2 packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d/Hands-on_Ex04d.html#installing-and-launching-r-package",
    "href": "Hands-on_Ex/Hands-on_Ex04d/Hands-on_Ex04d.html#installing-and-launching-r-package",
    "title": "Hands-on Exercise 4d",
    "section": "",
    "text": "In this exercise, four R packages will be used. They are:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d/Hands-on_Ex04d.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex04d/Hands-on_Ex04d.html#importing-data",
    "title": "Hands-on Exercise 4d",
    "section": "",
    "text": "In this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)\n\nRows: 267 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): City, District, Sub-district\ndbl (4): Sub-district ID, Positive, Recovered, Death\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d/Hands-on_Ex04d.html#funnelplotr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04d/Hands-on_Ex04d.html#funnelplotr-methods",
    "title": "Hands-on Exercise 4d",
    "section": "",
    "text": "FunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Positive,\n  denominator = Death,\n  group = `Sub-district`\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 0 are outliers.  Plot is adjusted for overdispersion. \nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\nWarning: The `xrange` argument deprecated; please use the `x_range` argument\ninstead.  For more options, see the help: `?funnel_plot`\n\n\nWarning: The `yrange` argument deprecated; please use the `y_range` argument\ninstead.  For more options, see the help: `?funnel_plot`\n\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 7 are outliers. Plot is adjusted for overdispersion.\nThings to learn from the code chunk above. + data_type argument is used to change from default “SR” to “PR” (i.e. proportions). + xrange and yrange are used to set the range of x-axis and y-axis\n\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\nWarning: The `xrange` argument deprecated; please use the `x_range` argument\ninstead.  For more options, see the help: `?funnel_plot`\n\n\nWarning: The `yrange` argument deprecated; please use the `y_range` argument\ninstead.  For more options, see the help: `?funnel_plot`\n\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 7 are outliers. Plot is adjusted for overdispersion.\nThings to learn from the code chunk above.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d/Hands-on_Ex04d.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04d/Hands-on_Ex04d.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on Exercise 4d",
    "section": "",
    "text": "In this section, you will gain hands-on experience on building funnel plots step-by-step by using ggplot2. It aims to enhance you working experience of ggplot2 to customise speciallised data visualisation like funnel plot.\n\n\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\n\nWarning in geom_point(aes(label = `Sub-district`), alpha = 0.4): Ignoring\nunknown aesthetics: label\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\np\n\n\n\n\n\n\n\n\n\n\n\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d/Hands-on_Ex04d.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex04d/Hands-on_Ex04d.html#references",
    "title": "Hands-on Exercise 4d",
    "section": "",
    "text": "funnelPlotR package.\nFunnel Plots for Indirectly-standardised ratios.\nChanging funnel plot options\nggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b/Hands-on_Ex04b.html",
    "href": "Hands-on_Ex/Hands-on_Ex04b/Hands-on_Ex04b.html",
    "title": "Hands-on Exercise 4b",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on using:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters\n\n\n\n\nggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n- To provide alternative statistical inference methods by default. - To follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the [APA](https://my.ilstu.edu/~jhkahn/apastats.html) gold standard for statistical reporting. For example, here are results from a robust t-test:\n\n\n\n\n\nIn this exercise, ggstatsplot and tidyverse will be used.\n\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\n\n\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\n\n\n\n\n\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to visualise model diagnostic and model parameters by using parameters package.\n\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n\n\n\n\n\n\n\npacman::p_load(readxl, performance, parameters, see)\n\n\n\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\n\n\n\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n\nIn the code chunk, check_collinearity() of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk, check_normality() of performance package.\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_n &lt;- check_normality(model1)\n\n\nplot(check_n)\n\nFor confidence bands, please install `qqplotr`.\n\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk, check_heteroscedasticity() of performance package.\n\ncheck_h &lt;- check_heteroscedasticity(model1)\n\n\nplot(check_h)\n\n\n\n\n\n\n\n\n\n\n\nWe can also perform the complete by using check_model().\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\n\n\n\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n\n\n\n\n\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b/Hands-on_Ex04b.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex04b/Hands-on_Ex04b.html#learning-outcome",
    "title": "Hands-on Exercise 4b",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on using:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b/Hands-on_Ex04b.html#visual-statistical-analysis-with-ggstatsplot",
    "href": "Hands-on_Ex/Hands-on_Ex04b/Hands-on_Ex04b.html#visual-statistical-analysis-with-ggstatsplot",
    "title": "Hands-on Exercise 4b",
    "section": "",
    "text": "ggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n- To provide alternative statistical inference methods by default. - To follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the [APA](https://my.ilstu.edu/~jhkahn/apastats.html) gold standard for statistical reporting. For example, here are results from a robust t-test:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b/Hands-on_Ex04b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04b/Hands-on_Ex04b.html#getting-started",
    "title": "Hands-on Exercise 4b",
    "section": "",
    "text": "In this exercise, ggstatsplot and tidyverse will be used.\n\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\n\n\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\n\n\n\n\n\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b/Hands-on_Ex04b.html#visualising-models",
    "href": "Hands-on_Ex/Hands-on_Ex04b/Hands-on_Ex04b.html#visualising-models",
    "title": "Hands-on Exercise 4b",
    "section": "",
    "text": "In this section, you will learn how to visualise model diagnostic and model parameters by using parameters package.\n\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b/Hands-on_Ex04b.html#installing-and-loading-the-required-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex04b/Hands-on_Ex04b.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 4b",
    "section": "",
    "text": "pacman::p_load(readxl, performance, parameters, see)\n\n\n\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\n\n\n\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n\nIn the code chunk, check_collinearity() of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk, check_normality() of performance package.\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_n &lt;- check_normality(model1)\n\n\nplot(check_n)\n\nFor confidence bands, please install `qqplotr`.\n\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk, check_heteroscedasticity() of performance package.\n\ncheck_h &lt;- check_heteroscedasticity(model1)\n\n\nplot(check_h)\n\n\n\n\n\n\n\n\n\n\n\nWe can also perform the complete by using check_model().\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\n\n\n\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n\n\n\n\n\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html",
    "href": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html",
    "title": "Hands-on Exercise 3b",
    "section": "",
    "text": "pacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\n\n\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_each_(funs(factor(.)), col) %&gt;%\n  mutate(Year = as.integer(Year))\n\nWarning: `mutate_each_()` was deprecated in dplyr 0.7.0.\nℹ Please use `across()` instead.\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(col, as.factor)`.\nCaused by warning:\n! Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(col)\n\n  # Now:\n  data %&gt;% select(all_of(col))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\n\n\n\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n\n\n\n\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nWarning in geom_point(aes(size = Population, frame = Year), alpha = 0.7, :\nIgnoring unknown aesthetics: frame\n\nggplotly(gg)\n\nWarning in p$x$data[firstFrame] &lt;- p$x$frames[[1]]$data: number of items to\nreplace is not a multiple of replacement length\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nWarning in geom_point(aes(size = Population, frame = Year), alpha = 0.7):\nIgnoring unknown aesthetics: frame\n\nggplotly(gg)\n\nWarning in p$x$data[firstFrame] &lt;- p$x$frames[[1]]$data: number of items to\nreplace is not a multiple of replacement length\n\n\n\n\n\n\n\n\n\n\nbp &lt;- suppressWarnings({\n  globalPop %&gt;%\n    plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\n})\nbp\n\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\n\n\n\n\n\n\n\n\n\n\n\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html#getting-started",
    "title": "Hands-on Exercise 3b",
    "section": "",
    "text": "pacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\n\n\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_each_(funs(factor(.)), col) %&gt;%\n  mutate(Year = as.integer(Year))\n\nWarning: `mutate_each_()` was deprecated in dplyr 0.7.0.\nℹ Please use `across()` instead.\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(col, as.factor)`.\nCaused by warning:\n! Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(col)\n\n  # Now:\n  data %&gt;% select(all_of(col))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html#animated-data-visualisation-gganimate-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html#animated-data-visualisation-gganimate-methods",
    "title": "Hands-on Exercise 3b",
    "section": "",
    "text": "ggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n\n\n\n\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html#animated-data-visualisation-plotly",
    "href": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html#animated-data-visualisation-plotly",
    "title": "Hands-on Exercise 3b",
    "section": "",
    "text": "gg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nWarning in geom_point(aes(size = Population, frame = Year), alpha = 0.7, :\nIgnoring unknown aesthetics: frame\n\nggplotly(gg)\n\nWarning in p$x$data[firstFrame] &lt;- p$x$frames[[1]]$data: number of items to\nreplace is not a multiple of replacement length\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nWarning in geom_point(aes(size = Population, frame = Year), alpha = 0.7):\nIgnoring unknown aesthetics: frame\n\nggplotly(gg)\n\nWarning in p$x$data[firstFrame] &lt;- p$x$frames[[1]]$data: number of items to\nreplace is not a multiple of replacement length\n\n\n\n\n\n\n\n\n\n\nbp &lt;- suppressWarnings({\n  globalPop %&gt;%\n    plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\n})\nbp\n\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values.\nWarning: `line.width` does not currently support multiple values."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex03b/Hands-on_Ex03b.html#reference",
    "title": "Hands-on Exercise 3b",
    "section": "",
    "text": "Getting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_EX02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_EX02/Hands-on_Ex02.html",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "pacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse)\n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: ggrepel: 317 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\np1\n\n\n\n\n\n\n\n\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\np2\n\n\n\n\n\n\n\n\n\np3 &lt;- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\np3\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n\n(p1 / p2) | p3\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_economist()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_EX02/Hands-on_Ex02.html#installing-and-loading-the-required-libraries",
    "href": "Hands-on_Ex/Hands-on_EX02/Hands-on_Ex02.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "pacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_EX02/Hands-on_Ex02.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_EX02/Hands-on_Ex02.html#importing-data",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "exam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_EX02/Hands-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "href": "Hands-on_Ex/Hands-on_EX02/Hands-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "ggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: ggrepel: 317 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_EX02/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_EX02/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_EX02/Hands-on_Ex02.html#beyond-single-graph",
    "href": "Hands-on_Ex/Hands-on_EX02/Hands-on_Ex02.html#beyond-single-graph",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "p1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\np1\n\n\n\n\n\n\n\n\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\np2\n\n\n\n\n\n\n\n\n\np3 &lt;- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\np3\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n\n(p1 / p2) | p3\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_economist()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are, then they will be launched into R.\n\npacman::p_load(tidyverse)\n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#install-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#install-and-launching-r-packages",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are, then they will be launched into R.\n\npacman::p_load(tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "exam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_bar",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_bar",
    "title": "Hands-on Exercise 1",
    "section": "Geometric Objects: geom_bar",
    "text": "Geometric Objects: geom_bar\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_dotplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_dotplot",
    "title": "Hands-on Exercise 1",
    "section": "Geometric Objects: geom_dotplot",
    "text": "Geometric Objects: geom_dotplot\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(dotsize = 0.5)\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(binwidth=2.5,         \n               dotsize = 0.5) +      \n  scale_y_continuous(NULL,           \n                     breaks = NULL)  \n\n\n\n\n\n\n\n\n\nGeometric Objects: geom_histogram()\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram()       \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#modifying-a-geometric-object-by-changing-geom",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#modifying-a-geometric-object-by-changing-geom",
    "title": "Hands-on Exercise 1",
    "section": "Modifying a geometric object by changing geom()",
    "text": "Modifying a geometric object by changing geom()\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"black\",      \n                 fill=\"light blue\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#modifying-a-geometric-object-by-changing-aes",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#modifying-a-geometric-object-by-changing-aes",
    "title": "Hands-on Exercise 1",
    "section": "Modifying a geometric object by changing aes()",
    "text": "Modifying a geometric object by changing aes()\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           fill = GENDER)) +\n  geom_histogram(bins=20, \n                 color=\"grey30\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom-density",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom-density",
    "title": "Hands-on Exercise 1",
    "section": "Geometric Objects: geom-density()",
    "text": "Geometric Objects: geom-density()\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_density()           \n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x = MATHS, \n           colour = GENDER)) +\n  geom_density()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_boxplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_boxplot",
    "title": "Hands-on Exercise 1",
    "section": "Geometric Objects: geom_boxplot",
    "text": "Geometric Objects: geom_boxplot\n\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER)) +    \n  geom_boxplot()            \n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot(notch=TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_violin",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_violin",
    "title": "Hands-on Exercise 1",
    "section": "Geometric Objects: geom_violin",
    "text": "Geometric Objects: geom_violin\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_violin()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_point",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_point",
    "title": "Hands-on Exercise 1",
    "section": "Geometric Objects: geom_point()",
    "text": "Geometric Objects: geom_point()\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geom-objects-can-be-combined",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geom-objects-can-be-combined",
    "title": "Hands-on Exercise 1",
    "section": "geom objects can be combined",
    "text": "geom objects can be combined\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-stat",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-stat",
    "title": "Hands-on Exercise 1",
    "section": "Essential Grammatical Elements in ggplot2: stat",
    "text": "Essential Grammatical Elements in ggplot2: stat\n\nWorking with stat()\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat---the-stat_summary-method",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat---the-stat_summary-method",
    "title": "Hands-on Exercise 1",
    "section": "Working with stat - the stat_summary() method",
    "text": "Working with stat - the stat_summary() method\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun = \"mean\",         \n               colour =\"red\",        \n               size=4)               \n\n\n\n\n\n\n\n\n\nWorking with stat - the geom() method\n\n\nAdding a best fit curve on a scatterplot?\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(size=0.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-facets",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-facets",
    "title": "Hands-on Exercise 1",
    "section": "Essential Grammatical Elements in ggplot2: Facets",
    "text": "Essential Grammatical Elements in ggplot2: Facets\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_wrap(~ CLASS)\n\n\n\n\n\n\n\n\n\nfacet_grid() function\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_grid(~ CLASS)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-coordinates",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-coordinates",
    "title": "Hands-on Exercise 1",
    "section": "Essential Grammatical Elements in ggplot2: Coordinates",
    "text": "Essential Grammatical Elements in ggplot2: Coordinates\n\nWorking with Coordinate\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\nChanging the y- and x-axis range\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, size=0.5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-themes",
    "title": "Hands-on Exercise 1",
    "section": "Essential Grammatical Elements in ggplot2: themes",
    "text": "Essential Grammatical Elements in ggplot2: themes\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html",
    "href": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html",
    "title": "Hands-on Exercise 3a",
    "section": "",
    "text": "pacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse) \n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\n\n\n\n\n\n\n\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\n\n\n\n\n\n\n\ntooltip_css &lt;- \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)                                        \n\n\n\n\n\n\n\n\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                        \n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\n\n\n\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                                        \n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter\n\n\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\n\n\n\n\n\n\n\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)        \n\nSetting the `off` event (i.e., 'plotly_deselect') to match the `on` event (i.e., 'plotly_selected'). You can change this default via the `highlight()` function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis link provides online version of the reference guide and several useful articles. Use this link to download the pdf version of the reference guide.\n\nHow to Plot With Ggiraph\nInteractive map of France with ggiraph\nCustom interactive sunbursts with ggplot in R\nThis link provides code example on how ggiraph is used to interactive graphs for Swiss Olympians - the solo specialists.\n\n\n\n\n\nGetting Started with Plotly in R\nA collection of plotly R graphs are available via this link.\nCarson Sievert (2020) Interactive web-based data visualization with R, plotly, and shiny, Chapman and Hall/CRC is the best resource to learn plotly for R. The online version is available via this link\nPlotly R Figure Reference provides a comprehensive discussion of each visual representations.\nPlotly R Library Fundamentals is a good place to learn the fundamental features of Plotly’s R API.\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#getting-started",
    "title": "Hands-on Exercise 3a",
    "section": "",
    "text": "pacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#importing-data",
    "title": "Hands-on Exercise 3a",
    "section": "",
    "text": "exam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactive-data-visualisation---ggiraph-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactive-data-visualisation---ggiraph-methods",
    "title": "Hands-on Exercise 3a",
    "section": "",
    "text": "p &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactivity",
    "href": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactivity",
    "title": "Hands-on Exercise 3a",
    "section": "",
    "text": "exam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactivity-1",
    "href": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactivity-1",
    "title": "Hands-on Exercise 3a",
    "section": "",
    "text": "tooltip_css &lt;- \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)                                        \n\n\n\n\n\n\n\n\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                        \n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\n\n\n\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                                        \n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactive-data-visualisation---plotly-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactive-data-visualisation---plotly-methods",
    "title": "Hands-on Exercise 3a",
    "section": "",
    "text": "plot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter\n\n\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactive-data-visualisation---crosstalk-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#interactive-data-visualisation---crosstalk-methods",
    "title": "Hands-on Exercise 3a",
    "section": "",
    "text": "DT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)        \n\nSetting the `off` event (i.e., 'plotly_deselect') to match the `on` event (i.e., 'plotly_selected'). You can change this default via the `highlight()` function."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex03a/Hands-on_Ex03a.html#reference",
    "title": "Hands-on Exercise 3a",
    "section": "",
    "text": "This link provides online version of the reference guide and several useful articles. Use this link to download the pdf version of the reference guide.\n\nHow to Plot With Ggiraph\nInteractive map of France with ggiraph\nCustom interactive sunbursts with ggplot in R\nThis link provides code example on how ggiraph is used to interactive graphs for Swiss Olympians - the solo specialists.\n\n\n\n\n\nGetting Started with Plotly in R\nA collection of plotly R graphs are available via this link.\nCarson Sievert (2020) Interactive web-based data visualization with R, plotly, and shiny, Chapman and Hall/CRC is the best resource to learn plotly for R. The online version is available via this link\nPlotly R Figure Reference provides a comprehensive discussion of each visual representations.\nPlotly R Library Fundamentals is a good place to learn the fundamental features of Plotly’s R API.\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a/Hands-on_Ex04a.html",
    "href": "Hands-on_Ex/Hands-on_Ex04a/Hands-on_Ex04a.html",
    "title": "Hands-on Exercise 4a",
    "section": "",
    "text": "Visualizing distribution is not new in statistical analysis. In chapter 1 we have shared with you some of the popular statistical graphics methods for visualising distribution are histogram, probability density curve (pdf), boxplot, notch plot and violin plot and how they can be created by using ggplot2. In this chapter, we are going to share with you two relatively new statistical graphic methods for visualising distribution, namely ridgeline plot and raincloud plot by using ggplot2 and its extensions.\n\n\n\nFor the purpose of this exercise, the following R packages will be used, they are:\n\nggridges, a ggplot2 extension specially designed for plotting ridgeline plots,\nggdist, a ggplot2 extension spacially desgin for visualising distribution and uncertainty,\ntidyverse, a family of R packages to meet the modern data science and visual communication needs,\nggthemes, a ggplot extension that provides the user additional themes, scales, and geoms for the ggplots package, and\ncolorspace, an R package provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualisations.\n\nThe code chunk below will be used load these R packages into RStudio environment.\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\n\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nRidgeline plot (sometimes called Joyplot) is a data visualisation technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\nFigure below is a ridgelines plot showing the distribution of English score by class.\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\nWarning: `stat(x)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(x)` instead.\n\n\nPicking joint bandwidth of 3.18\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\nPicking joint bandwidth of 3.18\n\n\n\n\n\n\n\n\n\nIt is important include the argument calc_ecdf = TRUE in stat_density_ridges().\n\n\n\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown in the figure below.\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\nPicking joint bandwidth of 3.18\n\n\n\n\n\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()\n\nPicking joint bandwidth of 3.18\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaincloud Plot is a data visualisation techniques that produces a half-density to a distribution plot. It gets the name because the density plot is in the shape of a “raincloud”. The raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). The boxplot does not show where densities are clustered, but the raincloud plot does!\nIn this section, you will learn how to create a raincloud plot to visualise the distribution of English score by race. It will be created by using functions provided by ggdist and ggplot2 packages.\n\n\nFirst, we will plot a Half-Eye graph by using stat_halfeye() of ggdist package.\nThis produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\n\n\n\n\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n\n\n\n\n\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = “left” to indicate we want it on the left-hand side.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\n\n\n\n\nLastly, coord_flip() of ggplot2 package will be used to flip the raincloud chart horizontally to give it the raincloud appearance. At the same time, theme_economist() of ggthemes package is used to give the raincloud chart a professional publishing standard look.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()\n\nWarning: The provided binwidth will cause dots to overflow the boundaries of the\ngeometry.\n→ Set `binwidth = NA` to automatically determine a binwidth that ensures dots\n  fit within the bounds,\n→ OR set `overflow = \"compress\"` to automatically reduce the spacing between\n  dots to ensure the dots fit within the bounds,\n→ OR set `overflow = \"keep\"` to allow dots to overflow the bounds of the\n  geometry without producing a warning.\nℹ For more information, see the documentation of the `binwidth` and `overflow`\n  arguments of `?ggdist::geom_dots()` or the section on constraining dot sizes\n  in vignette(\"dotsinterval\") (`vignette(ggdist::dotsinterval)`).\n\n\n\n\n\n\n\n\n\nReference\n\nIntroducing Ridgeline Plots (formerly Joyplots)\nClaus O. Wilke Fundamentals of Data Visualization especially Chapter 6, 7, 8, 9 and 10.\nAllen M, Poggiali D, Whitaker K et al. “Raincloud plots: a multi-platform tool for robust data. visualization” [version 2; peer review: 2 approved]. Welcome Open Res 2021, pp. 4:63.\nDots + interval stats and geoms"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a/Hands-on_Ex04a.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex04a/Hands-on_Ex04a.html#learning-outcome",
    "title": "Hands-on Exercise 4a",
    "section": "",
    "text": "Visualizing distribution is not new in statistical analysis. In chapter 1 we have shared with you some of the popular statistical graphics methods for visualising distribution are histogram, probability density curve (pdf), boxplot, notch plot and violin plot and how they can be created by using ggplot2. In this chapter, we are going to share with you two relatively new statistical graphic methods for visualising distribution, namely ridgeline plot and raincloud plot by using ggplot2 and its extensions."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a/Hands-on_Ex04a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04a/Hands-on_Ex04a.html#getting-started",
    "title": "Hands-on Exercise 4a",
    "section": "",
    "text": "For the purpose of this exercise, the following R packages will be used, they are:\n\nggridges, a ggplot2 extension specially designed for plotting ridgeline plots,\nggdist, a ggplot2 extension spacially desgin for visualising distribution and uncertainty,\ntidyverse, a family of R packages to meet the modern data science and visual communication needs,\nggthemes, a ggplot extension that provides the user additional themes, scales, and geoms for the ggplots package, and\ncolorspace, an R package provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualisations.\n\nThe code chunk below will be used load these R packages into RStudio environment.\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\n\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a/Hands-on_Ex04a.html#visualising-distribution-with-ridgeline-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04a/Hands-on_Ex04a.html#visualising-distribution-with-ridgeline-plot",
    "title": "Hands-on Exercise 4a",
    "section": "",
    "text": "Ridgeline plot (sometimes called Joyplot) is a data visualisation technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\nFigure below is a ridgelines plot showing the distribution of English score by class.\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\nWarning: `stat(x)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(x)` instead.\n\n\nPicking joint bandwidth of 3.18\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\nPicking joint bandwidth of 3.18\n\n\n\n\n\n\n\n\n\nIt is important include the argument calc_ecdf = TRUE in stat_density_ridges().\n\n\n\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown in the figure below.\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\nPicking joint bandwidth of 3.18\n\n\n\n\n\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()\n\nPicking joint bandwidth of 3.18"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a/Hands-on_Ex04a.html#visualising-distribution-with-raincloud-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04a/Hands-on_Ex04a.html#visualising-distribution-with-raincloud-plot",
    "title": "Hands-on Exercise 4a",
    "section": "",
    "text": "Raincloud Plot is a data visualisation techniques that produces a half-density to a distribution plot. It gets the name because the density plot is in the shape of a “raincloud”. The raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). The boxplot does not show where densities are clustered, but the raincloud plot does!\nIn this section, you will learn how to create a raincloud plot to visualise the distribution of English score by race. It will be created by using functions provided by ggdist and ggplot2 packages.\n\n\nFirst, we will plot a Half-Eye graph by using stat_halfeye() of ggdist package.\nThis produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\n\n\n\n\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n\n\n\n\n\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = “left” to indicate we want it on the left-hand side.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\n\n\n\n\nLastly, coord_flip() of ggplot2 package will be used to flip the raincloud chart horizontally to give it the raincloud appearance. At the same time, theme_economist() of ggthemes package is used to give the raincloud chart a professional publishing standard look.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()\n\nWarning: The provided binwidth will cause dots to overflow the boundaries of the\ngeometry.\n→ Set `binwidth = NA` to automatically determine a binwidth that ensures dots\n  fit within the bounds,\n→ OR set `overflow = \"compress\"` to automatically reduce the spacing between\n  dots to ensure the dots fit within the bounds,\n→ OR set `overflow = \"keep\"` to allow dots to overflow the bounds of the\n  geometry without producing a warning.\nℹ For more information, see the documentation of the `binwidth` and `overflow`\n  arguments of `?ggdist::geom_dots()` or the section on constraining dot sizes\n  in vignette(\"dotsinterval\") (`vignette(ggdist::dotsinterval)`).\n\n\n\n\n\n\n\n\n\nReference\n\nIntroducing Ridgeline Plots (formerly Joyplots)\nClaus O. Wilke Fundamentals of Data Visualization especially Chapter 6, 7, 8, 9 and 10.\nAllen M, Poggiali D, Whitaker K et al. “Raincloud plots: a multi-platform tool for robust data. visualization” [version 2; peer review: 2 approved]. Welcome Open Res 2021, pp. 4:63.\nDots + interval stats and geoms"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c/Hands-on_Ex04c.html",
    "href": "Hands-on_Ex/Hands-on_Ex04c/Hands-on_Ex04c.html",
    "title": "Hands-on Exercise 4c",
    "section": "",
    "text": "Visualising uncertainty is relatively new in statistical graphics. In this chapter, you will gain hands-on experience on creating statistical graphics for visualising uncertainty. By the end of this chapter you will be able:\n\nto plot statistics error bars by using ggplot2,\nto plot interactive error bars by combining ggplot2, plotly and DT,\nto create advanced by using ggdist, and\nto create hypothetical outcome plots (HOPs) by using ungeviz package.\n\n\n\n\n\n\nFor the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\npacman::p_load(plotly, crosstalk, DT, \n               ggdist, ggridges, colorspace,\n               gganimate, tidyverse)\n\n\n\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nA point estimate is a single number, such as a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval.\nIn this section, you will learn how to plot error bars of maths scores by race by using data provided in exam tibble data frame.\nFirstly, code chunk below will be used to derive the necessary summary statistics.\n\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\nThings to learn from the code chunk above\n\ngroup_by() of dplyr package is used to group the observation by RACE,\nsummarise() is used to compute the count of observations, mean, standard deviation\nmutate() is used to derive standard error of Maths by RACE, and\nthe output is save as a tibble data table called my_sum.\n\nNext, the code chunk below will be used to display my_sum tibble data frame in an html table format.\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\nNow we are ready to plot the standard error bars of mean maths score by race as shown below.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    linewidth=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\n\n\n\n\n\n\n\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    linewidth=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\n\n\n\n\nn this section, you will learn how to plot interactive error bars for the 99% confidence interval of mean maths score by race as shown in the figure below.\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning in geom_point(aes(x = RACE, y = mean, text = paste(\"Race:\", RACE, :\nIgnoring unknown aesthetics: text\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\nFor example, in the code chunk below the following arguments are used:\n\n.width = 0.95\n.point = median\n.interval = qi\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\nWarning in layer_slabinterval(data = data, mapping = mapping, stat =\nStatPointinterval, : Ignoring unknown parameters: `.point` and `.interval`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(\n    show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\nGentle advice: This function comes with many arguments, students are advised to read the syntax reference for more detail.\n\n\n\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\nWarning in draw_slabs(self, ...): `fill_type = \"gradient\"` is not supported by the current graphics device.\nℹ Falling back to `fill_type = \"segments\"`.\n→ If you believe your current graphics device does support `fill_type =\n  \"gradient\"` but auto-detection failed, try setting `fill_type = \"gradient\"`\n  explicitly. If this causes the gradient to display correctly, then this\n  warning is likely a false positive caused by the graphics device failing to\n  properly report its support for the `\"LinearGradient\"` pattern via\n  `grDevices::dev.capabilities()`. Consider reporting a bug to the author of\n  the graphics device.\nℹ For more information, see the documentation for `fill_type` in\n  `ggdist::geom_slabinterval()` or the documentation for\n  `ggplot2::check_device()`.\nCaused by warning in `draw_slabs()`:\n! Unable to check the capabilities of the png device.\n\n\n\n\n\n\n\n\n\nGentle advice: This function comes with many arguments, students are advised to read the syntax reference for more detail.\n\n\n\n\n\n\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\nWARNING: Rtools is required to build R packages, but is not currently installed.\n\nPlease download and install Rtools 4.4 from https://cran.r-project.org/bin/windows/Rtools/.\n\n\nUsing GitHub PAT from the git credential store.\n\n\nSkipping install of 'ungeviz' from a github remote, the SHA1 (d43afb69) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\n\n\n\n\nlibrary(ungeviz)\n\n\n\n\nNext, the code chunk below will be used to build the HOPs.\n\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)\n\nWarning in geom_hpline(data = sampler(25, group = RACE), height = 0.6, color =\n\"#D55E00\"): Ignoring unknown parameters: `height`"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c/Hands-on_Ex04c.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex04c/Hands-on_Ex04c.html#learning-outcome",
    "title": "Hands-on Exercise 4c",
    "section": "",
    "text": "Visualising uncertainty is relatively new in statistical graphics. In this chapter, you will gain hands-on experience on creating statistical graphics for visualising uncertainty. By the end of this chapter you will be able:\n\nto plot statistics error bars by using ggplot2,\nto plot interactive error bars by combining ggplot2, plotly and DT,\nto create advanced by using ggdist, and\nto create hypothetical outcome plots (HOPs) by using ungeviz package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c/Hands-on_Ex04c.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04c/Hands-on_Ex04c.html#getting-started",
    "title": "Hands-on Exercise 4c",
    "section": "",
    "text": "For the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\npacman::p_load(plotly, crosstalk, DT, \n               ggdist, ggridges, colorspace,\n               gganimate, tidyverse)\n\n\n\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c/Hands-on_Ex04c.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04c/Hands-on_Ex04c.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "title": "Hands-on Exercise 4c",
    "section": "",
    "text": "A point estimate is a single number, such as a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval.\nIn this section, you will learn how to plot error bars of maths scores by race by using data provided in exam tibble data frame.\nFirstly, code chunk below will be used to derive the necessary summary statistics.\n\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\nThings to learn from the code chunk above\n\ngroup_by() of dplyr package is used to group the observation by RACE,\nsummarise() is used to compute the count of observations, mean, standard deviation\nmutate() is used to derive standard error of Maths by RACE, and\nthe output is save as a tibble data table called my_sum.\n\nNext, the code chunk below will be used to display my_sum tibble data frame in an html table format.\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\nNow we are ready to plot the standard error bars of mean maths score by race as shown below.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    linewidth=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\n\n\n\n\n\n\n\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    linewidth=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\n\n\n\n\nn this section, you will learn how to plot interactive error bars for the 99% confidence interval of mean maths score by race as shown in the figure below.\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning in geom_point(aes(x = RACE, y = mean, text = paste(\"Race:\", RACE, :\nIgnoring unknown aesthetics: text"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c/Hands-on_Ex04c.html#visualising-uncertainty-ggdist-package",
    "href": "Hands-on_Ex/Hands-on_Ex04c/Hands-on_Ex04c.html#visualising-uncertainty-ggdist-package",
    "title": "Hands-on Exercise 4c",
    "section": "",
    "text": "ggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\nFor example, in the code chunk below the following arguments are used:\n\n.width = 0.95\n.point = median\n.interval = qi\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\nWarning in layer_slabinterval(data = data, mapping = mapping, stat =\nStatPointinterval, : Ignoring unknown parameters: `.point` and `.interval`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(\n    show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\nGentle advice: This function comes with many arguments, students are advised to read the syntax reference for more detail.\n\n\n\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\nWarning in draw_slabs(self, ...): `fill_type = \"gradient\"` is not supported by the current graphics device.\nℹ Falling back to `fill_type = \"segments\"`.\n→ If you believe your current graphics device does support `fill_type =\n  \"gradient\"` but auto-detection failed, try setting `fill_type = \"gradient\"`\n  explicitly. If this causes the gradient to display correctly, then this\n  warning is likely a false positive caused by the graphics device failing to\n  properly report its support for the `\"LinearGradient\"` pattern via\n  `grDevices::dev.capabilities()`. Consider reporting a bug to the author of\n  the graphics device.\nℹ For more information, see the documentation for `fill_type` in\n  `ggdist::geom_slabinterval()` or the documentation for\n  `ggplot2::check_device()`.\nCaused by warning in `draw_slabs()`:\n! Unable to check the capabilities of the png device.\n\n\n\n\n\n\n\n\n\nGentle advice: This function comes with many arguments, students are advised to read the syntax reference for more detail."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c/Hands-on_Ex04c.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands-on_Ex/Hands-on_Ex04c/Hands-on_Ex04c.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Hands-on Exercise 4c",
    "section": "",
    "text": "devtools::install_github(\"wilkelab/ungeviz\")\n\nWARNING: Rtools is required to build R packages, but is not currently installed.\n\nPlease download and install Rtools 4.4 from https://cran.r-project.org/bin/windows/Rtools/.\n\n\nUsing GitHub PAT from the git credential store.\n\n\nSkipping install of 'ungeviz' from a github remote, the SHA1 (d43afb69) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\n\n\n\n\nlibrary(ungeviz)\n\n\n\n\nNext, the code chunk below will be used to build the HOPs.\n\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)\n\nWarning in geom_hpline(data = sampler(25, group = RACE), height = 0.6, color =\n\"#D55E00\"): Ignoring unknown parameters: `height`"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05a/Hands-on_Ex05a.html",
    "href": "Hands-on_Ex/Hands-on_Ex05a/Hands-on_Ex05a.html",
    "title": "Hands-on Exercise 5a",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, you will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\nThe hands-on exercise consists of four steps:\n\nInstall and launch tidyverse and ggtern packages.\nDerive three new measures using mutate() function of dplyr package.\nBuild a static ternary plot using ggtern() function of ggtern package.\nBuild an interactive ternary plot using plot-ly() function of Plotly R package.\n\n\n\n\nFor this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\nIn this exercise, version 3.2.1 of ggplot2 will be installed instead of the latest version of ggplot2. This is because the current version of ggtern package is not compatible to the latest version of ggplot2.\nThe code chunks below will accomplish the task.\n\npacman::p_load(plotly, ggtern, tidyverse)\n\n\n\n\n\n\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\n\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\nRows: 108126 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): PA, SZ, AG\ndbl (2): Year, Population\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n\n\n\n\n\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )\n\nNo scatterternary mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05a/Hands-on_Ex05a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05a/Hands-on_Ex05a.html#overview",
    "title": "Hands-on Exercise 5a",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, you will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\nThe hands-on exercise consists of four steps:\n\nInstall and launch tidyverse and ggtern packages.\nDerive three new measures using mutate() function of dplyr package.\nBuild a static ternary plot using ggtern() function of ggtern package.\nBuild an interactive ternary plot using plot-ly() function of Plotly R package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05a/Hands-on_Ex05a.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05a/Hands-on_Ex05a.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 5a",
    "section": "",
    "text": "For this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\nIn this exercise, version 3.2.1 of ggplot2 will be installed instead of the latest version of ggplot2. This is because the current version of ggtern package is not compatible to the latest version of ggplot2.\nThe code chunks below will accomplish the task.\n\npacman::p_load(plotly, ggtern, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05a/Hands-on_Ex05a.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex05a/Hands-on_Ex05a.html#data-preparation",
    "title": "Hands-on Exercise 5a",
    "section": "",
    "text": "For the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\n\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\nRows: 108126 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): PA, SZ, AG\ndbl (2): Year, Population\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05a/Hands-on_Ex05a.html#plotting-ternary-diagram-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex05a/Hands-on_Ex05a.html#plotting-ternary-diagram-with-r",
    "title": "Hands-on Exercise 5a",
    "section": "",
    "text": "Use ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )\n\nNo scatterternary mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html",
    "href": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html",
    "title": "Hands-on Exercise 5c",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\nIn this hands-on exercise, you will learn how to plot data visualisation for visualising correlation matrix with R. It consists of three main sections. First, you will learn how to create correlation matrix using pairs() of R Graphics. Next, you will learn how to plot corrgram using corrplot package of R. Lastly, you will learn how to create an interactive correlation matrix using plotly R.\n\n\n\nBefore you get started, you are required to open a new Quarto document. Keep the default html authoring format.\nNext, you will use the code chunk below to install and launch corrplot, ggpubr, plotly and tidyverse in RStudio.\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)\n\n\n\n\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\n\nFirst, let us import the data into R by using read_csv() of readr package.\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\nRows: 6497 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): type\ndbl (12): fixed acidity, volatile acidity, citric acid, residual sugar, chlo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNotice that beside quality and type, the rest of the variables are numerical and continuous data type.\n\n\n\n\nThere are more than one way to build scatterplot matrix with R. In this section, you will learn how to create a scatterplot matrix by using the pairs function of R Graphics.\nBefore you continue to the next step, you should read the syntax description of pairsfunction.\n\n\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\npairs(wine[,1:11])\n\n\n\n\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function. Columns 2 to 12 of wine dataframe is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n\n\n\n\n\npairs function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used as shown in the code chunk below.\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n\n\nSimilarly, you can display the upper half of the correlation matrix by using the code chun below.\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\nDon’t worry about the details for now-just type this code into your R session or script. Let’s have more fun way to display the correlation matrix.\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)\n\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To over come this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages provide function to plot corrgram, they are:\n\ncorrgram\nellipse\ncorrplot\n\nOn top that, some R package like ggstatsplot package also provides functions for building corrgram.\nIn this section, you will learn how to visualising correlation matrix by using ggcorrmat() of ggstatsplot package.\n\n\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram.\nggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\nThe sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))\n\n\n\n\n\nSince ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package.\n\n\n\n\nIn this hands-on exercise, we will focus on corrplot. However, you are encouraged to explore the other two packages too.\nBefore getting started, you are required to read An Introduction to corrplot Package in order to gain basic understanding of corrplot package.\n\n\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\ncorrplot(wine.cor)\n\n\n\n\n\n\n\n\nNotice that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\n\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\n\nFeel free to change the method argument to other supported visual geometrics.\n\n\n\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\nPlease feel free to experiment with other layout design argument such as tl.pos, tl.cex, tl.offset, cl.pos, cl.cex and cl.offset, just to mention a few of them.\n\n\n\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nThe code chunk used to plot the corrgram are shown below.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nNotice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n\n\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nFigure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n\n“AOE”, “FPC”, “hclust”, “alphabet”. More algorithms can be found in seriation package.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html#overview",
    "title": "Hands-on Exercise 5c",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\nIn this hands-on exercise, you will learn how to plot data visualisation for visualising correlation matrix with R. It consists of three main sections. First, you will learn how to create correlation matrix using pairs() of R Graphics. Next, you will learn how to plot corrgram using corrplot package of R. Lastly, you will learn how to create an interactive correlation matrix using plotly R."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 5c",
    "section": "",
    "text": "Before you get started, you are required to open a new Quarto document. Keep the default html authoring format.\nNext, you will use the code chunk below to install and launch corrplot, ggpubr, plotly and tidyverse in RStudio.\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html#importing-and-preparing-the-data-set",
    "href": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html#importing-and-preparing-the-data-set",
    "title": "Hands-on Exercise 5c",
    "section": "",
    "text": "In this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\n\nFirst, let us import the data into R by using read_csv() of readr package.\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\nRows: 6497 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): type\ndbl (12): fixed acidity, volatile acidity, citric acid, residual sugar, chlo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNotice that beside quality and type, the rest of the variables are numerical and continuous data type."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html#building-correlation-matrix-pairs-method",
    "href": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html#building-correlation-matrix-pairs-method",
    "title": "Hands-on Exercise 5c",
    "section": "",
    "text": "There are more than one way to build scatterplot matrix with R. In this section, you will learn how to create a scatterplot matrix by using the pairs function of R Graphics.\nBefore you continue to the next step, you should read the syntax description of pairsfunction.\n\n\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\npairs(wine[,1:11])\n\n\n\n\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function. Columns 2 to 12 of wine dataframe is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n\n\n\n\n\npairs function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used as shown in the code chunk below.\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n\n\nSimilarly, you can display the upper half of the correlation matrix by using the code chun below.\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\nDon’t worry about the details for now-just type this code into your R session or script. Let’s have more fun way to display the correlation matrix.\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)\n\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on Exercise 5c",
    "section": "",
    "text": "One of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To over come this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages provide function to plot corrgram, they are:\n\ncorrgram\nellipse\ncorrplot\n\nOn top that, some R package like ggstatsplot package also provides functions for building corrgram.\nIn this section, you will learn how to visualising correlation matrix by using ggcorrmat() of ggstatsplot package.\n\n\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram.\nggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\nThe sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html#building-multiple-plots",
    "href": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html#building-multiple-plots",
    "title": "Hands-on Exercise 5c",
    "section": "",
    "text": "Since ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html#visualising-correlation-matrix-using-corrplot-package",
    "href": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html#visualising-correlation-matrix-using-corrplot-package",
    "title": "Hands-on Exercise 5c",
    "section": "",
    "text": "In this hands-on exercise, we will focus on corrplot. However, you are encouraged to explore the other two packages too.\nBefore getting started, you are required to read An Introduction to corrplot Package in order to gain basic understanding of corrplot package.\n\n\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\ncorrplot(wine.cor)\n\n\n\n\n\n\n\n\nNotice that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\n\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\n\nFeel free to change the method argument to other supported visual geometrics.\n\n\n\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\nPlease feel free to experiment with other layout design argument such as tl.pos, tl.cex, tl.offset, cl.pos, cl.cex and cl.offset, just to mention a few of them.\n\n\n\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nThe code chunk used to plot the corrgram are shown below.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nNotice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n\n\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nFigure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n\n“AOE”, “FPC”, “hclust”, “alphabet”. More algorithms can be found in seriation package.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html#r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05c/Hands-on_Ex05c.html#r-packages",
    "title": "Hands-on Exercise 5c",
    "section": "7.1 R packages",
    "text": "7.1 R packages\n\nggcormat() of ggstatsplot package\nggscatmat and ggpairs of GGally.\ncorrplot. A graphical display of a correlation matrix or general matrix. It also contains some algorithms to do matrix reordering. In addition, corrplot is good at details, including choosing color, text labels, color labels, layout, etc.\ncorrgram calculates correlation of variables and displays the results graphically. Included panel functions can display points, shading, ellipses, and correlation values with confidence intervals."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Applications. In this website, you will find my coursework prepared for this course."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d/Hands-on_Ex05d.html",
    "href": "Hands-on_Ex/Hands-on_Ex05d/Hands-on_Ex05d.html",
    "title": "Hands-on Exercise 5d",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package.\n\n\n\n\nFor this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\npacman::p_load(GGally, parallelPlot, tidyverse)\n\n\n\n\nIn this hands-on exercise, the World Happinees 2018 (http://worldhappiness.report/ed/2018/) data will be used. The data set is download at https://s3.amazonaws.com/happiness-report/2018/WHR2018Chapter2OnlineData.xls. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr package is used to import WHData-2018.csv into R and save it into a tibble data frame object called wh.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nRows: 156 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Country, Region\ndbl (10): Happiness score, Whisker-high, Whisker-low, Dystopia, GDP per capi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nIn this section, you will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\n\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\n\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above.\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\n\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\n\n\n\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))\n\n\n\n\n\n\n\n\n\n\n\n\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\n\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\nNotice that some of the axis labels are too long. You will learn how to overcome this problem in the next step.\n\n\n\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\n\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunl below.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)\n\n\n\n\n\n\n\n\n\n\nggparcoord() of GGally package\nparcoords user guide\nparallelPlot"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d/Hands-on_Ex05d.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05d/Hands-on_Ex05d.html#overview",
    "title": "Hands-on Exercise 5d",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d/Hands-on_Ex05d.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05d/Hands-on_Ex05d.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 5d",
    "section": "",
    "text": "For this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\npacman::p_load(GGally, parallelPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d/Hands-on_Ex05d.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex05d/Hands-on_Ex05d.html#data-preparation",
    "title": "Hands-on Exercise 5d",
    "section": "",
    "text": "In this hands-on exercise, the World Happinees 2018 (http://worldhappiness.report/ed/2018/) data will be used. The data set is download at https://s3.amazonaws.com/happiness-report/2018/WHR2018Chapter2OnlineData.xls. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr package is used to import WHData-2018.csv into R and save it into a tibble data frame object called wh.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nRows: 156 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Country, Region\ndbl (10): Happiness score, Whisker-high, Whisker-low, Dystopia, GDP per capi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d/Hands-on_Ex05d.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05d/Hands-on_Ex05d.html#plotting-static-parallel-coordinates-plot",
    "title": "Hands-on Exercise 5d",
    "section": "",
    "text": "In this section, you will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\n\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\n\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above.\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\n\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\n\n\n\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d/Hands-on_Ex05d.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands-on_Ex/Hands-on_Ex05d/Hands-on_Ex05d.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hands-on Exercise 5d",
    "section": "",
    "text": "parallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\n\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\nNotice that some of the axis labels are too long. You will learn how to overcome this problem in the next step.\n\n\n\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\n\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunl below.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d/Hands-on_Ex05d.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex05d/Hands-on_Ex05d.html#references",
    "title": "Hands-on Exercise 5d",
    "section": "",
    "text": "ggparcoord() of GGally package\nparcoords user guide\nparallelPlot"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e/Hands-on_Ex05e.html",
    "href": "Hands-on_Ex/Hands-on_Ex05e/Hands-on_Ex05e.html",
    "title": "Hands-on Exercise 5e",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experiences on designing treemap using appropriate R packages. The hands-on exercise consists of three main section. First, you will learn how to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, you will learn how to plot static treemap by using treemap package. In the third section, you will learn how to design interactive treemap by using d3treeR package.\n\n\n\nBefore we get started, you are required to check if treemap and tidyverse pacakges have been installed in you R.\n\npacman::p_load(treemap, treemapify, tidyverse) \n\n\n\n\nIn this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal (https://spring.ura.gov.sg/lad/ore/login/index.cfm) of Urban Redevelopment Authority (URA).\n\n\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\nRows: 23205 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): Project Name, Address, Type of Area, Nett Price($), Sale Date, Pro...\ndbl  (8): No. of Units, Area (sqm), Transacted Price ($), Unit Price ($ psm)...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe output tibble data.frame is called realis2018.\n\n\n\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\n\n\nThe code chank below shows a typical two lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n`summarise()` has grouped output by 'Project Name', 'Planning Region',\n'Planning Area', 'Property Type'. You can override using the `.groups`\nargument.\n\n\n\nNote\n\nAggregation functions such as sum() and meadian() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\n\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%:\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n`summarise()` has grouped output by 'Project Name', 'Planning Region',\n'Planning Area', 'Property Type'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\n\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\nWarning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\n\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThinking to learn from the conde chunk above.\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very copnfusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\n\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to “treemapify” its user guide.\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis slide shows you how to install a R package which is not available in cran.\n\nIf this is the first time you install a package from github, you should install devtools package by using the code below or else you can skip this step.\n\n\n\nNext, you will load the devtools library and install the package found in github by using the codes below.\n\n\nlibrary(devtools)\n\nLoading required package: usethis\n\ninstall_github(\"timelyportfolio/d3treeR\")\n\nWARNING: Rtools is required to build R packages, but is not currently installed.\n\nPlease download and install Rtools 4.4 from https://cran.r-project.org/bin/windows/Rtools/.\n\n\nUsing GitHub PAT from the git credential store.\n\n\nSkipping install of 'd3treeR' from a github remote, the SHA1 (ebb833db) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\n\nNow you are ready to launch d3treeR package\n\n\nlibrary(d3treeR)\n\n\n\n\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n2.Then d3tree() is used to build an interactive treemap.\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e/Hands-on_Ex05e.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05e/Hands-on_Ex05e.html#overview",
    "title": "Hands-on Exercise 5e",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experiences on designing treemap using appropriate R packages. The hands-on exercise consists of three main section. First, you will learn how to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, you will learn how to plot static treemap by using treemap package. In the third section, you will learn how to design interactive treemap by using d3treeR package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e/Hands-on_Ex05e.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05e/Hands-on_Ex05e.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 5e",
    "section": "",
    "text": "Before we get started, you are required to check if treemap and tidyverse pacakges have been installed in you R.\n\npacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e/Hands-on_Ex05e.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex05e/Hands-on_Ex05e.html#data-wrangling",
    "title": "Hands-on Exercise 5e",
    "section": "",
    "text": "In this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal (https://spring.ura.gov.sg/lad/ore/login/index.cfm) of Urban Redevelopment Authority (URA).\n\n\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\nRows: 23205 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): Project Name, Address, Type of Area, Nett Price($), Sale Date, Pro...\ndbl  (8): No. of Units, Area (sqm), Transacted Price ($), Unit Price ($ psm)...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe output tibble data.frame is called realis2018.\n\n\n\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\n\n\nThe code chank below shows a typical two lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n`summarise()` has grouped output by 'Project Name', 'Planning Region',\n'Planning Area', 'Property Type'. You can override using the `.groups`\nargument.\n\n\n\nNote\n\nAggregation functions such as sum() and meadian() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\n\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%:\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n`summarise()` has grouped output by 'Project Name', 'Planning Region',\n'Planning Area', 'Property Type'. You can override using the `.groups`\nargument."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e/Hands-on_Ex05e.html#designing-treemap-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex05e/Hands-on_Ex05e.html#designing-treemap-with-treemap-package",
    "title": "Hands-on Exercise 5e",
    "section": "",
    "text": "treemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\n\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\nWarning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\n\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThinking to learn from the conde chunk above.\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very copnfusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e/Hands-on_Ex05e.html#designing-treemap-using-treemapify-package",
    "href": "Hands-on_Ex/Hands-on_Ex05e/Hands-on_Ex05e.html#designing-treemap-using-treemapify-package",
    "title": "Hands-on Exercise 5e",
    "section": "",
    "text": "treemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to “treemapify” its user guide.\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e/Hands-on_Ex05e.html#designing-interactive-treemap-using-d3treer",
    "href": "Hands-on_Ex/Hands-on_Ex05e/Hands-on_Ex05e.html#designing-interactive-treemap-using-d3treer",
    "title": "Hands-on Exercise 5e",
    "section": "",
    "text": "This slide shows you how to install a R package which is not available in cran.\n\nIf this is the first time you install a package from github, you should install devtools package by using the code below or else you can skip this step.\n\n\n\nNext, you will load the devtools library and install the package found in github by using the codes below.\n\n\nlibrary(devtools)\n\nLoading required package: usethis\n\ninstall_github(\"timelyportfolio/d3treeR\")\n\nWARNING: Rtools is required to build R packages, but is not currently installed.\n\nPlease download and install Rtools 4.4 from https://cran.r-project.org/bin/windows/Rtools/.\n\n\nUsing GitHub PAT from the git credential store.\n\n\nSkipping install of 'd3treeR' from a github remote, the SHA1 (ebb833db) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\n\nNow you are ready to launch d3treeR package\n\n\nlibrary(d3treeR)\n\n\n\n\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n2.Then d3tree() is used to build an interactive treemap.\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "title": "Hands-on Exercise 7",
    "section": "",
    "text": "By the end of this hands-on exercise you will be able create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart\n\n\n\n\nWrite a code chunk to check, install and launch the following R packages: scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table and tidyverse.\n\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)\n\n\n\n\nIn this section, you will learn how to plot a calender heatmap programmatically by using ggplot2 package.\nBy the end of this section, you will be able to:\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\n\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n\n\nFirst, you will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\nRows: 199999 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): source_country, tz\ndttm (1): timestamp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nFor example, kable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\n\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\nSys.setlocale(\"LC_TIME\", \"C\")\n\n[1] \"C\"\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\n\nwindowsFonts(Arial = windowsFont(\"Arial\"))\n\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  filter(!is.na(wkday), !is.na(hour))\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n            linewidth = 0.1) + \n  theme_tufte(base_family = \"Arial\")  + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                      low = \"sky blue\", \n                      high = \"dark blue\") +\n  labs(x = NULL, \n       y = NULL, \n       title = \"Attacks by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\nThen we can simply group the count by hour and wkday and plot it, since we know that we have values for every combination there’s no need to further preprocess the data.\n\n\n\nChallenge: Building multiple heatmaps for the top four countries with the highest number of attacks.\n\n\n\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  filter(!is.na(source_country), !is.na(wkday), !is.na(hour))\n\n\n\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nif (nrow(top4_attacks) &gt; 0) {\n  ggplot(top4_attacks, \n         aes(hour, \n             wkday, \n             fill = n)) + \n    geom_tile(color = \"white\", \n              linewidth = 0.1) + \n    theme_tufte(base_family = \"Helvetica\") + \n    coord_equal() +\n    scale_fill_gradient(name = \"# of attacks\",\n                        low = \"sky blue\", \n                        high = \"dark blue\") +\n    facet_wrap(~source_country, ncol = 2) +\n    labs(x = NULL, y = NULL, \n         title = \"Attacks on top 4 countries by weekday and time of day\") +\n    theme(axis.ticks = element_blank(),\n          axis.text.x = element_text(size = 7),\n          plot.title = element_text(hjust = 0.5),\n          legend.title = element_text(size = 8),\n          legend.text = element_text(size = 6) )\n} else {\n  print(\"No data available for the selected countries.\")\n}\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\n\nNext, the code chunk below is use to extract data for the target country (i.e. Vietnam)\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\n\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\nThe code chunk below is used to plot the cycle plot as shown in Slide 12/23.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section you will learn how to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\n\nImport the rice data set into R environment by using the code chunk below.\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\nRows: 550 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Country\ndbl (3): Year, Yield, Production\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\nConverting 'Year' to an ordered factor"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#learning-outcome",
    "title": "Hands-on Exercise 7",
    "section": "",
    "text": "By the end of this hands-on exercise you will be able create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started",
    "title": "Hands-on Exercise 7",
    "section": "",
    "text": "Write a code chunk to check, install and launch the following R packages: scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table and tidyverse.\n\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "In-class_Ex05",
    "section": "",
    "text": "Getting started\n\npacman::p_load(tidyverse, readxl, SmartEDA, easystats, gtsummary, ggstatsplot)\n\n\n\nImporting Data\n\ncar_resale &lt;-\n  read_xls(\"data/ToyotaCorolla.xls\", \"data\")\n\n\nglimpse(car_resale)\n\nRows: 1,436\nColumns: 38\n$ Id               &lt;dbl&gt; 81, 1, 2, 3, 4, 5, 6, 7, 8, 44, 45, 46, 47, 49, 51, 6…\n$ Model            &lt;chr&gt; \"TOYOTA Corolla 1.6 5drs 1 4/5-Doors\", \"TOYOTA Coroll…\n$ Price            &lt;dbl&gt; 18950, 13500, 13750, 13950, 14950, 13750, 12950, 1690…\n$ Age_08_04        &lt;dbl&gt; 25, 23, 23, 24, 26, 30, 32, 27, 30, 27, 22, 23, 27, 2…\n$ Mfg_Month        &lt;dbl&gt; 8, 10, 10, 9, 7, 3, 1, 6, 3, 6, 11, 10, 6, 11, 11, 11…\n$ Mfg_Year         &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002,…\n$ KM               &lt;dbl&gt; 20019, 46986, 72937, 41711, 48000, 38500, 61000, 9461…\n$ Quarterly_Tax    &lt;dbl&gt; 100, 210, 210, 210, 210, 210, 210, 210, 210, 234, 234…\n$ Weight           &lt;dbl&gt; 1180, 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245,…\n$ Guarantee_Period &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ HP_Bin           &lt;chr&gt; \"100-120\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100…\n$ CC_bin           &lt;chr&gt; \"1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", …\n$ Doors            &lt;dbl&gt; 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3,…\n$ Gears            &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Cylinders        &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ Fuel_Type        &lt;chr&gt; \"Petrol\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Di…\n$ Color            &lt;chr&gt; \"Blue\", \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"…\n$ Met_Color        &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ Automatic        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mfr_Guarantee    &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ BOVAG_Guarantee  &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ABS              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_1         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_2         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airco            &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Automatic_airco  &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,…\n$ Boardcomputer    &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ CD_Player        &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,…\n$ Central_Lock     &lt;dbl&gt; 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Powered_Windows  &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Power_Steering   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Radio            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mistlamps        &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Sport_Model      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ Backseat_Divider &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Metallic_Rim     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Radio_cassette   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Tow_Bar          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\nlist(car_resale)\n\n[[1]]\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\n\nsummary(car_resale)\n\n       Id            Model               Price         Age_08_04    \n Min.   :   1.0   Length:1436        Min.   : 4350   Min.   : 1.00  \n 1st Qu.: 361.8   Class :character   1st Qu.: 8450   1st Qu.:44.00  \n Median : 721.5   Mode  :character   Median : 9900   Median :61.00  \n Mean   : 721.6                      Mean   :10731   Mean   :55.95  \n 3rd Qu.:1081.2                      3rd Qu.:11950   3rd Qu.:70.00  \n Max.   :1442.0                      Max.   :32500   Max.   :80.00  \n   Mfg_Month         Mfg_Year          KM         Quarterly_Tax   \n Min.   : 1.000   Min.   :1998   Min.   :     1   Min.   : 19.00  \n 1st Qu.: 3.000   1st Qu.:1998   1st Qu.: 43000   1st Qu.: 69.00  \n Median : 5.000   Median :1999   Median : 63390   Median : 85.00  \n Mean   : 5.549   Mean   :2000   Mean   : 68533   Mean   : 87.12  \n 3rd Qu.: 8.000   3rd Qu.:2001   3rd Qu.: 87021   3rd Qu.: 85.00  \n Max.   :12.000   Max.   :2004   Max.   :243000   Max.   :283.00  \n     Weight     Guarantee_Period    HP_Bin             CC_bin         \n Min.   :1000   Min.   : 3.000   Length:1436        Length:1436       \n 1st Qu.:1040   1st Qu.: 3.000   Class :character   Class :character  \n Median :1070   Median : 3.000   Mode  :character   Mode  :character  \n Mean   :1072   Mean   : 3.815                                        \n 3rd Qu.:1085   3rd Qu.: 3.000                                        \n Max.   :1615   Max.   :36.000                                        \n     Doors           Gears         Cylinders  Fuel_Type        \n Min.   :2.000   Min.   :3.000   Min.   :4   Length:1436       \n 1st Qu.:3.000   1st Qu.:5.000   1st Qu.:4   Class :character  \n Median :4.000   Median :5.000   Median :4   Mode  :character  \n Mean   :4.033   Mean   :5.026   Mean   :4                     \n 3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:4                     \n Max.   :5.000   Max.   :6.000   Max.   :4                     \n    Color             Met_Color        Automatic       Mfr_Guarantee   \n Length:1436        Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n Class :character   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Mode  :character   Median :1.0000   Median :0.00000   Median :0.0000  \n                    Mean   :0.6748   Mean   :0.05571   Mean   :0.4095  \n                    3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n                    Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n BOVAG_Guarantee       ABS            Airbag_1         Airbag_2     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8955   Mean   :0.8134   Mean   :0.9708   Mean   :0.7228  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n     Airco        Automatic_airco   Boardcomputer      CD_Player     \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.00000   Median :0.0000   Median :0.0000  \n Mean   :0.5084   Mean   :0.05641   Mean   :0.2946   Mean   :0.2187  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n  Central_Lock    Powered_Windows Power_Steering       Radio       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.000   Median :1.0000   Median :0.0000  \n Mean   :0.5801   Mean   :0.562   Mean   :0.9777   Mean   :0.1462  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n   Mistlamps      Sport_Model     Backseat_Divider  Metallic_Rim   \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :0.000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.257   Mean   :0.3001   Mean   :0.7702   Mean   :0.2047  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n Radio_cassette      Tow_Bar      \n Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000  \n Mean   :0.1455   Mean   :0.2779  \n 3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000  \n\n\n\n\nData Overview\n\ncar_resale %&gt;%\n  ExpData(type = 1)\n\n                                          Descriptions     Value\n1                                   Sample size (nrow)      1436\n2                              No. of variables (ncol)        38\n3                    No. of numeric/interger variables        33\n4                              No. of factor variables         0\n5                                No. of text variables         5\n6                             No. of logical variables         0\n7                          No. of identifier variables         1\n8                                No. of date variables         0\n9             No. of zero variance variables (uniform)         1\n10               %. of variables having complete cases 100% (38)\n11   %. of variables having &gt;0% and &lt;50% missing cases    0% (0)\n12 %. of variables having &gt;=50% and &lt;90% missing cases    0% (0)\n13          %. of variables having &gt;=90% missing cases    0% (0)\n\n\n\ncar_resale %&gt;%\n  ExpData(type = 2)\n\n   Index    Variable_Name Variable_Type Sample_n Missing_Count Per_of_Missing\n1      1               Id       numeric     1436             0              0\n2      2            Model     character     1436             0              0\n3      3            Price       numeric     1436             0              0\n4      4        Age_08_04       numeric     1436             0              0\n5      5        Mfg_Month       numeric     1436             0              0\n6      6         Mfg_Year       numeric     1436             0              0\n7      7               KM       numeric     1436             0              0\n8      8    Quarterly_Tax       numeric     1436             0              0\n9      9           Weight       numeric     1436             0              0\n10    10 Guarantee_Period       numeric     1436             0              0\n11    11           HP_Bin     character     1436             0              0\n12    12           CC_bin     character     1436             0              0\n13    13            Doors       numeric     1436             0              0\n14    14            Gears       numeric     1436             0              0\n15    15        Cylinders       numeric     1436             0              0\n16    16        Fuel_Type     character     1436             0              0\n17    17            Color     character     1436             0              0\n18    18        Met_Color       numeric     1436             0              0\n19    19        Automatic       numeric     1436             0              0\n20    20    Mfr_Guarantee       numeric     1436             0              0\n21    21  BOVAG_Guarantee       numeric     1436             0              0\n22    22              ABS       numeric     1436             0              0\n23    23         Airbag_1       numeric     1436             0              0\n24    24         Airbag_2       numeric     1436             0              0\n25    25            Airco       numeric     1436             0              0\n26    26  Automatic_airco       numeric     1436             0              0\n27    27    Boardcomputer       numeric     1436             0              0\n28    28        CD_Player       numeric     1436             0              0\n29    29     Central_Lock       numeric     1436             0              0\n30    30  Powered_Windows       numeric     1436             0              0\n31    31   Power_Steering       numeric     1436             0              0\n32    32            Radio       numeric     1436             0              0\n33    33        Mistlamps       numeric     1436             0              0\n34    34      Sport_Model       numeric     1436             0              0\n35    35 Backseat_Divider       numeric     1436             0              0\n36    36     Metallic_Rim       numeric     1436             0              0\n37    37   Radio_cassette       numeric     1436             0              0\n38    38          Tow_Bar       numeric     1436             0              0\n   No_of_distinct_values\n1                   1436\n2                    372\n3                    236\n4                     77\n5                     12\n6                      7\n7                   1263\n8                     13\n9                     59\n10                     9\n11                     3\n12                     3\n13                     4\n14                     4\n15                     1\n16                     3\n17                    10\n18                     2\n19                     2\n20                     2\n21                     2\n22                     2\n23                     2\n24                     2\n25                     2\n26                     2\n27                     2\n28                     2\n29                     2\n30                     2\n31                     2\n32                     2\n33                     2\n34                     2\n35                     2\n36                     2\n37                     2\n38                     2\n\n\n\ncols &lt;- c(\"Mfg_Month\", \"HP_Bin\", \"CC_bin\", \"Doors\", \"Gears\", \"Cylinders\", \"Fuel_Type\", \"Color\",\n          \"Met_Color\", \"Automatic\", \"Mfr_Guarantee\", \"BOVAG_Guarantee\", \"ABS\", \"Airbag_1\",\n          \"Airbag_2\", \"Airco\", \"Automatic_airco\", \"Boardcomputer\", \"CD_Player\",\n          \"Central_Lock\", \"Powered_Windows\", \"Power_Steering\", \"Radio\", \"Mistlamps\",\n          \"Sport_Model\", \"Backseat_Divider\", \"Metallic_Rim\",\n          \"Radio_cassette\", \"Tow_Bar\")\n\ncar_resale &lt;- read_xls (\"data/ToyotaCorolla.xls\",\n                        sheet = \"data\") %&gt;%\n  mutate(Id = as.character(Id)) %&gt;%\n  mutate_each_(funs(factor(.)),cols)\n\nWarning: `mutate_each_()` was deprecated in dplyr 0.7.0.\nℹ Please use `across()` instead.\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\n\ncar_resale %&gt;%\n  ExpNumStat(by=\"A\",\n             gp=NULL,\n             Qnt=seq(0,1,0.1),\n             MesofShape = 2,\n             Outlier=TRUE,\n             round=2,\n             Nlim=10)\n\n          Vname Group   TN nNeg nZero nPos NegInf PosInf NA_Value\n2     Age_08_04   All 1436    0     0 1436      0      0        0\n3            KM   All 1436    0     0 1436      0      0        0\n1         Price   All 1436    0     0 1436      0      0        0\n4 Quarterly_Tax   All 1436    0     0 1436      0      0        0\n5        Weight   All 1436    0     0 1436      0      0        0\n  Per_of_Missing      sum  min    max     mean  median       SD   CV      IQR\n2              0    80340    1     80    55.95    61.0    18.60 0.33    26.00\n3              0 98413761    1 243000 68533.26 63389.5 37506.45 0.55 44020.75\n1              0 15409464 4350  32500 10730.82  9900.0  3626.96 0.34  3500.00\n4              0   125108   19    283    87.12    85.0    41.13 0.47    16.00\n5              0  1540052 1000   1615  1072.46  1070.0    52.64 0.05    45.00\n  Skewness Kurtosis   0%     10%   20%   30%   40%     50%   60%     70%   80%\n2    -0.83    -0.08    1    27.0    40    49    55    61.0    65    68.0    73\n3     1.01     1.68    1 26241.5 37320 47355 56349 63389.5 72090 81094.5 94606\n1     1.70     3.72 4350  7450.0  7950  8750  8950  9900.0 10500 11250.0 12500\n4     1.99     4.28   19    69.0    69    69    69    85.0    85    85.0    85\n5     3.11    19.29 1000  1020.0  1035  1050  1050  1070.0  1075  1080.0  1105\n       90%   100%    LB.25%   UB.75% nOutliers\n2     77.0     80      5.00    109.0         7\n3 117087.5 243000 -23031.12 153051.9        49\n1  15950.0  32500   3200.00  17200.0       110\n4    185.0    283     45.00    109.0       224\n5   1130.0   1615    972.50   1152.5        66\n\n\n\ncar_resale %&gt;%\n  ExpNumViz(target=NULL,\n            nlim=10,\n            Page=c(2,2))\n\n$`0`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncar_resale %&gt;%\n  ExpNumViz(target=\"Price\",\n            nlim=10,\n            Page=c(2,2))\n\n$`0`\n\n\n\n\n\n\n\n\n\nBar plots for all categorical variables\n\ncar_resale %&gt;%\n  ExpCatViz(target=NULL,\n            col =\"sky blue\",\n            clim=10,\n            margin=2,\n            Page = c(4,4),\n            sample=16)\n\n$`0`\n\n\n\n\n\n\n\n\n\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n            Weight + Guarantee_Period, data = car_resale)\ncheck_normality(model1)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\nplot(model1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncheck_heteroscedasticity(model1)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001).\n\n\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\n\nsummary(model1)\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10249.4   -768.6    -15.4    738.5   6356.5 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -2.186e+03  9.722e+02  -2.248   0.0247 *  \nAge_08_04        -1.195e+02  2.760e+00 -43.292   &lt;2e-16 ***\nKM               -2.406e-02  1.201e-03 -20.042   &lt;2e-16 ***\nWeight            1.972e+01  8.379e-01  23.533   &lt;2e-16 ***\nGuarantee_Period  2.682e+01  1.261e+01   2.126   0.0336 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1413 on 1431 degrees of freedom\nMultiple R-squared:  0.8486,    Adjusted R-squared:  0.8482 \nF-statistic:  2005 on 4 and 1431 DF,  p-value: &lt; 2.2e-16\n\n\n\ntbl_regression(model1,\n               intercept = TRUE)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n-2,186\n-4,093, -278\n0.025\n\n\nAge_08_04\n-119\n-125, -114\n&lt;0.001\n\n\nKM\n-0.02\n-0.03, -0.02\n&lt;0.001\n\n\nWeight\n20\n18, 21\n&lt;0.001\n\n\nGuarantee_Period\n27\n2.1, 52\n0.034\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\ntbl_regression(model1,\n               intercept = TRUE) %&gt;%\n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared,\n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-2,186\n-4,093, -278\n0.025\n    Age_08_04\n-119\n-125, -114\n&lt;0.001\n    KM\n-0.02\n-0.03, -0.02\n&lt;0.001\n    Weight\n20\n18, 21\n&lt;0.001\n    Guarantee_Period\n27\n2.1, 52\n0.034\n  \n  \n    \n      R² = 0.849; Adjusted R² = 0.848; AIC = 24,915; Statistic = 2,005; p-value = &lt;0.001; σ = 1,413\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\nP_model1 &lt;- parameters(model1)\n\n\nplot(parameters(model1))\n\n\n\n\n\n\n\n\n\nggcoefstats(model1,\n             output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#plotting-calendar-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#plotting-calendar-heatmap",
    "title": "Hands-on Exercise 7",
    "section": "",
    "text": "In this section, you will learn how to plot a calender heatmap programmatically by using ggplot2 package.\nBy the end of this section, you will be able to:\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\n\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n\n\nFirst, you will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\nRows: 199999 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): source_country, tz\ndttm (1): timestamp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nFor example, kable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\n\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\nSys.setlocale(\"LC_TIME\", \"C\")\n\n[1] \"C\"\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\n\nwindowsFonts(Arial = windowsFont(\"Arial\"))\n\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  filter(!is.na(wkday), !is.na(hour))\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n            linewidth = 0.1) + \n  theme_tufte(base_family = \"Arial\")  + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                      low = \"sky blue\", \n                      high = \"dark blue\") +\n  labs(x = NULL, \n       y = NULL, \n       title = \"Attacks by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\nThen we can simply group the count by hour and wkday and plot it, since we know that we have values for every combination there’s no need to further preprocess the data.\n\n\n\nChallenge: Building multiple heatmaps for the top four countries with the highest number of attacks.\n\n\n\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  filter(!is.na(source_country), !is.na(wkday), !is.na(hour))\n\n\n\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nif (nrow(top4_attacks) &gt; 0) {\n  ggplot(top4_attacks, \n         aes(hour, \n             wkday, \n             fill = n)) + \n    geom_tile(color = \"white\", \n              linewidth = 0.1) + \n    theme_tufte(base_family = \"Helvetica\") + \n    coord_equal() +\n    scale_fill_gradient(name = \"# of attacks\",\n                        low = \"sky blue\", \n                        high = \"dark blue\") +\n    facet_wrap(~source_country, ncol = 2) +\n    labs(x = NULL, y = NULL, \n         title = \"Attacks on top 4 countries by weekday and time of day\") +\n    theme(axis.ticks = element_blank(),\n          axis.text.x = element_text(size = 7),\n          plot.title = element_text(hjust = 0.5),\n          legend.title = element_text(size = 8),\n          legend.text = element_text(size = 6) )\n} else {\n  print(\"No data available for the selected countries.\")\n}\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#plotting-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#plotting-cycle-plot",
    "title": "Hands-on Exercise 7",
    "section": "",
    "text": "In this section, you will learn how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\n\nNext, the code chunk below is use to extract data for the target country (i.e. Vietnam)\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\n\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\nThe code chunk below is used to plot the cycle plot as shown in Slide 12/23.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#plotting-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#plotting-slopegraph",
    "title": "Hands-on Exercise 7",
    "section": "",
    "text": "In this section you will learn how to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\n\nImport the rice data set into R environment by using the code chunk below.\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\nRows: 550 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Country\ndbl (3): Year, Yield, Production\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\nConverting 'Year' to an ordered factor"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home_Ex02",
    "section": "",
    "text": "We try to apply newly acquired techniques to explore and analyze the changing trends and patterns of Singapore’s international trade since 2015.\n\n\n\nSingapore is a major global trade hub, and its trade patterns reflect economic trends, policy changes, and global events. Since 2015, factors like the U.S.-China trade war, COVID-19, and economic recovery have influenced trade fluctuations.This analysis aims to explore Singapore’s international trade trends using data visualization and time-series analysis.\na. Critique Existing Visualizations – Identify strengths and weaknesses of current trade charts.\nb. Create Improved Visualizations – Use ggplot2 and R packages to enhance clarity. Perform\nc. Time-Series Analysis and Forecasting – Analyze trade trends and predict future movements.\nBy combining visual and analytical approaches, we aim to uncover insights into Singapore’s trade patterns and anticipate future trends.\n\n\n\nCritique Existing Visualizations\nSelect three visualizations from the provided webpage.\nAnalyze and comment on their pros and cons (strengths and weaknesses).\nProvide sketches of redesigned versions (make-over) to improve the visualizations.\nCreate Improved Visualizations\nUse ggplot2 and other appropriate R packages.\nImplement the make-over versions of the three visualizations with improved design and clarity.\nTime-Series Analysis or Forecasting\nAnalyze the data using time-series analysis or time-series forecasting techniques.\nSupport the analysis with appropriate visualizations and relevant R packages.\n\n\n\n\n\nFor this exercise, we load the following R packages using the pacman::p_load() function, R packages including: scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table and tidyverse.\n\npacman::p_load(scales, viridis, lubridate, ggthemes,\n               gridExtra, readxl, knitr, data.table,\n               CGPfunctions, ggHoriPlot, tidyverse)\n\nwe can download 3 files (M451491, M451501, M451511) from the website (download Merchandise Trade by Region/Market from Department of Statistics Singapore, DOS), and the 3 files represent different data, as shown below\nM451491-Merchandise Trade By Region and Selected Market (Imports), Monthly\nM451501-Merchandise Trade By Region and Selected Market (Domestic Exports),Monthly\nM451511-Merchandise Trade By Region and Selected Market (Re-Exports), Monthly\n\n\n\n\ndf_Im &lt;- read_csv(\"data/M451491.csv\", skip = 10, show_col_types = FALSE)\nhead(df_Im)\n\n# A tibble: 6 × 266\n  `Data Series`       `2025 Jan` `2024 Dec` `2024 Nov` `2024 Oct` `2024 Sep`\n  &lt;chr&gt;                    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Total All Markets       54746.    56136.      51802.    51416.     49068. \n2 America                  6923.     7874.       7880.     8078.      9112  \n3 Antigua And Barbuda         0         0           0         0          0  \n4 Argentina                   4        12.5       116.        4.1        8.1\n5 Bahamas                     0         8.1         0         0          0  \n6 Bermuda                     0         0           0         0          0  \n# ℹ 260 more variables: `2024 Aug` &lt;dbl&gt;, `2024 Jul` &lt;dbl&gt;, `2024 Jun` &lt;dbl&gt;,\n#   `2024 May` &lt;dbl&gt;, `2024 Apr` &lt;dbl&gt;, `2024 Mar` &lt;dbl&gt;, `2024 Feb` &lt;dbl&gt;,\n#   `2024 Jan` &lt;dbl&gt;, `2023 Dec` &lt;dbl&gt;, `2023 Nov` &lt;dbl&gt;, `2023 Oct` &lt;dbl&gt;,\n#   `2023 Sep` &lt;dbl&gt;, `2023 Aug` &lt;dbl&gt;, `2023 Jul` &lt;dbl&gt;, `2023 Jun` &lt;dbl&gt;,\n#   `2023 May` &lt;dbl&gt;, `2023 Apr` &lt;dbl&gt;, `2023 Mar` &lt;dbl&gt;, `2023 Feb` &lt;dbl&gt;,\n#   `2023 Jan` &lt;dbl&gt;, `2022 Dec` &lt;dbl&gt;, `2022 Nov` &lt;dbl&gt;, `2022 Oct` &lt;dbl&gt;,\n#   `2022 Sep` &lt;dbl&gt;, `2022 Aug` &lt;dbl&gt;, `2022 Jul` &lt;dbl&gt;, `2022 Jun` &lt;dbl&gt;, …\n\n\n\ndf_DE &lt;- read_csv(\"data/M451501.csv\", skip = 10, show_col_types = FALSE)\ndf_RE &lt;- read_csv(\"data/M451511.csv\", skip = 10, show_col_types = FALSE)\ndf_commodity &lt;- read_csv(\"data/M451001.csv\", skip = 10, show_col_types = FALSE)\nhead(df_DE)\n\n# A tibble: 6 × 266\n  `Data Series`       `2025 Jan` `2024 Dec` `2024 Nov` `2024 Oct` `2024 Sep`\n  &lt;chr&gt;                    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Total All Markets      24672      23685.     23439.     22148.     21967. \n2 America                 4263.      3608.      3129.      2936.      3294. \n3 Antigua And Barbuda        8.8        7.1        8.3        7.7        8.2\n4 Argentina                  5.3        6          3.9        6.4        3  \n5 Bahamas                   51.4       48.1       60.5       36.1       59.3\n6 Bermuda                    2.7        7.7        0.5        5.4        0.7\n# ℹ 260 more variables: `2024 Aug` &lt;dbl&gt;, `2024 Jul` &lt;dbl&gt;, `2024 Jun` &lt;dbl&gt;,\n#   `2024 May` &lt;dbl&gt;, `2024 Apr` &lt;dbl&gt;, `2024 Mar` &lt;dbl&gt;, `2024 Feb` &lt;dbl&gt;,\n#   `2024 Jan` &lt;dbl&gt;, `2023 Dec` &lt;dbl&gt;, `2023 Nov` &lt;dbl&gt;, `2023 Oct` &lt;dbl&gt;,\n#   `2023 Sep` &lt;dbl&gt;, `2023 Aug` &lt;dbl&gt;, `2023 Jul` &lt;dbl&gt;, `2023 Jun` &lt;dbl&gt;,\n#   `2023 May` &lt;dbl&gt;, `2023 Apr` &lt;dbl&gt;, `2023 Mar` &lt;dbl&gt;, `2023 Feb` &lt;dbl&gt;,\n#   `2023 Jan` &lt;dbl&gt;, `2022 Dec` &lt;dbl&gt;, `2022 Nov` &lt;dbl&gt;, `2022 Oct` &lt;dbl&gt;,\n#   `2022 Sep` &lt;dbl&gt;, `2022 Aug` &lt;dbl&gt;, `2022 Jul` &lt;dbl&gt;, `2022 Jun` &lt;dbl&gt;, …\n\nhead(df_RE)\n\n# A tibble: 6 × 266\n  `Data Series`       `2025 Jan` `2024 Dec` `2024 Nov` `2024 Oct` `2024 Sep`\n  &lt;chr&gt;                    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Total All Markets      34736.     36458.     34892.     33963.     32477. \n2 America                 3110.      3893.      3528.      3389.      3197. \n3 Antigua And Barbuda        1.9        0.1        0          0.2        0  \n4 Argentina                 25.7       22.3       19.4       23.8       27.1\n5 Bahamas                    9.8        1.3        3.7        4.3        8.1\n6 Bermuda                    0.1        0          0          0.1        0  \n# ℹ 260 more variables: `2024 Aug` &lt;dbl&gt;, `2024 Jul` &lt;dbl&gt;, `2024 Jun` &lt;dbl&gt;,\n#   `2024 May` &lt;dbl&gt;, `2024 Apr` &lt;dbl&gt;, `2024 Mar` &lt;dbl&gt;, `2024 Feb` &lt;dbl&gt;,\n#   `2024 Jan` &lt;dbl&gt;, `2023 Dec` &lt;dbl&gt;, `2023 Nov` &lt;dbl&gt;, `2023 Oct` &lt;dbl&gt;,\n#   `2023 Sep` &lt;dbl&gt;, `2023 Aug` &lt;dbl&gt;, `2023 Jul` &lt;dbl&gt;, `2023 Jun` &lt;dbl&gt;,\n#   `2023 May` &lt;dbl&gt;, `2023 Apr` &lt;dbl&gt;, `2023 Mar` &lt;dbl&gt;, `2023 Feb` &lt;dbl&gt;,\n#   `2023 Jan` &lt;dbl&gt;, `2022 Dec` &lt;dbl&gt;, `2022 Nov` &lt;dbl&gt;, `2022 Oct` &lt;dbl&gt;,\n#   `2022 Sep` &lt;dbl&gt;, `2022 Aug` &lt;dbl&gt;, `2022 Jul` &lt;dbl&gt;, `2022 Jun` &lt;dbl&gt;, …\n\nhead(df_commodity)\n\n# A tibble: 6 × 734\n  `Data Series`           `2025 Jan` `2024 Dec` `2024 Nov` `2024 Oct` `2024 Sep`\n  &lt;chr&gt;                        &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Total Merchandise Trad… 114153980. 116278793. 110132324. 107525960. 103512460.\n2 Oil                      19490290.  18488974.  18061885.  17510050.  15686654.\n3 Petroleum                16418934.  15564654.  14910686.  14804334.  12611958.\n4 Oil Bunkers               3071356.   2924319.   3151200.   2705716.   3074695.\n5 Non-Oil                  94663690.  97789819.  92070439.  90015910.  87825806.\n6 Food & Live Animals       2460911.   2704579.   2237326.   2344745.   2235005.\n# ℹ 728 more variables: `2024 Aug` &lt;dbl&gt;, `2024 Jul` &lt;dbl&gt;, `2024 Jun` &lt;dbl&gt;,\n#   `2024 May` &lt;dbl&gt;, `2024 Apr` &lt;dbl&gt;, `2024 Mar` &lt;dbl&gt;, `2024 Feb` &lt;dbl&gt;,\n#   `2024 Jan` &lt;dbl&gt;, `2023 Dec` &lt;dbl&gt;, `2023 Nov` &lt;dbl&gt;, `2023 Oct` &lt;dbl&gt;,\n#   `2023 Sep` &lt;dbl&gt;, `2023 Aug` &lt;dbl&gt;, `2023 Jul` &lt;dbl&gt;, `2023 Jun` &lt;dbl&gt;,\n#   `2023 May` &lt;dbl&gt;, `2023 Apr` &lt;dbl&gt;, `2023 Mar` &lt;dbl&gt;, `2023 Feb` &lt;dbl&gt;,\n#   `2023 Jan` &lt;dbl&gt;, `2022 Dec` &lt;dbl&gt;, `2022 Nov` &lt;dbl&gt;, `2022 Oct` &lt;dbl&gt;,\n#   `2022 Sep` &lt;dbl&gt;, `2022 Aug` &lt;dbl&gt;, `2022 Jul` &lt;dbl&gt;, `2022 Jun` &lt;dbl&gt;, …\n\n\nAfter viewing the whole table, we should change the type of date series.\nFirst column (Data Series): Country/Region Name, Subsequent columns: Monthly data, such as “2025 Jan”, “2024 Dec”, “2024 Nov”, representing trade data for different months.\nThe column name is in YYYY Mon format (needs to be converted to standard time format YYYY-MM). Data needs to be converted to Long Format for time series analysis.\n\n# rename \"Data Series\" as \"Country\"\ncolnames(df_Im)[1] &lt;- \"Country\"\ncolnames(df_DE)[1] &lt;- \"Country\"\ncolnames(df_RE)[1] &lt;- \"Country\"\n\n# Extract time column names and convert them to YYYY-MM format\ntime_cols &lt;- colnames(df_Im)[-1]  \ntime_cols_cleaned &lt;- format(parse_date_time(time_cols, orders = \"ym\"), \"%Y-%m\")\n\ncolnames(df_Im)[-1] &lt;- time_cols_cleaned\ncolnames(df_DE)[-1] &lt;- time_cols_cleaned\ncolnames(df_RE)[-1] &lt;- time_cols_cleaned\n\nhead(df_Im)\n\n# A tibble: 6 × 266\n  Country  `2025-01` `2024-12` `2024-11` `2024-10` `2024-09` `2024-08` `2024-07`\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Total A…    54746.   56136.     51802.   51416.    49068.    49949     52965. \n2 America      6923.    7874.      7880.    8078.     9112      8581.     8462  \n3 Antigua…        0        0          0        0         0         0         0  \n4 Argenti…        4       12.5      116.       4.1       8.1       7.2       6.6\n5 Bahamas         0        8.1        0        0         0         0         0  \n6 Bermuda         0        0          0        0         0         0         0.1\n# ℹ 258 more variables: `2024-06` &lt;dbl&gt;, `2024-05` &lt;dbl&gt;, `2024-04` &lt;dbl&gt;,\n#   `2024-03` &lt;dbl&gt;, `2024-02` &lt;dbl&gt;, `2024-01` &lt;dbl&gt;, `2023-12` &lt;dbl&gt;,\n#   `2023-11` &lt;dbl&gt;, `2023-10` &lt;dbl&gt;, `2023-09` &lt;dbl&gt;, `2023-08` &lt;dbl&gt;,\n#   `2023-07` &lt;dbl&gt;, `2023-06` &lt;dbl&gt;, `2023-05` &lt;dbl&gt;, `2023-04` &lt;dbl&gt;,\n#   `2023-03` &lt;dbl&gt;, `2023-02` &lt;dbl&gt;, `2023-01` &lt;dbl&gt;, `2022-12` &lt;dbl&gt;,\n#   `2022-11` &lt;dbl&gt;, `2022-10` &lt;dbl&gt;, `2022-09` &lt;dbl&gt;, `2022-08` &lt;dbl&gt;,\n#   `2022-07` &lt;dbl&gt;, `2022-06` &lt;dbl&gt;, `2022-05` &lt;dbl&gt;, `2022-04` &lt;dbl&gt;, …\n\n\nAt present, the data is in Wide Format, and we need to convert it to Long Format for time series analysis:\n\ndf_Im_long &lt;- df_Im %&gt;%\n  pivot_longer(cols = -Country, names_to = \"Date\", values_to = \"Trade_Value\")\n\ndf_DE_long &lt;- df_DE %&gt;%\n  pivot_longer(cols = -Country, names_to = \"Date\", values_to = \"Trade_Value\")\n\ndf_RE_long &lt;- df_RE %&gt;%\n  pivot_longer(cols = -Country, names_to = \"Date\", values_to = \"Trade_Value\")\n\n\ndf_Im_long$Date &lt;- as.Date(paste0(df_Im_long$Date, \"-01\"))  # YYYY-MM-DD\ndf_DE_long$Date &lt;- as.Date(paste0(df_DE_long$Date, \"-01\"))\ndf_RE_long$Date &lt;- as.Date(paste0(df_RE_long$Date, \"-01\"))\n\nhead(df_Im_long)\n\n# A tibble: 6 × 3\n  Country           Date       Trade_Value\n  &lt;chr&gt;             &lt;date&gt;           &lt;dbl&gt;\n1 Total All Markets 2025-01-01      54746.\n2 Total All Markets 2024-12-01      56136.\n3 Total All Markets 2024-11-01      51802.\n4 Total All Markets 2024-10-01      51416.\n5 Total All Markets 2024-09-01      49068.\n6 Total All Markets 2024-08-01      49949 \n\nhead(df_DE_long)\n\n# A tibble: 6 × 3\n  Country           Date       Trade_Value\n  &lt;chr&gt;             &lt;date&gt;           &lt;dbl&gt;\n1 Total All Markets 2025-01-01      24672 \n2 Total All Markets 2024-12-01      23685.\n3 Total All Markets 2024-11-01      23439.\n4 Total All Markets 2024-10-01      22148.\n5 Total All Markets 2024-09-01      21967.\n6 Total All Markets 2024-08-01      23984 \n\nhead(df_RE_long)\n\n# A tibble: 6 × 3\n  Country           Date       Trade_Value\n  &lt;chr&gt;             &lt;date&gt;           &lt;dbl&gt;\n1 Total All Markets 2025-01-01      34736.\n2 Total All Markets 2024-12-01      36458.\n3 Total All Markets 2024-11-01      34892.\n4 Total All Markets 2024-10-01      33963.\n5 Total All Markets 2024-09-01      32477.\n6 Total All Markets 2024-08-01      31776.\n\n\nSave the cleaned data\n\nwrite_csv(df_Im_long, \"data/cleaned_Im.csv\")\nwrite_csv(df_DE_long, \"data/cleaned_DE.csv\")\nwrite_csv(df_RE_long, \"data/cleaned_RE.csv\")\n\nAfter completing the data cleaning, we began to gradually complete the task.\n\n\n\n\n\n\n\n\n\n\nIntuitively displays the import and export situation of major trading partners. The size of the bubble represents the trade volume, which is clear at a glance. Colors distinguish the proportion of imports and exports for easy comparison.\n\n\n\nDifficulty in accurately comparing bubble sizes.\nLarge space occupation, unable to display more countries.\nTime trend not shown, limited to 2024\n\n\n\nfirst, merge export from the two file\n\ndf_DE &lt;- read_csv(\"data/cleaned_DE.csv\") \n\nRows: 47965 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Country\ndbl  (1): Trade_Value\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_RE &lt;- read_csv(\"data/cleaned_RE.csv\")  \n\nRows: 47965 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Country\ndbl  (1): Trade_Value\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_DE &lt;- df_DE %&gt;% mutate(Date = as.Date(Date))\ndf_RE &lt;- df_RE %&gt;% mutate(Date = as.Date(Date))\n\n\ndf_merged_export &lt;- full_join(df_DE, df_RE, by = c(\"Country\", \"Date\"), suffix = c(\"_DE\", \"_RE\"))\n\nWarning in full_join(df_DE, df_RE, by = c(\"Country\", \"Date\"), suffix = c(\"_DE\", : Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 42401 of `x` matches multiple rows in `y`.\nℹ Row 42401 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\ndf_merged_export &lt;- df_merged_export %&gt;%\n  mutate(Export_Trade_Value = coalesce(Trade_Value_DE, 0) + coalesce(Trade_Value_RE, 0))\n\n\ndf_merged_export &lt;- df_merged_export %&gt;%\n  select(Country, Date, Trade_Value_DE, Trade_Value_RE, Export_Trade_Value)\n\nwrite_csv(df_merged_export, \"data/merged_export.csv\")\n\n\ndf_Im &lt;- read_csv(\"data/cleaned_Im.csv\")\n\nRows: 46905 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Country\ndbl  (1): Trade_Value\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_merged_export &lt;- read_csv(\"data/merged_export.csv\")\n\nRows: 51675 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Country\ndbl  (3): Trade_Value_DE, Trade_Value_RE, Export_Trade_Value\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nexclude_regions &lt;- c(\"Total All Markets\", \"America\", \"Asia\", \"Europe\", \"Oceania\", \"Africa\")\ndf_Im &lt;- df_Im %&gt;% filter(!Country %in% exclude_regions)\ndf_merged_export &lt;- df_merged_export %&gt;% filter(!Country %in% exclude_regions)\n\n\ndf_Im &lt;- df_Im %&gt;% mutate(Date = as.Date(Date))\ndf_merged_export &lt;- df_merged_export %&gt;% mutate(Date = as.Date(Date))\n\n\ndf_Im_2024 &lt;- df_Im %&gt;% filter(format(Date, \"%Y\") == \"2024\")\ndf_merged_export_2024 &lt;- df_merged_export %&gt;% filter(format(Date, \"%Y\") == \"2024\")\n\n\ndf_Im_agg &lt;- df_Im_2024 %&gt;% group_by(Country) %&gt;% summarise(Import = sum(Trade_Value, na.rm = TRUE)/1000)\ndf_merged_export_agg &lt;- df_merged_export_2024 %&gt;% group_by(Country) %&gt;% summarise(Export = sum(Export_Trade_Value, na.rm = TRUE)/1000)\n\n\ndf_total_trade &lt;- left_join(df_Im_agg, df_merged_export_agg, by = \"Country\") %&gt;%\n  mutate(Total_Trade = Import + Export)\n\n\ntop_10_trade_partners &lt;- df_total_trade %&gt;%\n  arrange(desc(Total_Trade)) %&gt;%\n  slice(1:10)\n\n\ndf_long &lt;- top_10_trade_partners %&gt;%\n  select(Country, Import, Export) %&gt;%\n  pivot_longer(cols = c(\"Import\", \"Export\"), names_to = \"Trade_Type\", values_to = \"Value\")\n\n\nggplot(df_long, aes(x = reorder(Country, -Value), y = Value, fill = Trade_Type)) +\n  geom_bar(stat = \"identity\") +\n  \n  geom_text(aes(label = round(Value, 1)), position = position_stack(vjust = 0.5), size = 3, color = \"white\") +\n  \n  geom_text(data = top_10_trade_partners, \n            aes(x = Country, y = Total_Trade, label = paste0(\"Total: \", round(Total_Trade, 1))),\n            vjust = -0.5, size = 3, fontface = \"bold\", inherit.aes = FALSE) +\n\n  labs(title = \"Singapore's Top 10 Trading Partners (2024)\",\n       x = \"Country\",\n       y = \"Trade Value (S$ Billion)\",\n       fill = \"Trade Type\") +\n  \n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),\n    axis.text.y = element_text(size = 8),\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 8, face = \"italic\"),\n    legend.position = \"top\",\n    legend.title = element_blank()\n  )\n\n\n\n\n\n\n\n\nWe will create an interactive chart in shinyapp that allows viewers to select a year and see the top 10 trade partners of each year.\nShinyapp - Click here to view the interactive Shiny app\n\n\n\nWe need to download the file (M451001)-Merchandise Trade By Commodity Section, (At Current Prices), Monthly\n\n\n\nStrength and weakness:\n\n\nStrengths:\n\nClear Category Breakdown – The chart categorizes key commodities and highlights export (orange) and import (blue) values separately for easy comparison.\nHelpful Numerical Labels – Each category displays exact trade values and percentages, improving readability.\nSimple and Effective Design – The color contrast and “Top 3 Commodity Sections” summary make it visually appealing and easy to interpret.\n\nWeakness:\n\nExport vs. Import Comparison is Misleading – Different scales make the gaps between exports and imports look bigger than they actually are.\nNo Overall Export vs. Import Ratio – The chart lacks a clear visualization of total exports vs. imports, making it harder to see the overall balance.\nNo Trend Analysis – It only shows 2024 data, missing past trends that could help understand market changes over time.\n\n\n\n\nFirst we have to clean dataset follow the code.\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\ndf_commodity &lt;- read_csv(\"data/M451001.csv\", skip = 10, show_col_types = FALSE)\n\ncolnames(df_commodity)[1] &lt;- \"Category\"\n\n\ntime_cols &lt;- colnames(df_commodity)[-1]\n\n\nvalid_time_cols &lt;- time_cols[!is.na(parse_date_time(time_cols, orders = c(\"Y b\", \"b Y\")))]\ntime_cols_cleaned &lt;- format(parse_date_time(valid_time_cols, orders = c(\"Y b\", \"b Y\")), \"%Y-%m\")\n\ncolnames(df_commodity) &lt;- c(\"Category\", time_cols_cleaned)\n\ncolumns_to_keep &lt;- c(\"Category\", time_cols_cleaned[year(parse_date_time(time_cols_cleaned, orders = \"ym\")) &gt;= 2010 & \n                                                    year(parse_date_time(time_cols_cleaned, orders = \"ym\")) &lt;= 2025])\n\ndf_filtered &lt;- df_commodity %&gt;%\n  select(all_of(columns_to_keep))\n\ndf_import &lt;- df_filtered[19:27, ] %&gt;% mutate(Type = \"Import\")\ndf_export &lt;- df_filtered[33:41, ] %&gt;% mutate(Type = \"Export\")\n\ndf_cleaned &lt;- bind_rows(df_import, df_export)\n\ndf_long &lt;- df_cleaned %&gt;%\n  pivot_longer(cols = -c(Category, Type), names_to = \"Date\", values_to = \"Trade_Value\")\n\nprint(head(df_long))\n\n# A tibble: 6 × 4\n  Category            Type   Date    Trade_Value\n  &lt;chr&gt;               &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;\n1 Food & Live Animals Import 2025-01    1227051.\n2 Food & Live Animals Import 2024-12    1264573.\n3 Food & Live Animals Import 2024-11    1192495.\n4 Food & Live Animals Import 2024-10    1219199.\n5 Food & Live Animals Import 2024-09    1130865.\n6 Food & Live Animals Import 2024-08    1238345.\n\nwrite_csv(df_long, \"data/cleaned_com.csv\")\n\nBy usig the file - cleaned-com.csv, we will create an interactive chart in shinyapp that allows viewers to select a year and see Top 10 Trading Partners and Non-Oil Merchandise Trade. We can compare every commodity export and import summary and the trend over years.\n\nShinyapp - Click here to view the interactive Shiny app\nhttp://wangxingyun.shinyapps.io/take_home_Ex02\n\n\n\n\n\nStrengths:\nClearly displays the annual import and export values.\nDifferent colors for each year make the visualization clear and easy to understand.\nWeaknesses:\nThe 6.6% increase does not specify which year it is being compared to.\nThe chart lacks a trend showing the changes over previous years.\nImprovements:\nCreate three separate tables:\nImport Table: Displays annual import values and indicates whether they increased or decreased compared to the previous year, along with the percentage change.\nExport Table: Shows annual export values and their trend.\nTotal Trade Table: Displays the sum of imports and exports for each year, including the percentage change compared to the previous year.\n\n\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\n\ndf_Im &lt;- read_csv(\"data/cleaned_Im.csv\", show_col_types = FALSE)\ndf_merged_export &lt;- read_csv(\"data/merged_export.csv\", show_col_types = FALSE)\n\n\ndf_Im &lt;- df_Im %&gt;% mutate(Date = as.Date(Date))\ndf_merged_export &lt;- df_merged_export %&gt;% mutate(Date = as.Date(Date))\n\n\ndf_import &lt;- df_Im %&gt;% \n  filter(Country == \"Total All Markets\") %&gt;%\n  select(Date, Import = Trade_Value) %&gt;%\n  mutate(Import = round(Import / 1000, 2))  \n\ndf_export &lt;- df_merged_export %&gt;%\n  filter(Country == \"Total All Markets\") %&gt;%\n  select(Date, Export = Export_Trade_Value) %&gt;%\n  mutate(Export = round(Export / 1000, 2))  \n\ndf_total_trade &lt;- full_join(df_import, df_export, by = \"Date\") %&gt;%\n  mutate(Total_Trade = Import + Export) %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  filter(Year &gt;= 2020 & Year &lt;= 2024) %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Import = sum(Import, na.rm = TRUE),\n            Export = sum(Export, na.rm = TRUE),\n            Total_Trade = sum(Total_Trade, na.rm = TRUE)) %&gt;%\n  arrange(Year) %&gt;%\n  mutate(Import_Change = round((Import - lag(Import)) / lag(Import) * 100, 1),\n         Export_Change = round((Export - lag(Export)) / lag(Export) * 100, 1),\n         Total_Change = round((Total_Trade - lag(Total_Trade)) / lag(Total_Trade) * 100, 1))\n\n\ncustom_colors &lt;- c(\"Import\" = \"#1f77b4\", \"Export\" = \"#ff7f0e\", \"Total Trade\" = \"#2ca02c\")\n\n\np1 &lt;- ggplot(df_total_trade, aes(x = factor(Year), y = Import)) +\n  geom_bar(stat = \"identity\", fill = custom_colors[\"Import\"], alpha = 0.7) +\n  geom_text(aes(label = sprintf(\"S$ %.2f B\", Import)), vjust = -0.5, size = 3, fontface = \"bold\") +  \n  geom_text(aes(y = Import * 0.98, label = ifelse(!is.na(Import_Change), paste0(Import_Change, \"%\"), \"\")), \n            vjust = 1.5, size = 3, color = \"red\", fontface = \"bold\") +\n  labs(title = \"Yearly Import Trend (2020-2024)\", x = \"Year\", y = \"Import Value (S$ Billion)\") +\n  theme_minimal()\n\n\np2 &lt;- ggplot(df_total_trade, aes(x = factor(Year), y = Export)) +\n  geom_bar(stat = \"identity\", fill = custom_colors[\"Export\"], alpha = 0.7) +\n  geom_text(aes(label = sprintf(\"S$ %.2f B\", Export)), vjust = -0.5, size = 3, fontface = \"bold\") +  \n  geom_text(aes(y = Export * 0.98, label = ifelse(!is.na(Export_Change), paste0(Export_Change, \"%\"), \"\")), \n            vjust = 1.5, size = 3, color = \"red\", fontface = \"bold\") +\n  labs(title = \"Yearly Export Trend (2020-2024)\", x = \"Year\", y = \"Export Value (S$ Billion)\") +\n  theme_minimal()\n\np3 &lt;- ggplot(df_total_trade, aes(x = factor(Year), y = Total_Trade)) +\n  geom_bar(stat = \"identity\", fill = custom_colors[\"Total Trade\"], alpha = 0.7) +\n  geom_text(aes(label = sprintf(\"S$ %.2f B\", Total_Trade)), vjust = -0.5, size = 3, fontface = \"bold\") +  \n  geom_text(aes(y = Total_Trade * 0.98, label = ifelse(!is.na(Total_Change), paste0(Total_Change, \"%\"), \"\")), \n            vjust = 1.5, size = 3, color = \"red\", fontface = \"bold\") +\n  labs(title = \"Total Trade (Import + Export) Trend (2020-2024)\", \n       x = \"Year\", y = \"Total Trade Value (S$ Billion)\") +\n  theme_minimal()\n\nprint(p1)  \n\n\n\n\n\n\n\nprint(p2)  \n\n\n\n\n\n\n\nprint(p3)  \n\n\n\n\n\n\n\n\n\n\n\n\nWe can first work with the dataset again and observe the data over the past 10 years.\nClean up the data and ensure its format is correct, especially the time column must be of Date type and aggregated by year/month.\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\n\ndf_Im &lt;- read_csv(\"data/cleaned_Im.csv\", show_col_types = FALSE)\ndf_merged_export &lt;- read_csv(\"data/merged_export.csv\", show_col_types = FALSE)\n\ndf_Im &lt;- df_Im %&gt;% mutate(Date = as.Date(Date), Year = year(Date), Month = month(Date))\ndf_merged_export &lt;- df_merged_export %&gt;% mutate(Date = as.Date(Date), Year = year(Date), Month = month(Date))\n\n\n\ndf_import &lt;- df_Im %&gt;% filter(Country == \"Total All Markets\") %&gt;%\n  select(Date, Year, Month, Import = Trade_Value)\n\ndf_export &lt;- df_merged_export %&gt;% filter(Country == \"Total All Markets\") %&gt;%\n  select(Date, Year, Month, Export = Export_Trade_Value)\n\n\ndf_total_trade &lt;- full_join(df_import, df_export, by = c(\"Date\", \"Year\", \"Month\")) %&gt;%\n  mutate(Total_Trade = Import + Export) %&gt;%\n  filter(Year &gt;= 2010 & Year &lt;= 2024)  \n\n\n\nSuitable for analyzing seasonal patterns in time series data, helping to identify variations across different months. Merchandise trade data is often influenced by seasonal factors such as holidays and policy changes. The Cycle Plot allows us to observe monthly trade patterns across different years, helping to detect recurring trends.\n\nlibrary(ggplot2)\n\nggplot(df_total_trade, aes(x = Month, y = Total_Trade, group = Year, color = factor(Year))) +\n  geom_line(size = 1) +\n  geom_point(size = 2) +\n  labs(title = \"Cycle Plot of Total Trade (2010-2024)\",\n       x = \"Month\", y = \"Trade Value (S$ Billion)\",\n       color = \"Year\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nObservation:\n\nThe trade value has shown a general upward trend over the years, with 2022 and 2023 having notably high trade values compared to previous years.\n2020-2024 trade values are significantly higher than 2010-2019, indicating economic expansion.\nTrade values tend to drop in the first quarter (January - March), likely due to the Chinese New Year and seasonal slowdowns.\nThere is a consistent increase mid-year (May - July) and another peak towards the year-end (October - December), possibly due to global supply chain demands, year-end trade, and holiday-related exports/imports.\n2022 and 2023 exhibit higher volatility in trade values compared to earlier years. This suggests that post-pandemic recovery and trade policy adjustments played a role in boosting trade volumes.\nThe dip in early 2020 aligns with COVID-19-related trade disruptions, but subsequent years have shown strong rebounds.\n2022 and 2023 saw exceptionally high trade values in certain months, especially around mid-year and year-end.\n\n\n\n\nSlope Graph is useful for comparing two points in time to show increases or decreases in values clearly. The Slope Graph effectively visualizes total trade volume changes between 2010 and 2024, providing a direct view of long-term growth.\n\ndf_slope &lt;- df_total_trade %&gt;%\n  filter(Year %in% c(2010, 2024)) %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Total_Trade = sum(Total_Trade, na.rm = TRUE))  \n\n\nggplot(df_slope, aes(x = factor(Year), y = Total_Trade, group = 1)) +\n  geom_line(aes(color = \"Total Trade\"), size = 1) +  \n  geom_point(size = 3, color = \"red\") +  \n  geom_text(aes(label = round(Total_Trade / 1000, 2)), vjust = -0.5, size = 2.5, fontface = \"bold\") +  \n  labs(title = \"Total Trade Change from 2010 to 2024\",\n       x = \"Year\", y = \"Trade Value (S$ Billion)\",\n       color = \"Trade Type\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")  \n\n\n\n\n\n\n\n\n11\n\ndf_slope &lt;- df_total_trade %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Total_Trade = sum(Total_Trade, na.rm = TRUE))  \n\n\nggplot(df_slope, aes(x = factor(Year), y = Total_Trade, group = 1)) +\n  geom_line(color = \"red\", size = 1) +  \n  geom_point(size = 3, color = \"red\") +  \n  geom_text(aes(label = round(Total_Trade / 1000, 2)), vjust = -0.5, size = 3, fontface = \"bold\") +  \n  labs(title = \"Total Trade Change from 2010 to 2024\",\n       x = \"Year\", y = \"Trade Value (S$ Billion)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nObservation:\n\nThe total trade value has shown a significant increase from 907.04 billion S$ in 2010 to 1,285.86 billion S$ in 2024. This indicates a strong upward trend in trade over the years.\nThe total trade value was relatively stable between 2010 and 2014, staying around 1,000 billion S$. A notable decline occurred between 2014 and 2016, reaching the lowest point in 2016 (870.22 billion S$). From 2017 onwards, trade value exhibited a sharp recovery, particularly in 2021 and 2022, where trade peaked at 1,365.4 billion S$.\nThe spike in 2021 and 2022 aligns with the post-pandemic recovery, where global supply chains and trade rebounded.\nThe slight dip in 2023 (1,205.72 billion S$) suggests potential economic uncertainties or adjustments after the strong post-pandemic growth.\nWhile the overall trend is upward, the trade values exhibit periodic declines and rebounds.\n\n\n\n\nHeatmap is suitable for displaying data distribution across different time periods, helping to identify trends, anomalies, and seasonal patterns. For my task, the Year-Month Heatmap enables a clear visualization of trade volume variations, revealing peak and low seasons over the years.\n\nggplot(df_total_trade, aes(x = Month, y = factor(Year), fill = Total_Trade)) +\n  geom_tile(color = \"white\") +\n  scale_fill_viridis_c(option = \"C\", direction = -1) +  \n  labs(title = \"Trade Value Heatmap (2010-2024)\",\n       x = \"Month\", y = \"Year\",\n       fill = \"Trade Value (S$ Billion)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nObservation:\n\nThe heatmap clearly shows an increase in trade value over time. The earlier years (2010-2016) are predominantly yellow and orange, indicating lower trade values. In the later years (2020-2024), the colors shift to red and purple, representing higher trade values, confirming a long-term increasing trend.\nThe trade value fluctuates across different months. Certain months, such as early months of 2020 and 2016, exhibit higher trade activity, as indicated by brighter yellow colors. Conversely, 2022 and 2024 show lower trade activity during certain months (dark blue colors), potentially due to economic disruptions or market slowdowns.\nA sharp increase is observed in 2021, which aligns with post-pandemic global economic recovery. In 2022, certain months show deep purple (high trade activity), while others dip, indicating potential global supply chain adjustments. The global economic downturn or policy shifts might have influenced fluctuations in trade in the later years.\nSome years display more balanced trade across months, while others have strong peaks and troughs. Trade tends to rise towards mid-year in several years, suggesting cyclical economic patterns. The year-end months (November-December) are generally stable, likely due to established supply chain and trade cycles.\n\n\n\n\nTime Series Forecasting is suitable for predicting future trends based on historical data, aiding decision-making processes. For my task: Using ARIMA to forecast trade data for 2025, we can estimate future trends and potential fluctuations in trade volume.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(tsibble)\n\nRegistered S3 methods overwritten by 'tsibble':\n  method               from \n  as_tibble.grouped_df dplyr\n  format.interval      inum \n\n\n\nAttaching package: 'tsibble'\n\n\nThe following object is masked from 'package:data.table':\n\n    key\n\n\nThe following object is masked from 'package:lubridate':\n\n    interval\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, union\n\nlibrary(fable)\n\nLoading required package: fabletools\n\ndf_total_trade &lt;- df_total_trade %&gt;%\n  mutate(Date = yearmonth(Date)) %&gt;%  \n  as_tsibble(index = Date) %&gt;%  \n  fill_gaps() %&gt;%  \n  mutate(Total_Trade = ifelse(is.na(Total_Trade), mean(Total_Trade, na.rm = TRUE), Total_Trade))  \n\n\nforecast_model &lt;- df_total_trade %&gt;%\n  model(ARIMA(Total_Trade))\n\n\nforecast_result &lt;- forecast_model %&gt;%\n  forecast(h = \"6 months\")\n\n\nautoplot(forecast_result) +\n  labs(title = \"Forecast for Total Trade\")\n\n\n\n\n\n\n\n\nObservation:\n\nThe forecast predicts a gradual increase in total trade from January 2025 to May 2025, peaking around April-May. However, there is a slight decline towards June 2025, indicating potential seasonal trade adjustments.\nConfidence Intervals: The darker blue region (80% confidence interval) represents a more probable forecast range. The lighter blue region (95% confidence interval) indicates wider uncertainty in predictions, showing possible fluctuations. The confidence intervals widen towards the future, which is expected as uncertainty increases over time.\n\n\n\n\n\nCycle Plot: Reveals monthly trade patterns and seasonal fluctuations, clearly identifying peak and low trade seasons over the years. This helps in understanding recurring trends and planning for seasonal demand variations.\nSlope Graph:Illustrates the significant increase in trade volume from 2010 to 2024, highlighting the long-term upward trend in total trade. It effectively emphasizes the steady expansion of trade activity over the past decade.\nHeatmap: Provides a detailed view of trade distribution across years and months, making it easier to spot anomalies, cyclical trends, and long-term growth patterns. This visualization helps in identifying seasonal trade fluctuations and market shifts.\nTime Series Forecasting: Projects trade volume trends for 2025, indicating potential growth with some uncertainty. The confidence intervals highlight the expected range of trade values, offering insights into possible market fluctuations and risks.\n\n\n\n\nVisualising and Analysing Time-series Data"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#overview",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#overview",
    "title": "Take-home_Ex02",
    "section": "",
    "text": "We try to apply newly acquired techniques to explore and analyze the changing trends and patterns of Singapore’s international trade since 2015."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#getting-started",
    "title": "Take-home_Ex02",
    "section": "",
    "text": "For this exercise, we load the following R packages using the pacman::p_load() function, R packages including: scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table and tidyverse.\n\npacman::p_load(scales, viridis, lubridate, ggthemes,\n               gridExtra, readxl, knitr, data.table,\n               CGPfunctions, ggHoriPlot, tidyverse)\n\nwe can download 3 files (M451491, M451501, M451511) from the website (download Merchandise Trade by Region/Market from Department of Statistics Singapore, DOS), and the 3 files represent different data, as shown below\nM451491-Merchandise Trade By Region and Selected Market (Imports), Monthly\nM451501-Merchandise Trade By Region and Selected Market (Domestic Exports),Monthly\nM451511-Merchandise Trade By Region and Selected Market (Re-Exports), Monthly\n\n\n\n\ndf_Im &lt;- read_csv(\"data/M451491.csv\", skip = 10, show_col_types = FALSE)\nhead(df_Im)\n\n# A tibble: 6 × 266\n  `Data Series`       `2025 Jan` `2024 Dec` `2024 Nov` `2024 Oct` `2024 Sep`\n  &lt;chr&gt;                    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Total All Markets       54746.    56136.      51802.    51416.     49068. \n2 America                  6923.     7874.       7880.     8078.      9112  \n3 Antigua And Barbuda         0         0           0         0          0  \n4 Argentina                   4        12.5       116.        4.1        8.1\n5 Bahamas                     0         8.1         0         0          0  \n6 Bermuda                     0         0           0         0          0  \n# ℹ 260 more variables: `2024 Aug` &lt;dbl&gt;, `2024 Jul` &lt;dbl&gt;, `2024 Jun` &lt;dbl&gt;,\n#   `2024 May` &lt;dbl&gt;, `2024 Apr` &lt;dbl&gt;, `2024 Mar` &lt;dbl&gt;, `2024 Feb` &lt;dbl&gt;,\n#   `2024 Jan` &lt;dbl&gt;, `2023 Dec` &lt;dbl&gt;, `2023 Nov` &lt;dbl&gt;, `2023 Oct` &lt;dbl&gt;,\n#   `2023 Sep` &lt;dbl&gt;, `2023 Aug` &lt;dbl&gt;, `2023 Jul` &lt;dbl&gt;, `2023 Jun` &lt;dbl&gt;,\n#   `2023 May` &lt;dbl&gt;, `2023 Apr` &lt;dbl&gt;, `2023 Mar` &lt;dbl&gt;, `2023 Feb` &lt;dbl&gt;,\n#   `2023 Jan` &lt;dbl&gt;, `2022 Dec` &lt;dbl&gt;, `2022 Nov` &lt;dbl&gt;, `2022 Oct` &lt;dbl&gt;,\n#   `2022 Sep` &lt;dbl&gt;, `2022 Aug` &lt;dbl&gt;, `2022 Jul` &lt;dbl&gt;, `2022 Jun` &lt;dbl&gt;, …\n\n\n\ndf_DE &lt;- read_csv(\"data/M451501.csv\", skip = 10, show_col_types = FALSE)\ndf_RE &lt;- read_csv(\"data/M451511.csv\", skip = 10, show_col_types = FALSE)\ndf_commodity &lt;- read_csv(\"data/M451001.csv\", skip = 10, show_col_types = FALSE)\nhead(df_DE)\n\n# A tibble: 6 × 266\n  `Data Series`       `2025 Jan` `2024 Dec` `2024 Nov` `2024 Oct` `2024 Sep`\n  &lt;chr&gt;                    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Total All Markets      24672      23685.     23439.     22148.     21967. \n2 America                 4263.      3608.      3129.      2936.      3294. \n3 Antigua And Barbuda        8.8        7.1        8.3        7.7        8.2\n4 Argentina                  5.3        6          3.9        6.4        3  \n5 Bahamas                   51.4       48.1       60.5       36.1       59.3\n6 Bermuda                    2.7        7.7        0.5        5.4        0.7\n# ℹ 260 more variables: `2024 Aug` &lt;dbl&gt;, `2024 Jul` &lt;dbl&gt;, `2024 Jun` &lt;dbl&gt;,\n#   `2024 May` &lt;dbl&gt;, `2024 Apr` &lt;dbl&gt;, `2024 Mar` &lt;dbl&gt;, `2024 Feb` &lt;dbl&gt;,\n#   `2024 Jan` &lt;dbl&gt;, `2023 Dec` &lt;dbl&gt;, `2023 Nov` &lt;dbl&gt;, `2023 Oct` &lt;dbl&gt;,\n#   `2023 Sep` &lt;dbl&gt;, `2023 Aug` &lt;dbl&gt;, `2023 Jul` &lt;dbl&gt;, `2023 Jun` &lt;dbl&gt;,\n#   `2023 May` &lt;dbl&gt;, `2023 Apr` &lt;dbl&gt;, `2023 Mar` &lt;dbl&gt;, `2023 Feb` &lt;dbl&gt;,\n#   `2023 Jan` &lt;dbl&gt;, `2022 Dec` &lt;dbl&gt;, `2022 Nov` &lt;dbl&gt;, `2022 Oct` &lt;dbl&gt;,\n#   `2022 Sep` &lt;dbl&gt;, `2022 Aug` &lt;dbl&gt;, `2022 Jul` &lt;dbl&gt;, `2022 Jun` &lt;dbl&gt;, …\n\nhead(df_RE)\n\n# A tibble: 6 × 266\n  `Data Series`       `2025 Jan` `2024 Dec` `2024 Nov` `2024 Oct` `2024 Sep`\n  &lt;chr&gt;                    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Total All Markets      34736.     36458.     34892.     33963.     32477. \n2 America                 3110.      3893.      3528.      3389.      3197. \n3 Antigua And Barbuda        1.9        0.1        0          0.2        0  \n4 Argentina                 25.7       22.3       19.4       23.8       27.1\n5 Bahamas                    9.8        1.3        3.7        4.3        8.1\n6 Bermuda                    0.1        0          0          0.1        0  \n# ℹ 260 more variables: `2024 Aug` &lt;dbl&gt;, `2024 Jul` &lt;dbl&gt;, `2024 Jun` &lt;dbl&gt;,\n#   `2024 May` &lt;dbl&gt;, `2024 Apr` &lt;dbl&gt;, `2024 Mar` &lt;dbl&gt;, `2024 Feb` &lt;dbl&gt;,\n#   `2024 Jan` &lt;dbl&gt;, `2023 Dec` &lt;dbl&gt;, `2023 Nov` &lt;dbl&gt;, `2023 Oct` &lt;dbl&gt;,\n#   `2023 Sep` &lt;dbl&gt;, `2023 Aug` &lt;dbl&gt;, `2023 Jul` &lt;dbl&gt;, `2023 Jun` &lt;dbl&gt;,\n#   `2023 May` &lt;dbl&gt;, `2023 Apr` &lt;dbl&gt;, `2023 Mar` &lt;dbl&gt;, `2023 Feb` &lt;dbl&gt;,\n#   `2023 Jan` &lt;dbl&gt;, `2022 Dec` &lt;dbl&gt;, `2022 Nov` &lt;dbl&gt;, `2022 Oct` &lt;dbl&gt;,\n#   `2022 Sep` &lt;dbl&gt;, `2022 Aug` &lt;dbl&gt;, `2022 Jul` &lt;dbl&gt;, `2022 Jun` &lt;dbl&gt;, …\n\nhead(df_commodity)\n\n# A tibble: 6 × 734\n  `Data Series`           `2025 Jan` `2024 Dec` `2024 Nov` `2024 Oct` `2024 Sep`\n  &lt;chr&gt;                        &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Total Merchandise Trad… 114153980. 116278793. 110132324. 107525960. 103512460.\n2 Oil                      19490290.  18488974.  18061885.  17510050.  15686654.\n3 Petroleum                16418934.  15564654.  14910686.  14804334.  12611958.\n4 Oil Bunkers               3071356.   2924319.   3151200.   2705716.   3074695.\n5 Non-Oil                  94663690.  97789819.  92070439.  90015910.  87825806.\n6 Food & Live Animals       2460911.   2704579.   2237326.   2344745.   2235005.\n# ℹ 728 more variables: `2024 Aug` &lt;dbl&gt;, `2024 Jul` &lt;dbl&gt;, `2024 Jun` &lt;dbl&gt;,\n#   `2024 May` &lt;dbl&gt;, `2024 Apr` &lt;dbl&gt;, `2024 Mar` &lt;dbl&gt;, `2024 Feb` &lt;dbl&gt;,\n#   `2024 Jan` &lt;dbl&gt;, `2023 Dec` &lt;dbl&gt;, `2023 Nov` &lt;dbl&gt;, `2023 Oct` &lt;dbl&gt;,\n#   `2023 Sep` &lt;dbl&gt;, `2023 Aug` &lt;dbl&gt;, `2023 Jul` &lt;dbl&gt;, `2023 Jun` &lt;dbl&gt;,\n#   `2023 May` &lt;dbl&gt;, `2023 Apr` &lt;dbl&gt;, `2023 Mar` &lt;dbl&gt;, `2023 Feb` &lt;dbl&gt;,\n#   `2023 Jan` &lt;dbl&gt;, `2022 Dec` &lt;dbl&gt;, `2022 Nov` &lt;dbl&gt;, `2022 Oct` &lt;dbl&gt;,\n#   `2022 Sep` &lt;dbl&gt;, `2022 Aug` &lt;dbl&gt;, `2022 Jul` &lt;dbl&gt;, `2022 Jun` &lt;dbl&gt;, …\n\n\nAfter viewing the whole table, we should change the type of date series.\nFirst column (Data Series): Country/Region Name, Subsequent columns: Monthly data, such as “2025 Jan”, “2024 Dec”, “2024 Nov”, representing trade data for different months.\nThe column name is in YYYY Mon format (needs to be converted to standard time format YYYY-MM). Data needs to be converted to Long Format for time series analysis.\n\n# rename \"Data Series\" as \"Country\"\ncolnames(df_Im)[1] &lt;- \"Country\"\ncolnames(df_DE)[1] &lt;- \"Country\"\ncolnames(df_RE)[1] &lt;- \"Country\"\n\n# Extract time column names and convert them to YYYY-MM format\ntime_cols &lt;- colnames(df_Im)[-1]  \ntime_cols_cleaned &lt;- format(parse_date_time(time_cols, orders = \"ym\"), \"%Y-%m\")\n\ncolnames(df_Im)[-1] &lt;- time_cols_cleaned\ncolnames(df_DE)[-1] &lt;- time_cols_cleaned\ncolnames(df_RE)[-1] &lt;- time_cols_cleaned\n\nhead(df_Im)\n\n# A tibble: 6 × 266\n  Country  `2025-01` `2024-12` `2024-11` `2024-10` `2024-09` `2024-08` `2024-07`\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Total A…    54746.   56136.     51802.   51416.    49068.    49949     52965. \n2 America      6923.    7874.      7880.    8078.     9112      8581.     8462  \n3 Antigua…        0        0          0        0         0         0         0  \n4 Argenti…        4       12.5      116.       4.1       8.1       7.2       6.6\n5 Bahamas         0        8.1        0        0         0         0         0  \n6 Bermuda         0        0          0        0         0         0         0.1\n# ℹ 258 more variables: `2024-06` &lt;dbl&gt;, `2024-05` &lt;dbl&gt;, `2024-04` &lt;dbl&gt;,\n#   `2024-03` &lt;dbl&gt;, `2024-02` &lt;dbl&gt;, `2024-01` &lt;dbl&gt;, `2023-12` &lt;dbl&gt;,\n#   `2023-11` &lt;dbl&gt;, `2023-10` &lt;dbl&gt;, `2023-09` &lt;dbl&gt;, `2023-08` &lt;dbl&gt;,\n#   `2023-07` &lt;dbl&gt;, `2023-06` &lt;dbl&gt;, `2023-05` &lt;dbl&gt;, `2023-04` &lt;dbl&gt;,\n#   `2023-03` &lt;dbl&gt;, `2023-02` &lt;dbl&gt;, `2023-01` &lt;dbl&gt;, `2022-12` &lt;dbl&gt;,\n#   `2022-11` &lt;dbl&gt;, `2022-10` &lt;dbl&gt;, `2022-09` &lt;dbl&gt;, `2022-08` &lt;dbl&gt;,\n#   `2022-07` &lt;dbl&gt;, `2022-06` &lt;dbl&gt;, `2022-05` &lt;dbl&gt;, `2022-04` &lt;dbl&gt;, …\n\n\nAt present, the data is in Wide Format, and we need to convert it to Long Format for time series analysis:\n\ndf_Im_long &lt;- df_Im %&gt;%\n  pivot_longer(cols = -Country, names_to = \"Date\", values_to = \"Trade_Value\")\n\ndf_DE_long &lt;- df_DE %&gt;%\n  pivot_longer(cols = -Country, names_to = \"Date\", values_to = \"Trade_Value\")\n\ndf_RE_long &lt;- df_RE %&gt;%\n  pivot_longer(cols = -Country, names_to = \"Date\", values_to = \"Trade_Value\")\n\n\ndf_Im_long$Date &lt;- as.Date(paste0(df_Im_long$Date, \"-01\"))  # YYYY-MM-DD\ndf_DE_long$Date &lt;- as.Date(paste0(df_DE_long$Date, \"-01\"))\ndf_RE_long$Date &lt;- as.Date(paste0(df_RE_long$Date, \"-01\"))\n\nhead(df_Im_long)\n\n# A tibble: 6 × 3\n  Country           Date       Trade_Value\n  &lt;chr&gt;             &lt;date&gt;           &lt;dbl&gt;\n1 Total All Markets 2025-01-01      54746.\n2 Total All Markets 2024-12-01      56136.\n3 Total All Markets 2024-11-01      51802.\n4 Total All Markets 2024-10-01      51416.\n5 Total All Markets 2024-09-01      49068.\n6 Total All Markets 2024-08-01      49949 \n\nhead(df_DE_long)\n\n# A tibble: 6 × 3\n  Country           Date       Trade_Value\n  &lt;chr&gt;             &lt;date&gt;           &lt;dbl&gt;\n1 Total All Markets 2025-01-01      24672 \n2 Total All Markets 2024-12-01      23685.\n3 Total All Markets 2024-11-01      23439.\n4 Total All Markets 2024-10-01      22148.\n5 Total All Markets 2024-09-01      21967.\n6 Total All Markets 2024-08-01      23984 \n\nhead(df_RE_long)\n\n# A tibble: 6 × 3\n  Country           Date       Trade_Value\n  &lt;chr&gt;             &lt;date&gt;           &lt;dbl&gt;\n1 Total All Markets 2025-01-01      34736.\n2 Total All Markets 2024-12-01      36458.\n3 Total All Markets 2024-11-01      34892.\n4 Total All Markets 2024-10-01      33963.\n5 Total All Markets 2024-09-01      32477.\n6 Total All Markets 2024-08-01      31776.\n\n\nSave the cleaned data\n\nwrite_csv(df_Im_long, \"data/cleaned_Im.csv\")\nwrite_csv(df_DE_long, \"data/cleaned_DE.csv\")\nwrite_csv(df_RE_long, \"data/cleaned_RE.csv\")\n\nAfter completing the data cleaning, we began to gradually complete the task."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "title": "In-class_Ex07",
    "section": "",
    "text": "pacman::p_load(scales, tidyverse, tsibble, feasts, fable, seasonal, urca)\nts_data &lt;- read_csv(\"data/visitor_arrivals_by_air.csv\")\n\nRows: 144 Columns: 34\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Month-Year\ndbl (33): Republic of South Africa, Canada, USA, Bangladesh, Brunei, China, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nts_data$`Month-Year` &lt;- dmy(\n  ts_data$`Month-Year`)\nts_data\n\n# A tibble: 144 × 34\n   `Month-Year` `Republic of South Africa` Canada   USA Bangladesh Brunei China\n   &lt;date&gt;                            &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 2008-01-01                         3680   6972 31155       6786   3729 79599\n 2 2008-02-01                         1662   6056 27738       6314   3070 82074\n 3 2008-03-01                         3394   6220 31349       7502   4805 72546\n 4 2008-04-01                         3337   4764 26376       7333   3096 76112\n 5 2008-05-01                         2089   4460 26788       7988   3586 64808\n 6 2008-06-01                         2515   3888 29725       8301   5284 55238\n 7 2008-07-01                         2919   5313 33183       9004   4070 80747\n 8 2008-08-01                         2471   4519 27427       7913   4183 66625\n 9 2008-09-01                         2492   3421 21588       7549   3160 52649\n10 2008-10-01                         3023   4756 25112       7527   2983 54423\n# ℹ 134 more rows\n# ℹ 27 more variables: `Hong Kong SAR (China)` &lt;dbl&gt;, India &lt;dbl&gt;,\n#   Indonesia &lt;dbl&gt;, Japan &lt;dbl&gt;, `South Korea` &lt;dbl&gt;, Kuwait &lt;dbl&gt;,\n#   Malaysia &lt;dbl&gt;, Myanmar &lt;dbl&gt;, Pakistan &lt;dbl&gt;, Philippines &lt;dbl&gt;,\n#   `Saudi Arabia` &lt;dbl&gt;, `Sri Lanka` &lt;dbl&gt;, Taiwan &lt;dbl&gt;, Thailand &lt;dbl&gt;,\n#   `United Arab Emirates` &lt;dbl&gt;, Vietnam &lt;dbl&gt;, `Belgium & Luxembourg` &lt;dbl&gt;,\n#   Finland &lt;dbl&gt;, France &lt;dbl&gt;, Germany &lt;dbl&gt;, Italy &lt;dbl&gt;, …\nts_data_ts &lt;- ts(ts_data)\nhead(ts_data_ts)\n\n     Month-Year Republic of South Africa Canada   USA Bangladesh Brunei China\n[1,]      13879                     3680   6972 31155       6786   3729 79599\n[2,]      13910                     1662   6056 27738       6314   3070 82074\n[3,]      13939                     3394   6220 31349       7502   4805 72546\n[4,]      13970                     3337   4764 26376       7333   3096 76112\n[5,]      14000                     2089   4460 26788       7988   3586 64808\n[6,]      14031                     2515   3888 29725       8301   5284 55238\n     Hong Kong SAR (China) India Indonesia Japan South Korea Kuwait Malaysia\n[1,]                 17103 41639     62683 37673       27937    284    31352\n[2,]                 21089 37170     47834 35297       22633    241    35030\n[3,]                 23230 44815     64688 42575       22876    206    37629\n[4,]                 17688 49527     58074 26839       20634    193    37521\n[5,]                 19340 67754     57089 30814       22785    140    38044\n[6,]                 19152 57380     70118 31001       22575    354    40419\n     Myanmar Pakistan Philippines Saudi Arabia Sri Lanka Taiwan Thailand\n[1,]    5269     1395       18622          406      5289  13757    18370\n[2,]    4643     1027       21609          591      4767  13921    16400\n[3,]    6218     1635       28464          626      4988  11181    23387\n[4,]    7324     1232       30131          644      7639  11665    24469\n[5,]    5395     1306       30193          470      5125  11436    21935\n[6,]    5542     1996       25800          772      4791  10689    19900\n     United Arab Emirates Vietnam Belgium & Luxembourg Finland France Germany\n[1,]                 2652   10315                 1341    1179   6918   11982\n[2,]                 2230   13415                 1449    1207   7876   13256\n[3,]                 3353   14320                 1674    1071   8066   15185\n[4,]                 3245   15413                 1426     768   8312   11604\n[5,]                 2856   14424                 1243     690   7066    9853\n[6,]                 4292   21368                 1255     624   5926    9347\n     Italy Netherlands Spain Switzerland United Kingdom Australia New Zealand\n[1,]  2953        4938  1668        4450          41934     71260        7806\n[2,]  2704        4885  1568        4381          44029     45595        4729\n[3,]  2822        5015  2254        5015          49489     53191        6106\n[4,]  3018        4902  1503        5434          35771     56514        7560\n[5,]  2165        4397  1365        4427          24464     57808        9090\n[6,]  2022        4166  1446        3359          22473     63350        9681\nts_tsibble &lt;- ts_data %&gt;%\n  mutate(Month = yearmonth(`Month-Year`)) %&gt;%\n  as_tsibble(index = `Month`)\nts_tsibble\n\n# A tsibble: 144 x 35 [1M]\n   `Month-Year` `Republic of South Africa` Canada   USA Bangladesh Brunei China\n   &lt;date&gt;                            &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 2008-01-01                         3680   6972 31155       6786   3729 79599\n 2 2008-02-01                         1662   6056 27738       6314   3070 82074\n 3 2008-03-01                         3394   6220 31349       7502   4805 72546\n 4 2008-04-01                         3337   4764 26376       7333   3096 76112\n 5 2008-05-01                         2089   4460 26788       7988   3586 64808\n 6 2008-06-01                         2515   3888 29725       8301   5284 55238\n 7 2008-07-01                         2919   5313 33183       9004   4070 80747\n 8 2008-08-01                         2471   4519 27427       7913   4183 66625\n 9 2008-09-01                         2492   3421 21588       7549   3160 52649\n10 2008-10-01                         3023   4756 25112       7527   2983 54423\n# ℹ 134 more rows\n# ℹ 28 more variables: `Hong Kong SAR (China)` &lt;dbl&gt;, India &lt;dbl&gt;,\n#   Indonesia &lt;dbl&gt;, Japan &lt;dbl&gt;, `South Korea` &lt;dbl&gt;, Kuwait &lt;dbl&gt;,\n#   Malaysia &lt;dbl&gt;, Myanmar &lt;dbl&gt;, Pakistan &lt;dbl&gt;, Philippines &lt;dbl&gt;,\n#   `Saudi Arabia` &lt;dbl&gt;, `Sri Lanka` &lt;dbl&gt;, Taiwan &lt;dbl&gt;, Thailand &lt;dbl&gt;,\n#   `United Arab Emirates` &lt;dbl&gt;, Vietnam &lt;dbl&gt;, `Belgium & Luxembourg` &lt;dbl&gt;,\n#   Finland &lt;dbl&gt;, France &lt;dbl&gt;, Germany &lt;dbl&gt;, Italy &lt;dbl&gt;, …\nts_longer &lt;- ts_data %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")\nts_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;%\n  ggplot(aes(x = `Month-Year`, \n             y = Arrivals))+\n  geom_line(size = 0.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nggplot(data = ts_longer, \n       aes(x = `Month-Year`, \n           y = Arrivals,\n           color = Country))+\n  geom_line(size = 0.5) +\n  theme(legend.position = \"bottom\", \n        legend.box.spacing = unit(0.5, \"cm\"))\nggplot(data = ts_longer, \n       aes(x = `Month-Year`, \n           y = Arrivals))+\n  geom_line(size = 0.5) +\n  facet_wrap(~ Country,\n             ncol = 3,\n             scales = \"free_y\") +\n  theme_bw()\ntsibble_longer &lt;- ts_tsibble %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")\ntsibble_longer %&gt;%\n  filter(Country == \"Italy\" |\n         Country == \"Vietnam\" |\n         Country == \"United Kingdom\" |\n         Country == \"Germany\") %&gt;% \n  gg_season(Arrivals)\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  autoplot(Arrivals) + \n  facet_grid(Country ~ ., scales = \"free_y\")\nIn the code chunk below, cycle plots using gg_subseries() of feasts package are created. Notice that the cycle plots show not only seasonal patterns but also trend.\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  gg_subseries(Arrivals)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visual-forecasting",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visual-forecasting",
    "title": "In-class_Ex07",
    "section": "Visual Forecasting",
    "text": "Visual Forecasting\n\n19.7.1 Time Series Data Sampling\nFirst, an extra column called Type indicating training or hold-out will be created by using mutate() of dplyr package. It will be extremely useful for subsequent data visualisation.\n\nvietnam_ts &lt;- tsibble_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;% \n  mutate(Type = if_else(\n    `Month-Year` &gt;= \"2019-01-01\", \n    \"Hold-out\", \"Training\"))\n\nNext, a training data set is extracted from the original data set by using filter() of dplyr package.\n\nvietnam_train &lt;- vietnam_ts %&gt;%\n  filter(`Month-Year` &lt; \"2019-01-01\")\n\n\n\n19.7.2 Exploratory Data Analysis (EDA): Time Series Data\nBefore fitting forecasting models, it is a good practice to analysis the time series data by using EDA methods.\n\nvietnam_train %&gt;%\n  model(stl = STL(Arrivals)) %&gt;%\n  components() %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n19.7.3 Fitting forecasting models\n\n19.7.3.1 Fitting Exponential Smoothing State Space (ETS) Models: fable methods\nIn fable, Exponential Smoothing State Space Models are supported by ETS(). The combinations are specified through the formula:\n\nfit_ses &lt;- vietnam_train %&gt;%\n  model(ETS(Arrivals ~ error(\"A\") \n            + trend(\"N\") \n            + season(\"N\")))\nfit_ses\n\n# A mable: 1 x 2\n# Key:     Country [1]\n  Country `ETS(Arrivals ~ error(\"A\") + trend(\"N\") + season(\"N\"))`\n  &lt;chr&gt;                                                   &lt;model&gt;\n1 Vietnam                                            &lt;ETS(A,N,N)&gt;\n\n\n\ngg_tsresiduals(fit_ses)\n\n\n\n\n\n\n\n\n\nfit_ses %&gt;%\n  report()\n\nSeries: Arrivals \nModel: ETS(A,N,N) \n  Smoothing parameters:\n    alpha = 0.9998995 \n\n  Initial states:\n     l[0]\n 10312.99\n\n  sigma^2:  27939164\n\n     AIC     AICc      BIC \n2911.726 2911.913 2920.374 \n\n\n\n\n19.7.3.5 Fitting ETS Methods with Trend: Holt’s Linear\n\n\n19.7.3.6 Trend methods\n\nvietnam_H &lt;- vietnam_train %&gt;%\n  model(`Holt's method` = \n          ETS(Arrivals ~ error(\"A\") +\n                trend(\"A\") + \n                season(\"N\")))\nvietnam_H %&gt;% report()\n\nSeries: Arrivals \nModel: ETS(A,A,N) \n  Smoothing parameters:\n    alpha = 0.9998995 \n    beta  = 0.0001004625 \n\n  Initial states:\n     l[0]     b[0]\n 13673.29 525.8859\n\n  sigma^2:  28584805\n\n     AIC     AICc      BIC \n2916.695 2917.171 2931.109 \n\n\n\n\n19.7.3.7 Damped Trend methods\n\nvietnam_HAd &lt;- vietnam_train %&gt;%\n  model(`Holt's method` = \n          ETS(Arrivals ~ error(\"A\") +\n                trend(\"Ad\") + \n                season(\"N\")))\nvietnam_HAd %&gt;% report()\n\nSeries: Arrivals \nModel: ETS(A,Ad,N) \n  Smoothing parameters:\n    alpha = 0.9998999 \n    beta  = 0.0001098602 \n    phi   = 0.9784562 \n\n  Initial states:\n     l[0]   b[0]\n 13257.28 523.54\n\n  sigma^2:  28641536\n\n     AIC     AICc      BIC \n2917.921 2918.593 2935.218 \n\n\n\n\n19.7.3.8 Checking for results\nCheck the model assumptions with residuals plots.\n\ngg_tsresiduals(vietnam_H)\n\n\n\n\n\n\n\n\n\ngg_tsresiduals(vietnam_HAd)\n\n\n\n\n\n\n\n\n\n\n\n19.7.4 Fitting ETS Methods with Season: Holt-Winters\n\nVietnam_WH &lt;- vietnam_train %&gt;%\n  model(\n    Additive = ETS(Arrivals ~ error(\"A\") \n                   + trend(\"A\") \n                   + season(\"A\")),\n    Multiplicative = ETS(Arrivals ~ error(\"M\") \n                         + trend(\"A\") \n                         + season(\"M\"))\n    )\n\nVietnam_WH %&gt;% report()\n\nWarning in report.mdl_df(.): Model reporting is only supported for individual\nmodels, so a glance will be shown. To see the report for a specific model, use\n`select()` and `filter()` to identify a single model.\n\n\n# A tibble: 2 × 10\n  Country .model          sigma2 log_lik   AIC  AICc   BIC    MSE   AMSE     MAE\n  &lt;chr&gt;   &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 Vietnam Additive       5.33e+6  -1336. 2706. 2711. 2755. 4.68e6 8.56e6 1.72e+3\n2 Vietnam Multiplicative 4.55e-3  -1300. 2635. 2640. 2684. 3.05e6 3.42e6 5.20e-2\n\n\n\n\n19.7.5 Fitting multiple ETS Models\n\nfit_ETS &lt;- vietnam_train %&gt;%\n  model(`SES` = ETS(Arrivals ~ error(\"A\") + \n                      trend(\"N\") + \n                      season(\"N\")),\n        `Holt`= ETS(Arrivals ~ error(\"A\") +\n                      trend(\"A\") +\n                      season(\"N\")),\n        `damped Holt` = \n          ETS(Arrivals ~ error(\"A\") +\n                trend(\"Ad\") + \n                season(\"N\")),\n        `WH_A` = ETS(\n          Arrivals ~ error(\"A\") + \n            trend(\"A\") + \n            season(\"A\")),\n        `WH_M` = ETS(Arrivals ~ error(\"M\") \n                         + trend(\"A\") \n                         + season(\"M\"))\n  )\n\n\n\n19.7.6 The model coefficient\ntidy() of fabletools is be used to extract model coefficients from a mable.\n\nfit_ETS %&gt;%\n  tidy()\n\n# A tibble: 45 × 4\n   Country .model      term      estimate\n   &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt;\n 1 Vietnam SES         alpha     1.00    \n 2 Vietnam SES         l[0]  10313.      \n 3 Vietnam Holt        alpha     1.00    \n 4 Vietnam Holt        beta      0.000100\n 5 Vietnam Holt        l[0]  13673.      \n 6 Vietnam Holt        b[0]    526.      \n 7 Vietnam damped Holt alpha     1.00    \n 8 Vietnam damped Holt beta      0.000110\n 9 Vietnam damped Holt phi       0.978   \n10 Vietnam damped Holt l[0]  13257.      \n# ℹ 35 more rows\n\n\n\n\n19.7.7 Step 4: Model Comparison\nglance() of fabletool\n\nfit_ETS %&gt;% \n  report()\n\nWarning in report.mdl_df(.): Model reporting is only supported for individual\nmodels, so a glance will be shown. To see the report for a specific model, use\n`select()` and `filter()` to identify a single model.\n\n\n# A tibble: 5 × 10\n  Country .model       sigma2 log_lik   AIC  AICc   BIC       MSE   AMSE     MAE\n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 Vietnam SES         2.79e+7  -1453. 2912. 2912. 2920. 27515844. 5.99e7 3.91e+3\n2 Vietnam Holt        2.86e+7  -1453. 2917. 2917. 2931. 27718599. 6.03e7 3.92e+3\n3 Vietnam damped Holt 2.86e+7  -1453. 2918. 2919. 2935. 27556629. 5.97e7 3.92e+3\n4 Vietnam WH_A        5.33e+6  -1336. 2706. 2711. 2755.  4684271. 8.56e6 1.72e+3\n5 Vietnam WH_M        4.55e-3  -1300. 2635. 2640. 2684.  3046059. 3.42e6 5.20e-2\n\n\n\n\n19.7.8 Step 5: Forecasting future values\nTo forecast the future values, forecast() of fable will be used. Notice that the forecast period is 12 months.\n\nfit_ETS %&gt;%\n  forecast(h = \"12 months\") %&gt;%\n  autoplot(vietnam_ts, \n           level = NULL)\n\n\n\n\n\n\n\n\n\n\n19.7.9 Fitting ETS Automatically\n\nfit_autoETS &lt;- vietnam_train %&gt;%\n  model(ETS(Arrivals))\nfit_autoETS %&gt;% report()\n\nSeries: Arrivals \nModel: ETS(M,A,M) \n  Smoothing parameters:\n    alpha = 0.1613503 \n    beta  = 0.0001021811 \n    gamma = 0.0001030996 \n\n  Initial states:\n     l[0]     b[0]      s[0]     s[-1]     s[-2]     s[-3]    s[-4]    s[-5]\n 15001.12 212.3552 0.9167302 0.8311728 0.8739287 0.8690543 1.104668 1.485584\n    s[-6]     s[-7]    s[-8]     s[-9]    s[-10]    s[-11]\n 1.311207 0.9917759 1.014187 0.8973028 0.8816768 0.8227129\n\n  sigma^2:  0.0046\n\n     AIC     AICc      BIC \n2634.751 2640.119 2683.759 \n\n\n\n\n19.7.10 Fitting Fitting ETS Automatically\nNext, we will check the model assumptions with residuals plots by using gg_tsresiduals() of feasts package\n\ngg_tsresiduals(fit_autoETS)\n\n\n\n\n\n\n\n\n\n\n19.7.11 Forecast the future values\nIn the code chunk below, forecast() of fable package is used to forecast the future values. Then, autoplot() of feasts package is used to see the training data along with the forecast values.\n\nfit_autoETS %&gt;%\n  forecast(h = \"12 months\") %&gt;%\n  autoplot(vietnam_train)\n\n\n\n\n\n\n\n\n\n\n19.7.12 Visualising AutoETS model with ggplot2\nThere are time that we are interested to visualise relationship between training data and fit data and forecasted values versus the hold-out data.\n\n\n19.7.13 Visualising AutoETS model with ggplot2\nCode chunk below is used to create the data visualisation in previous slide\n\nfc_autoETS &lt;- fit_autoETS %&gt;%\n  forecast(h = \"12 months\")\n\nvietnam_ts %&gt;%\n  ggplot(aes(x=`Month`, \n             y=Arrivals)) +\n  autolayer(fc_autoETS, \n            alpha = 0.6) +\n  geom_line(aes(\n    color = Type), \n    alpha = 0.8) + \n  geom_line(aes(\n    y = .mean, \n    colour = \"Forecast\"), \n    data = fc_autoETS) +\n  geom_line(aes(\n    y = .fitted, \n    colour = \"Fitted\"), \n    data = augment(fit_autoETS))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#autoregressive-integrated-moving-averagearima-methods-for-time-series-forecasting-fable-tidyverts-methods",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#autoregressive-integrated-moving-averagearima-methods-for-time-series-forecasting-fable-tidyverts-methods",
    "title": "In-class_Ex07",
    "section": "19.8 AutoRegressive Integrated Moving Average(ARIMA) Methods for Time Series Forecasting: fable (tidyverts) methods",
    "text": "19.8 AutoRegressive Integrated Moving Average(ARIMA) Methods for Time Series Forecasting: fable (tidyverts) methods\n\n19.8.1 Visualising Autocorrelations: feasts methods\nfeasts package provides a very handy function for visualising ACF and PACF of a time series called gg_tsdiaply().\n\nvietnam_train %&gt;%\n  gg_tsdisplay(plot_type='partial')\n\nPlot variable not specified, automatically selected `y = Arrivals`\n\n\n\n\n\n\n\n\n\n\n\n19.8.2 Visualising Autocorrelations: feasts methods\n\ntsibble_longer %&gt;%\n  filter(`Country` == \"Vietnam\") %&gt;%\n  ACF(Arrivals) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\ntsibble_longer %&gt;%\n  filter(`Country` == \"United Kingdom\") %&gt;%\n  ACF(Arrivals) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n19.8.3 Differencing: fable methods\n\n19.8.3.1 Trend differencing\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;%\n  gg_tsdisplay(difference(\n    Arrivals,\n    lag = 1), \n    plot_type='partial')\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n19.8.3.2 Seasonal differencing\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;%\n  gg_tsdisplay(difference(\n    Arrivals,\n    difference = 12), \n    plot_type='partial')\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n19.8.4 Fitting ARIMA models manually: fable methods\n\nfit_arima &lt;- vietnam_train %&gt;%\n  model(\n    arima200 = ARIMA(Arrivals ~ pdq(2,0,0)),\n    sarima210 = ARIMA(Arrivals ~ pdq(2,0,0) + \n                        PDQ(2,1,0))\n    )\nreport(fit_arima)\n\nWarning in report.mdl_df(fit_arima): Model reporting is only supported for\nindividual models, so a glance will be shown. To see the report for a specific\nmodel, use `select()` and `filter()` to identify a single model.\n\n\n# A tibble: 2 × 9\n  Country .model      sigma2 log_lik   AIC  AICc   BIC ar_roots   ma_roots \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;     &lt;list&gt;   \n1 Vietnam arima200  4173906.  -1085. 2181. 2182. 2198. &lt;cpl [26]&gt; &lt;cpl [0]&gt;\n2 Vietnam sarima210 4173906.  -1085. 2181. 2182. 2198. &lt;cpl [26]&gt; &lt;cpl [0]&gt;\n\n\n\n\n19.8.5 Fitting ARIMA models automatically: fable methods\n\nfit_autoARIMA &lt;- vietnam_train %&gt;%\n  model(ARIMA(Arrivals))\nreport(fit_autoARIMA)\n\nSeries: Arrivals \nModel: ARIMA(2,0,0)(2,1,0)[12] w/ drift \n\nCoefficients:\n         ar1     ar2     sar1     sar2   constant\n      0.4748  0.1892  -0.5723  -0.1578  1443.2068\ns.e.  0.0924  0.0903   0.0989   0.1032   188.9468\n\nsigma^2 estimated as 4173906:  log likelihood=-1084.6\nAIC=2181.19   AICc=2181.94   BIC=2197.92\n\n\n\n\n19.8.7 Forecast Multiple Time Series\nIn this section, we will perform time series forecasting on multiple time series at one goal. For the purpose of the hand-on exercise, visitor arrivals from five selected ASEAN countries will be used.\nFirst, filter() is used to extract the selected countries’ data.\n\nASEAN &lt;- tsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Malaysia\" |\n         Country == \"Indonesia\" |\n         Country == \"Thailand\" |\n         Country == \"Philippines\")\n\n\nASEAN_train &lt;- ASEAN %&gt;%\n  mutate(Type = if_else(\n    `Month-Year` &gt;= \"2019-01-01\", \n    \"Hold-out\", \"Training\")) %&gt;%\n  filter(Type == \"Training\")\n\n\n\n19.8.8 Fitting Mulltiple Time Series\nIn the code chunk below auto ETS and ARIMA models are fitted by using model().\n\nASEAN_fit &lt;- ASEAN_train %&gt;%\n  model(\n    ets = ETS(Arrivals),\n    arima = ARIMA(Arrivals)\n  )\n\n\n\n19.8.9 Examining Models\nThe glance() of fabletools provides a one-row summary of each model, and commonly includes descriptions of the model’s fit such as the residual variance and information criteria.\n\nASEAN_fit %&gt;%\n  glance()\n\n# A tibble: 10 × 12\n   Country     .model  sigma2 log_lik   AIC  AICc   BIC      MSE    AMSE     MAE\n   &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Indonesia   ets    1.02e-2  -1561. 3156. 3161. 3205.   1.74e8  1.80e8  0.0732\n 2 Indonesia   arima  1.48e+8  -1290. 2589. 2590. 2603.  NA      NA      NA     \n 3 Malaysia    ets    4.67e-3  -1430. 2894. 2899. 2943.   2.04e7  2.00e7  0.0506\n 4 Malaysia    arima  2.62e+7  -1185. 2378. 2379. 2390.  NA      NA      NA     \n 5 Philippines ets    3.56e-3  -1343. 2722. 2728. 2774.   5.28e6  7.58e6  0.0461\n 6 Philippines arima  8.04e+6  -1122. 2260. 2262. 2283.  NA      NA      NA     \n 7 Thailand    ets    6.63e-3  -1343. 2722. 2728. 2774.   5.40e6  6.33e6  0.0584\n 8 Thailand    arima  8.51e+6  -1127. 2269. 2270. 2288.  NA      NA      NA     \n 9 Vietnam     ets    4.55e-3  -1300. 2635. 2640. 2684.   3.05e6  3.42e6  0.0520\n10 Vietnam     arima  4.17e+6  -1085. 2181. 2182. 2198.  NA      NA      NA     \n# ℹ 2 more variables: ar_roots &lt;list&gt;, ma_roots &lt;list&gt;\n\n\n\n\n19.8.10 Extracintg fitted and residual values\nThe fitted values and residuals from a model can obtained using fitted() and residuals() respectively. Additionally, the augment() function may be more convenient, which provides the original data along with both fitted values and their residuals.\n\nASEAN_fit %&gt;%\n  augment()\n\n# A tsibble: 1,320 x 7 [1M]\n# Key:       Country, .model [10]\n   Country   .model     Month Arrivals .fitted  .resid  .innov\n   &lt;chr&gt;     &lt;chr&gt;      &lt;mth&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Indonesia ets     2008 1月    62683  56534.   6149.  0.109 \n 2 Indonesia ets     2008 2月    47834  46417.   1417.  0.0305\n 3 Indonesia ets     2008 3月    64688  62660.   2028.  0.0324\n 4 Indonesia ets     2008 4月    58074  61045.  -2971. -0.0487\n 5 Indonesia ets     2008 5月    57089  62280.  -5191. -0.0833\n 6 Indonesia ets     2008 6月    70118  75791.  -5673. -0.0749\n 7 Indonesia ets     2008 7月    73805  78691.  -4886. -0.0621\n 8 Indonesia ets     2008 8月    58015  61910.  -3895. -0.0629\n 9 Indonesia ets     2008 9月    63730  74518. -10788. -0.145 \n10 Indonesia ets    2008 10月    71206  67869.   3337.  0.0492\n# ℹ 1,310 more rows\n\n\n\n\n19.8.11 Comparing Fit Models\nIn the code chunk below, accuracy() is used to compare the performances of the models.\n\nASEAN_fit %&gt;%\n  accuracy() %&gt;%\n  arrange(Country)\n\n# A tibble: 10 × 11\n   Country   .model .type      ME   RMSE   MAE    MPE  MAPE  MASE RMSSE     ACF1\n   &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 Indonesia ets    Trai… -1.81e3 13187. 9665. -1.83   7.57 0.556 0.619 -0.236  \n 2 Indonesia arima  Trai… -9.54e1 11351. 8382. -0.136  6.38 0.482 0.533 -0.00802\n 3 Malaysia  ets    Trai… -6.78e2  4515. 3538. -1.25   5.15 0.529 0.527 -0.288  \n 4 Malaysia  arima  Trai… -2.33e1  4801. 3684. -0.109  5.20 0.551 0.561 -0.00933\n 5 Philippi… ets    Trai… -2.35e0  2298. 1897. -0.334  4.64 0.464 0.408  0.0400 \n 6 Philippi… arima  Trai…  9.53e0  2624. 1934. -0.269  4.60 0.473 0.466  0.00717\n 7 Thailand  ets    Trai…  1.97e1  2323. 1773. -0.511  5.89 0.489 0.485 -0.0812 \n 8 Thailand  arima  Trai…  5.88e1  2710. 1932. -0.562  6.16 0.532 0.565 -0.0112 \n 9 Vietnam   ets    Trai… -3.52e1  1745. 1386. -0.728  5.29 0.467 0.473  0.279  \n10 Vietnam   arima  Trai…  1.95e0  1907. 1458. -0.671  5.37 0.491 0.517  0.0136 \n\n\n\n\n19.8.12 Forecast Future Values\nForecasts from these models can be produced directly as our specified models do not require any additional data.\n\nASEAN_fc &lt;- ASEAN_fit %&gt;%\n  forecast(h = \"12 months\")\n\n\n\n19.8.13 Visualising the forecasted values\nIn the code chunk below autoplot() of feasts package is used to plot the raw and fitted values.\n\nASEAN_fc %&gt;%\n  autoplot(ASEAN)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#evaluate-and-design-data-visualization",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#evaluate-and-design-data-visualization",
    "title": "Take-home_Ex02",
    "section": "",
    "text": "Intuitively displays the import and export situation of major trading partners. The size of the bubble represents the trade volume, which is clear at a glance. Colors distinguish the proportion of imports and exports for easy comparison.\n\n\n\nDifficulty in accurately comparing bubble sizes.\nLarge space occupation, unable to display more countries.\nTime trend not shown, limited to 2024\n\n\n\nfirst, merge export from the two file\n\ndf_DE &lt;- read_csv(\"data/cleaned_DE.csv\") \n\nRows: 47965 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Country\ndbl  (1): Trade_Value\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_RE &lt;- read_csv(\"data/cleaned_RE.csv\")  \n\nRows: 47965 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Country\ndbl  (1): Trade_Value\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_DE &lt;- df_DE %&gt;% mutate(Date = as.Date(Date))\ndf_RE &lt;- df_RE %&gt;% mutate(Date = as.Date(Date))\n\n\ndf_merged_export &lt;- full_join(df_DE, df_RE, by = c(\"Country\", \"Date\"), suffix = c(\"_DE\", \"_RE\"))\n\nWarning in full_join(df_DE, df_RE, by = c(\"Country\", \"Date\"), suffix = c(\"_DE\", : Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 42401 of `x` matches multiple rows in `y`.\nℹ Row 42401 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\ndf_merged_export &lt;- df_merged_export %&gt;%\n  mutate(Export_Trade_Value = coalesce(Trade_Value_DE, 0) + coalesce(Trade_Value_RE, 0))\n\n\ndf_merged_export &lt;- df_merged_export %&gt;%\n  select(Country, Date, Trade_Value_DE, Trade_Value_RE, Export_Trade_Value)\n\nwrite_csv(df_merged_export, \"data/merged_export.csv\")\n\n\ndf_Im &lt;- read_csv(\"data/cleaned_Im.csv\")\n\nRows: 46905 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Country\ndbl  (1): Trade_Value\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_merged_export &lt;- read_csv(\"data/merged_export.csv\")\n\nRows: 51675 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Country\ndbl  (3): Trade_Value_DE, Trade_Value_RE, Export_Trade_Value\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nexclude_regions &lt;- c(\"Total All Markets\", \"America\", \"Asia\", \"Europe\", \"Oceania\", \"Africa\")\ndf_Im &lt;- df_Im %&gt;% filter(!Country %in% exclude_regions)\ndf_merged_export &lt;- df_merged_export %&gt;% filter(!Country %in% exclude_regions)\n\n\ndf_Im &lt;- df_Im %&gt;% mutate(Date = as.Date(Date))\ndf_merged_export &lt;- df_merged_export %&gt;% mutate(Date = as.Date(Date))\n\n\ndf_Im_2024 &lt;- df_Im %&gt;% filter(format(Date, \"%Y\") == \"2024\")\ndf_merged_export_2024 &lt;- df_merged_export %&gt;% filter(format(Date, \"%Y\") == \"2024\")\n\n\ndf_Im_agg &lt;- df_Im_2024 %&gt;% group_by(Country) %&gt;% summarise(Import = sum(Trade_Value, na.rm = TRUE)/1000)\ndf_merged_export_agg &lt;- df_merged_export_2024 %&gt;% group_by(Country) %&gt;% summarise(Export = sum(Export_Trade_Value, na.rm = TRUE)/1000)\n\n\ndf_total_trade &lt;- left_join(df_Im_agg, df_merged_export_agg, by = \"Country\") %&gt;%\n  mutate(Total_Trade = Import + Export)\n\n\ntop_10_trade_partners &lt;- df_total_trade %&gt;%\n  arrange(desc(Total_Trade)) %&gt;%\n  slice(1:10)\n\n\ndf_long &lt;- top_10_trade_partners %&gt;%\n  select(Country, Import, Export) %&gt;%\n  pivot_longer(cols = c(\"Import\", \"Export\"), names_to = \"Trade_Type\", values_to = \"Value\")\n\n\nggplot(df_long, aes(x = reorder(Country, -Value), y = Value, fill = Trade_Type)) +\n  geom_bar(stat = \"identity\") +\n  \n  geom_text(aes(label = round(Value, 1)), position = position_stack(vjust = 0.5), size = 3, color = \"white\") +\n  \n  geom_text(data = top_10_trade_partners, \n            aes(x = Country, y = Total_Trade, label = paste0(\"Total: \", round(Total_Trade, 1))),\n            vjust = -0.5, size = 3, fontface = \"bold\", inherit.aes = FALSE) +\n\n  labs(title = \"Singapore's Top 10 Trading Partners (2024)\",\n       x = \"Country\",\n       y = \"Trade Value (S$ Billion)\",\n       fill = \"Trade Type\") +\n  \n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),\n    axis.text.y = element_text(size = 8),\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 8, face = \"italic\"),\n    legend.position = \"top\",\n    legend.title = element_blank()\n  )\n\n\n\n\n\n\n\n\nWe will create an interactive chart in shinyapp that allows viewers to select a year and see the top 10 trade partners of each year.\nShinyapp - Click here to view the interactive Shiny app\n\n\n\nWe need to download the file (M451001)-Merchandise Trade By Commodity Section, (At Current Prices), Monthly\n\n\n\nStrength and weakness:\n\n\nStrengths:\n\nClear Category Breakdown – The chart categorizes key commodities and highlights export (orange) and import (blue) values separately for easy comparison.\nHelpful Numerical Labels – Each category displays exact trade values and percentages, improving readability.\nSimple and Effective Design – The color contrast and “Top 3 Commodity Sections” summary make it visually appealing and easy to interpret.\n\nWeakness:\n\nExport vs. Import Comparison is Misleading – Different scales make the gaps between exports and imports look bigger than they actually are.\nNo Overall Export vs. Import Ratio – The chart lacks a clear visualization of total exports vs. imports, making it harder to see the overall balance.\nNo Trend Analysis – It only shows 2024 data, missing past trends that could help understand market changes over time.\n\n\n\n\nFirst we have to clean dataset follow the code.\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\ndf_commodity &lt;- read_csv(\"data/M451001.csv\", skip = 10, show_col_types = FALSE)\n\ncolnames(df_commodity)[1] &lt;- \"Category\"\n\n\ntime_cols &lt;- colnames(df_commodity)[-1]\n\n\nvalid_time_cols &lt;- time_cols[!is.na(parse_date_time(time_cols, orders = c(\"Y b\", \"b Y\")))]\ntime_cols_cleaned &lt;- format(parse_date_time(valid_time_cols, orders = c(\"Y b\", \"b Y\")), \"%Y-%m\")\n\ncolnames(df_commodity) &lt;- c(\"Category\", time_cols_cleaned)\n\ncolumns_to_keep &lt;- c(\"Category\", time_cols_cleaned[year(parse_date_time(time_cols_cleaned, orders = \"ym\")) &gt;= 2010 & \n                                                    year(parse_date_time(time_cols_cleaned, orders = \"ym\")) &lt;= 2025])\n\ndf_filtered &lt;- df_commodity %&gt;%\n  select(all_of(columns_to_keep))\n\ndf_import &lt;- df_filtered[19:27, ] %&gt;% mutate(Type = \"Import\")\ndf_export &lt;- df_filtered[33:41, ] %&gt;% mutate(Type = \"Export\")\n\ndf_cleaned &lt;- bind_rows(df_import, df_export)\n\ndf_long &lt;- df_cleaned %&gt;%\n  pivot_longer(cols = -c(Category, Type), names_to = \"Date\", values_to = \"Trade_Value\")\n\nprint(head(df_long))\n\n# A tibble: 6 × 4\n  Category            Type   Date    Trade_Value\n  &lt;chr&gt;               &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;\n1 Food & Live Animals Import 2025-01    1227051.\n2 Food & Live Animals Import 2024-12    1264573.\n3 Food & Live Animals Import 2024-11    1192495.\n4 Food & Live Animals Import 2024-10    1219199.\n5 Food & Live Animals Import 2024-09    1130865.\n6 Food & Live Animals Import 2024-08    1238345.\n\nwrite_csv(df_long, \"data/cleaned_com.csv\")\n\nBy usig the file - cleaned-com.csv, we will create an interactive chart in shinyapp that allows viewers to select a year and see Top 10 Trading Partners and Non-Oil Merchandise Trade. We can compare every commodity export and import summary and the trend over years.\n\nShinyapp - Click here to view the interactive Shiny app\nhttp://wangxingyun.shinyapps.io/take_home_Ex02\n\n\n\n\n\nStrengths:\nClearly displays the annual import and export values.\nDifferent colors for each year make the visualization clear and easy to understand.\nWeaknesses:\nThe 6.6% increase does not specify which year it is being compared to.\nThe chart lacks a trend showing the changes over previous years.\nImprovements:\nCreate three separate tables:\nImport Table: Displays annual import values and indicates whether they increased or decreased compared to the previous year, along with the percentage change.\nExport Table: Shows annual export values and their trend.\nTotal Trade Table: Displays the sum of imports and exports for each year, including the percentage change compared to the previous year.\n\n\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\n\ndf_Im &lt;- read_csv(\"data/cleaned_Im.csv\", show_col_types = FALSE)\ndf_merged_export &lt;- read_csv(\"data/merged_export.csv\", show_col_types = FALSE)\n\n\ndf_Im &lt;- df_Im %&gt;% mutate(Date = as.Date(Date))\ndf_merged_export &lt;- df_merged_export %&gt;% mutate(Date = as.Date(Date))\n\n\ndf_import &lt;- df_Im %&gt;% \n  filter(Country == \"Total All Markets\") %&gt;%\n  select(Date, Import = Trade_Value) %&gt;%\n  mutate(Import = round(Import / 1000, 2))  \n\ndf_export &lt;- df_merged_export %&gt;%\n  filter(Country == \"Total All Markets\") %&gt;%\n  select(Date, Export = Export_Trade_Value) %&gt;%\n  mutate(Export = round(Export / 1000, 2))  \n\ndf_total_trade &lt;- full_join(df_import, df_export, by = \"Date\") %&gt;%\n  mutate(Total_Trade = Import + Export) %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  filter(Year &gt;= 2020 & Year &lt;= 2024) %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Import = sum(Import, na.rm = TRUE),\n            Export = sum(Export, na.rm = TRUE),\n            Total_Trade = sum(Total_Trade, na.rm = TRUE)) %&gt;%\n  arrange(Year) %&gt;%\n  mutate(Import_Change = round((Import - lag(Import)) / lag(Import) * 100, 1),\n         Export_Change = round((Export - lag(Export)) / lag(Export) * 100, 1),\n         Total_Change = round((Total_Trade - lag(Total_Trade)) / lag(Total_Trade) * 100, 1))\n\n\ncustom_colors &lt;- c(\"Import\" = \"#1f77b4\", \"Export\" = \"#ff7f0e\", \"Total Trade\" = \"#2ca02c\")\n\n\np1 &lt;- ggplot(df_total_trade, aes(x = factor(Year), y = Import)) +\n  geom_bar(stat = \"identity\", fill = custom_colors[\"Import\"], alpha = 0.7) +\n  geom_text(aes(label = sprintf(\"S$ %.2f B\", Import)), vjust = -0.5, size = 3, fontface = \"bold\") +  \n  geom_text(aes(y = Import * 0.98, label = ifelse(!is.na(Import_Change), paste0(Import_Change, \"%\"), \"\")), \n            vjust = 1.5, size = 3, color = \"red\", fontface = \"bold\") +\n  labs(title = \"Yearly Import Trend (2020-2024)\", x = \"Year\", y = \"Import Value (S$ Billion)\") +\n  theme_minimal()\n\n\np2 &lt;- ggplot(df_total_trade, aes(x = factor(Year), y = Export)) +\n  geom_bar(stat = \"identity\", fill = custom_colors[\"Export\"], alpha = 0.7) +\n  geom_text(aes(label = sprintf(\"S$ %.2f B\", Export)), vjust = -0.5, size = 3, fontface = \"bold\") +  \n  geom_text(aes(y = Export * 0.98, label = ifelse(!is.na(Export_Change), paste0(Export_Change, \"%\"), \"\")), \n            vjust = 1.5, size = 3, color = \"red\", fontface = \"bold\") +\n  labs(title = \"Yearly Export Trend (2020-2024)\", x = \"Year\", y = \"Export Value (S$ Billion)\") +\n  theme_minimal()\n\np3 &lt;- ggplot(df_total_trade, aes(x = factor(Year), y = Total_Trade)) +\n  geom_bar(stat = \"identity\", fill = custom_colors[\"Total Trade\"], alpha = 0.7) +\n  geom_text(aes(label = sprintf(\"S$ %.2f B\", Total_Trade)), vjust = -0.5, size = 3, fontface = \"bold\") +  \n  geom_text(aes(y = Total_Trade * 0.98, label = ifelse(!is.na(Total_Change), paste0(Total_Change, \"%\"), \"\")), \n            vjust = 1.5, size = 3, color = \"red\", fontface = \"bold\") +\n  labs(title = \"Total Trade (Import + Export) Trend (2020-2024)\", \n       x = \"Year\", y = \"Total Trade Value (S$ Billion)\") +\n  theme_minimal()\n\nprint(p1)  \n\n\n\n\n\n\n\nprint(p2)  \n\n\n\n\n\n\n\nprint(p3)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#setting-the-scene",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#setting-the-scene",
    "title": "Take-home_Ex02",
    "section": "",
    "text": "Singapore is a major global trade hub, and its trade patterns reflect economic trends, policy changes, and global events. Since 2015, factors like the U.S.-China trade war, COVID-19, and economic recovery have influenced trade fluctuations.This analysis aims to explore Singapore’s international trade trends using data visualization and time-series analysis.\na. Critique Existing Visualizations – Identify strengths and weaknesses of current trade charts.\nb. Create Improved Visualizations – Use ggplot2 and R packages to enhance clarity. Perform\nc. Time-Series Analysis and Forecasting – Analyze trade trends and predict future movements.\nBy combining visual and analytical approaches, we aim to uncover insights into Singapore’s trade patterns and anticipate future trends."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#our-task",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#our-task",
    "title": "Take-home_Ex02",
    "section": "",
    "text": "Critique Existing Visualizations\nSelect three visualizations from the provided webpage.\nAnalyze and comment on their pros and cons (strengths and weaknesses).\nProvide sketches of redesigned versions (make-over) to improve the visualizations.\nCreate Improved Visualizations\nUse ggplot2 and other appropriate R packages.\nImplement the make-over versions of the three visualizations with improved design and clarity.\nTime-Series Analysis or Forecasting\nAnalyze the data using time-series analysis or time-series forecasting techniques.\nSupport the analysis with appropriate visualizations and relevant R packages."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#time-series-analysis",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#time-series-analysis",
    "title": "Take-home_Ex02",
    "section": "",
    "text": "We can first work with the dataset again and observe the data over the past 10 years.\nClean up the data and ensure its format is correct, especially the time column must be of Date type and aggregated by year/month.\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\n\ndf_Im &lt;- read_csv(\"data/cleaned_Im.csv\", show_col_types = FALSE)\ndf_merged_export &lt;- read_csv(\"data/merged_export.csv\", show_col_types = FALSE)\n\ndf_Im &lt;- df_Im %&gt;% mutate(Date = as.Date(Date), Year = year(Date), Month = month(Date))\ndf_merged_export &lt;- df_merged_export %&gt;% mutate(Date = as.Date(Date), Year = year(Date), Month = month(Date))\n\n\n\ndf_import &lt;- df_Im %&gt;% filter(Country == \"Total All Markets\") %&gt;%\n  select(Date, Year, Month, Import = Trade_Value)\n\ndf_export &lt;- df_merged_export %&gt;% filter(Country == \"Total All Markets\") %&gt;%\n  select(Date, Year, Month, Export = Export_Trade_Value)\n\n\ndf_total_trade &lt;- full_join(df_import, df_export, by = c(\"Date\", \"Year\", \"Month\")) %&gt;%\n  mutate(Total_Trade = Import + Export) %&gt;%\n  filter(Year &gt;= 2010 & Year &lt;= 2024)  \n\n\n\nSuitable for analyzing seasonal patterns in time series data, helping to identify variations across different months. Merchandise trade data is often influenced by seasonal factors such as holidays and policy changes. The Cycle Plot allows us to observe monthly trade patterns across different years, helping to detect recurring trends.\n\nlibrary(ggplot2)\n\nggplot(df_total_trade, aes(x = Month, y = Total_Trade, group = Year, color = factor(Year))) +\n  geom_line(size = 1) +\n  geom_point(size = 2) +\n  labs(title = \"Cycle Plot of Total Trade (2010-2024)\",\n       x = \"Month\", y = \"Trade Value (S$ Billion)\",\n       color = \"Year\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nObservation:\n\nThe trade value has shown a general upward trend over the years, with 2022 and 2023 having notably high trade values compared to previous years.\n2020-2024 trade values are significantly higher than 2010-2019, indicating economic expansion.\nTrade values tend to drop in the first quarter (January - March), likely due to the Chinese New Year and seasonal slowdowns.\nThere is a consistent increase mid-year (May - July) and another peak towards the year-end (October - December), possibly due to global supply chain demands, year-end trade, and holiday-related exports/imports.\n2022 and 2023 exhibit higher volatility in trade values compared to earlier years. This suggests that post-pandemic recovery and trade policy adjustments played a role in boosting trade volumes.\nThe dip in early 2020 aligns with COVID-19-related trade disruptions, but subsequent years have shown strong rebounds.\n2022 and 2023 saw exceptionally high trade values in certain months, especially around mid-year and year-end.\n\n\n\n\nSlope Graph is useful for comparing two points in time to show increases or decreases in values clearly. The Slope Graph effectively visualizes total trade volume changes between 2010 and 2024, providing a direct view of long-term growth.\n\ndf_slope &lt;- df_total_trade %&gt;%\n  filter(Year %in% c(2010, 2024)) %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Total_Trade = sum(Total_Trade, na.rm = TRUE))  \n\n\nggplot(df_slope, aes(x = factor(Year), y = Total_Trade, group = 1)) +\n  geom_line(aes(color = \"Total Trade\"), size = 1) +  \n  geom_point(size = 3, color = \"red\") +  \n  geom_text(aes(label = round(Total_Trade / 1000, 2)), vjust = -0.5, size = 2.5, fontface = \"bold\") +  \n  labs(title = \"Total Trade Change from 2010 to 2024\",\n       x = \"Year\", y = \"Trade Value (S$ Billion)\",\n       color = \"Trade Type\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")  \n\n\n\n\n\n\n\n\n11\n\ndf_slope &lt;- df_total_trade %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Total_Trade = sum(Total_Trade, na.rm = TRUE))  \n\n\nggplot(df_slope, aes(x = factor(Year), y = Total_Trade, group = 1)) +\n  geom_line(color = \"red\", size = 1) +  \n  geom_point(size = 3, color = \"red\") +  \n  geom_text(aes(label = round(Total_Trade / 1000, 2)), vjust = -0.5, size = 3, fontface = \"bold\") +  \n  labs(title = \"Total Trade Change from 2010 to 2024\",\n       x = \"Year\", y = \"Trade Value (S$ Billion)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nObservation:\n\nThe total trade value has shown a significant increase from 907.04 billion S$ in 2010 to 1,285.86 billion S$ in 2024. This indicates a strong upward trend in trade over the years.\nThe total trade value was relatively stable between 2010 and 2014, staying around 1,000 billion S$. A notable decline occurred between 2014 and 2016, reaching the lowest point in 2016 (870.22 billion S$). From 2017 onwards, trade value exhibited a sharp recovery, particularly in 2021 and 2022, where trade peaked at 1,365.4 billion S$.\nThe spike in 2021 and 2022 aligns with the post-pandemic recovery, where global supply chains and trade rebounded.\nThe slight dip in 2023 (1,205.72 billion S$) suggests potential economic uncertainties or adjustments after the strong post-pandemic growth.\nWhile the overall trend is upward, the trade values exhibit periodic declines and rebounds.\n\n\n\n\nHeatmap is suitable for displaying data distribution across different time periods, helping to identify trends, anomalies, and seasonal patterns. For my task, the Year-Month Heatmap enables a clear visualization of trade volume variations, revealing peak and low seasons over the years.\n\nggplot(df_total_trade, aes(x = Month, y = factor(Year), fill = Total_Trade)) +\n  geom_tile(color = \"white\") +\n  scale_fill_viridis_c(option = \"C\", direction = -1) +  \n  labs(title = \"Trade Value Heatmap (2010-2024)\",\n       x = \"Month\", y = \"Year\",\n       fill = \"Trade Value (S$ Billion)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nObservation:\n\nThe heatmap clearly shows an increase in trade value over time. The earlier years (2010-2016) are predominantly yellow and orange, indicating lower trade values. In the later years (2020-2024), the colors shift to red and purple, representing higher trade values, confirming a long-term increasing trend.\nThe trade value fluctuates across different months. Certain months, such as early months of 2020 and 2016, exhibit higher trade activity, as indicated by brighter yellow colors. Conversely, 2022 and 2024 show lower trade activity during certain months (dark blue colors), potentially due to economic disruptions or market slowdowns.\nA sharp increase is observed in 2021, which aligns with post-pandemic global economic recovery. In 2022, certain months show deep purple (high trade activity), while others dip, indicating potential global supply chain adjustments. The global economic downturn or policy shifts might have influenced fluctuations in trade in the later years.\nSome years display more balanced trade across months, while others have strong peaks and troughs. Trade tends to rise towards mid-year in several years, suggesting cyclical economic patterns. The year-end months (November-December) are generally stable, likely due to established supply chain and trade cycles.\n\n\n\n\nTime Series Forecasting is suitable for predicting future trends based on historical data, aiding decision-making processes. For my task: Using ARIMA to forecast trade data for 2025, we can estimate future trends and potential fluctuations in trade volume.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(tsibble)\n\nRegistered S3 methods overwritten by 'tsibble':\n  method               from \n  as_tibble.grouped_df dplyr\n  format.interval      inum \n\n\n\nAttaching package: 'tsibble'\n\n\nThe following object is masked from 'package:data.table':\n\n    key\n\n\nThe following object is masked from 'package:lubridate':\n\n    interval\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, union\n\nlibrary(fable)\n\nLoading required package: fabletools\n\ndf_total_trade &lt;- df_total_trade %&gt;%\n  mutate(Date = yearmonth(Date)) %&gt;%  \n  as_tsibble(index = Date) %&gt;%  \n  fill_gaps() %&gt;%  \n  mutate(Total_Trade = ifelse(is.na(Total_Trade), mean(Total_Trade, na.rm = TRUE), Total_Trade))  \n\n\nforecast_model &lt;- df_total_trade %&gt;%\n  model(ARIMA(Total_Trade))\n\n\nforecast_result &lt;- forecast_model %&gt;%\n  forecast(h = \"6 months\")\n\n\nautoplot(forecast_result) +\n  labs(title = \"Forecast for Total Trade\")\n\n\n\n\n\n\n\n\nObservation:\n\nThe forecast predicts a gradual increase in total trade from January 2025 to May 2025, peaking around April-May. However, there is a slight decline towards June 2025, indicating potential seasonal trade adjustments.\nConfidence Intervals: The darker blue region (80% confidence interval) represents a more probable forecast range. The lighter blue region (95% confidence interval) indicates wider uncertainty in predictions, showing possible fluctuations. The confidence intervals widen towards the future, which is expected as uncertainty increases over time.\n\n\n\n\n\nCycle Plot: Reveals monthly trade patterns and seasonal fluctuations, clearly identifying peak and low trade seasons over the years. This helps in understanding recurring trends and planning for seasonal demand variations.\nSlope Graph:Illustrates the significant increase in trade volume from 2010 to 2024, highlighting the long-term upward trend in total trade. It effectively emphasizes the steady expansion of trade activity over the past decade.\nHeatmap: Provides a detailed view of trade distribution across years and months, making it easier to spot anomalies, cyclical trends, and long-term growth patterns. This visualization helps in identifying seasonal trade fluctuations and market shifts.\nTime Series Forecasting: Projects trade volume trends for 2025, indicating potential growth with some uncertainty. The confidence intervals highlight the expected range of trade values, offering insights into possible market fluctuations and risks.\n\n\n\n\nVisualising and Analysing Time-series Data"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08a/Hands-on_Ex08a.html",
    "href": "Hands-on_Ex/Hands-on_Ex08a/Hands-on_Ex08a.html",
    "title": "Hands-on Exercise 8a",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called tmap package.\n\n\n\nIn this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n\n\n\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\Users\\xx905\\SMU\\4 semester\\visual analytics\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex08a\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nYou can examine the content of mpsz by using the code chunk below.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nRows: 984656 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\n\n\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\n\n\n\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_fill()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n[v3-&gt;v4] `tm_borders()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n! `tm_scale_bar()` is deprecated. Please use `tm_scalebar()` instead.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\n\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\n\n\n\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"jenks\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n' to 'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\n\n\n\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"equal\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n' to 'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\n\n\n\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\n\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_tm_fill()`: migrate the argument(s) related to the scale of the\nvisual variable `fill` namely 'breaks' to fill.scale = tm_scale(&lt;HERE&gt;).\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\n\nWarning: Values have found that are higher than the highest break. They are\nassigned to the highest interval\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\nMultiple palettes called \"greens\" found: \"brewer.greens\", \"matplotlib.greens\". The first one, \"brewer.greens\", is returned.\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"-Greens\" is named\n\"greens\" (in long format \"brewer.greens\")\n\n\n\n\n\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\n\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"jenks\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_fill()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'legend.is.portrait' (rename to 'orientation') to\n'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_fill()`: use `fill.chart = tm_chart_histogram()` instead of\n`legend.hist = TRUE`.\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\n\n\n\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\nstyle set to \"classic\"\n\nother available styles are: \"white\" (tmap default), \"gray\", \"natural\", \"cobalt\", \"albatross\", \"beaver\", \"bw\", \"watercolor\"\n\ntmap v3 styles: \"v3\" (tmap v3 default), \"gray_v3\", \"natural_v3\", \"cobalt_v3\", \"albatross_v3\", \"beaver_v3\", \"bw_v3\", \"classic_v3\", \"watercolor_v3\"\n\nMultiple palettes called \"greens\" found: \"brewer.greens\", \"matplotlib.greens\". The first one, \"brewer.greens\", is returned.\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"-Greens\" is named\n\"greens\" (in long format \"brewer.greens\")\n\n\n\n\n\n\n\n\n\n\n\n\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(size = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_fill()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n! `tm_scale_bar()` is deprecated. Please use `tm_scalebar()` instead.\n! The 'size' argument of `tm_scalebar()` is deprecated as of tmap 4.0.\nℹ Please use 'text.size' instead.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\nstyle set to \"white\" (tmap default)\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\"\n\n\ntmap v3 styles: \"v3\" (tmap v3 default), \"gray_v3\", \"natural_v3\", \"cobalt_v3\", \"albatross_v3\", \"beaver_v3\", \"bw_v3\", \"classic_v3\", \"watercolor_v3\"\n\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"equal\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\nstyle set to \"white\" (tmap default)\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\"\n\ntmap v3 styles: \"v3\" (tmap v3 default), \"gray_v3\", \"natural_v3\", \"cobalt_v3\", \"albatross_v3\", \"beaver_v3\", \"bw_v3\", \"classic_v3\", \"watercolor_v3\"\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"equal\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\nFor small multiples, specify a 'tm_scale_' for each multiple, and put them in a\nlist: 'fill'.scale = list(&lt;scale1&gt;, &lt;scale2&gt;, ...)'\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          fill.scale = tm_scale(values = \"brewer.blues\", n = 5, style = \"quantile\")) + # ✅ 更新 fill.scale 语法\n  tm_facets(by=\"REGION_N\", \n            drop.units=FALSE, # ✅ 更新 drop.shapes -&gt; drop.units\n            title = \"Regional Distribution\") +  # ✅ 手动设置 title，避免 `NA`\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 8) +  # ✅ 适当降低 title.size\n  tm_borders(fill_alpha = 0.5)  # ✅ 更新 alpha -&gt; fill_alpha\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_fill()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'legend.is.portrait' (rename to 'orientation') to\n'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08a/Hands-on_Ex08a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08a/Hands-on_Ex08a.html#overview",
    "title": "Hands-on Exercise 8a",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08a/Hands-on_Ex08a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08a/Hands-on_Ex08a.html#getting-started",
    "title": "Hands-on Exercise 8a",
    "section": "",
    "text": "In this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08a/Hands-on_Ex08a.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex08a/Hands-on_Ex08a.html#importing-data-into-r",
    "title": "Hands-on Exercise 8a",
    "section": "",
    "text": "Two data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\Users\\xx905\\SMU\\4 semester\\visual analytics\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex08a\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nYou can examine the content of mpsz by using the code chunk below.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nRows: 984656 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\n\n\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08a/Hands-on_Ex08a.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex08a/Hands-on_Ex08a.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 8a",
    "section": "",
    "text": "Two approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_fill()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n[v3-&gt;v4] `tm_borders()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n! `tm_scale_bar()` is deprecated. Please use `tm_scalebar()` instead.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\n\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\n\n\n\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"jenks\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n' to 'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\n\n\n\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"equal\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n' to 'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\n\n\n\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\n\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_tm_fill()`: migrate the argument(s) related to the scale of the\nvisual variable `fill` namely 'breaks' to fill.scale = tm_scale(&lt;HERE&gt;).\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\n\nWarning: Values have found that are higher than the highest break. They are\nassigned to the highest interval\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\nMultiple palettes called \"greens\" found: \"brewer.greens\", \"matplotlib.greens\". The first one, \"brewer.greens\", is returned.\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"-Greens\" is named\n\"greens\" (in long format \"brewer.greens\")\n\n\n\n\n\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\n\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"jenks\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_fill()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'legend.is.portrait' (rename to 'orientation') to\n'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_fill()`: use `fill.chart = tm_chart_histogram()` instead of\n`legend.hist = TRUE`.\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\n\n\n\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\nstyle set to \"classic\"\n\nother available styles are: \"white\" (tmap default), \"gray\", \"natural\", \"cobalt\", \"albatross\", \"beaver\", \"bw\", \"watercolor\"\n\ntmap v3 styles: \"v3\" (tmap v3 default), \"gray_v3\", \"natural_v3\", \"cobalt_v3\", \"albatross_v3\", \"beaver_v3\", \"bw_v3\", \"classic_v3\", \"watercolor_v3\"\n\nMultiple palettes called \"greens\" found: \"brewer.greens\", \"matplotlib.greens\". The first one, \"brewer.greens\", is returned.\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"-Greens\" is named\n\"greens\" (in long format \"brewer.greens\")\n\n\n\n\n\n\n\n\n\n\n\n\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(size = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_fill()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n! `tm_scale_bar()` is deprecated. Please use `tm_scalebar()` instead.\n! The 'size' argument of `tm_scalebar()` is deprecated as of tmap 4.0.\nℹ Please use 'text.size' instead.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\nstyle set to \"white\" (tmap default)\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\"\n\n\ntmap v3 styles: \"v3\" (tmap v3 default), \"gray_v3\", \"natural_v3\", \"cobalt_v3\", \"albatross_v3\", \"beaver_v3\", \"bw_v3\", \"classic_v3\", \"watercolor_v3\"\n\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"equal\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\nstyle set to \"white\" (tmap default)\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\"\n\ntmap v3 styles: \"v3\" (tmap v3 default), \"gray_v3\", \"natural_v3\", \"cobalt_v3\", \"albatross_v3\", \"beaver_v3\", \"bw_v3\", \"classic_v3\", \"watercolor_v3\"\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"equal\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\nFor small multiples, specify a 'tm_scale_' for each multiple, and put them in a\nlist: 'fill'.scale = list(&lt;scale1&gt;, &lt;scale2&gt;, ...)'\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          fill.scale = tm_scale(values = \"brewer.blues\", n = 5, style = \"quantile\")) + # ✅ 更新 fill.scale 语法\n  tm_facets(by=\"REGION_N\", \n            drop.units=FALSE, # ✅ 更新 drop.shapes -&gt; drop.units\n            title = \"Regional Distribution\") +  # ✅ 手动设置 title，避免 `NA`\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 8) +  # ✅ 适当降低 title.size\n  tm_borders(fill_alpha = 0.5)  # ✅ 更新 alpha -&gt; fill_alpha\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_fill()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'legend.is.portrait' (rename to 'orientation') to\n'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08a/Hands-on_Ex08a.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex08a/Hands-on_Ex08a.html#reference",
    "title": "Hands-on Exercise 8a",
    "section": "",
    "text": "tmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08b/Hands-on_Ex08b.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex08b/Hands-on_Ex08b.html#learning-outcome",
    "title": "Hands-on Exercise 8b",
    "section": "23.1 Learning outcome",
    "text": "23.1 Learning outcome\nBy the end of this hands-on exercise, you will acquire the following skills by using appropriate R packages:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08b/Hands-on_Ex08b.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex08b/Hands-on_Ex08b.html#the-data",
    "title": "Hands-on Exercise 8b",
    "section": "25.1 The data",
    "text": "25.1 The data\nThe data set use for this hands-on exercise is called SGPools_svy21. The data is in csv file format.\nFigure below shows the first 15 records of SGPools_svy21.csv. It consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08b/Hands-on_Ex08b.html#data-import-and-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex08b/Hands-on_Ex08b.html#data-import-and-preparation",
    "title": "Hands-on Exercise 8b",
    "section": "25.2 Data Import and Preparation",
    "text": "25.2 Data Import and Preparation\nThe code chunk below uses read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\nRows: 306 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): NAME, ADDRESS, OUTLET TYPE\ndbl (4): POSTCODE, XCOORD, YCOORD, Gp1Gp2 Winnings\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() is used to do the job.\n\nlist(sgpools) \n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows\n\n\nNotice that the sgpools data in tibble data frame and not the common R data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08b/Hands-on_Ex08b.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex08b/Hands-on_Ex08b.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "title": "Hands-on Exercise 8b",
    "section": "25.3 Creating a sf data frame from an aspatial data frame",
    "text": "25.3 Creating a sf data frame from an aspatial data frame\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\nThings to learn from the arguments above:\n\nThe coords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\nThe crs argument required you to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by refering to epsg.io.\n\nFigure below shows the data table of sgpools_sf. Notice that a new column called geometry has been added into the data frame.\nYou can display the basic information of the newly created sgpools_sf by using the code chunk below.\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\nThe output shows that sgppols_sf is in point feature class. It’s epsg ID is 3414. The bbox provides information of the extend of the geospatial data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08b/Hands-on_Ex08b.html#it-all-started-with-an-interactive-point-symbol-map",
    "href": "Hands-on_Ex/Hands-on_Ex08b/Hands-on_Ex08b.html#it-all-started-with-an-interactive-point-symbol-map",
    "title": "Hands-on Exercise 8b",
    "section": "26.1 It all started with an interactive point symbol map",
    "text": "26.1 It all started with an interactive point symbol map\nThe code chunks below are used to create an interactive point symbol map.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_bubbles()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').\nThis message is displayed once every 8 hours."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08b/Hands-on_Ex08b.html#lets-make-it-proportional",
    "href": "Hands-on_Ex/Hands-on_Ex08b/Hands-on_Ex08b.html#lets-make-it-proportional",
    "title": "Hands-on Exercise 8b",
    "section": "26.2 Lets make it proportional",
    "text": "26.2 Lets make it proportional\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\ntm_shape(sgpools_sf)+ tm_bubbles(col = “red”, size = “Gp1Gp2 Winnings”, border.col = “black”, border.lwd = 1)"
  }
]